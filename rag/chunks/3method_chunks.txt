Chunk 1:
In this section, we describe a novel framework  for enhancing the precision of retrieval results through sentence-level ranking and reconstruction, integrated into the RAG system. Note that  does not require additional training.

We first introduce the general RAG system, which consists of three steps: the retrieval step, the re-ranking step, and the generation step.
Note that all steps focus on passage-level documents.

==================================================

Chunk 2:
the re-ranking step, and the generation step.
Note that all steps focus on passage-level documents.The retrieval step searches for a potentially relevant document set $$ to the given query $q$ from a retrieval corpus $$ consisting of millions of documents. 
This retrieval step is conventionally performed using a sparse retriever $S$, such as BM25, which is widely used for processing large corpora due to its low latency. 
The sparse retriever $S$ fetches the relevant documents having high relevant scores based on lexical values such as document length or unique word count.
Formally, we define the retrieval step as:
\[
 = (q, ; S) = \{d_1, d_2, ..., d_n\}
\]
where $d_k$ represents a document having the top-$k$ score among the retrieval corpus $$ for a given query $q$, and $n$ denotes the size of $$, generally ranging from tens to hundreds.

==================================================

Chunk 3:
s $$ for a given query $q$, and $n$ denotes the size of $$, generally ranging from tens to hundreds.While the sparse retriever $S$ can efficiently handle a large corpus, it cannot consider semantic similarities, thereby limiting its retrieval performance for lexically different but semantically relevant pairs. 
To address this, the re-ranking step aims for more precise retrieval results by reordering the retrieved document set $$ using the ranking model $R$. 
This model transforms $$ into a newly ordered document set $$ based on relevance scores with a query $q$, capturing semantic meanings that could not be addressed in the retrieval step with $S$.
Formally, we define the re-ranking step as:
\[
=(q,;R)=\{d'_1,,d'_m\}
\]
where $d'_k$ represents the document that has top-$k$ relevance score among $$ and \(m  n\), indicating that the subset $$ contains significantly fewer documents than the original set $$.

==================================================

Chunk 4:
n\), indicating that the subset $$ contains significantly fewer documents than the original set $$.After the re-ranking step, the document set $$ is augmented to the LLM $M$ with the supporting documents to generate the correct answer $a$ for the given query $q$. The generation step can be formalized as:

\[
 a=(q, ; M)
\]

In RAG systems, the three key steps are designed to retrieve the most query-relevant knowledge for LLMs, typically at the passage level. However, this fixed granularity can overlook finer relevance between queries and individual sentences.

Therefore, in this work, we introduce a fine-grained, sentence-level ranking strategy in the re-ranking step, aiming to reduce distractions from irrelevant information and enhance answer accuracy.

==================================================

Chunk 5:
ranking step, aiming to reduce distractions from irrelevant information and enhance answer accuracy.We propose a novel unsupervised refinement framework, ocument Refinement with entence-evel e-ranking and Reconstruction (), designed to assess the fine-grained relevance of individual sentences within a passage and reconstruct to preserve the original contextual coherence. 
Figure  illustrates examples generated by each step in our  framework.

==================================================

Chunk 6:
iginal contextual coherence. 
Figure  illustrates examples generated by each step in our  framework.After the retrieval step (ยง), we conduct sentence-level re-ranking for the documents within the retrieved set $$. First, each document $d_i  $ is decomposed into a sentence set $_i = \{s_j\}_{j=1}^l$, where $s_j$ represents the $j$-th sentence in document $d_i$ and $l$ is the number of sentences in $d_i$. Then, the passage-level retrieved set $$ is redefined to the sentence-level retrieved set $ = _{i=1}^n _i$. For instance, as illustrated in Figure , a passage retrieved for a query ``How many episodes in "Grace and Frankie" Season 1?" is decomposed into three sentences \( s_1 \), \( s_2 \), and \( s_3 \) during the sentence decomposition step.

==================================================

Chunk 7:
sed into three sentences \( s_1 \), \( s_2 \), and \( s_3 \) during the sentence decomposition step.To extract sentences containing relevant information for a query \( q \), we initially perform re-ranking to assess relevance scores at the sentence level. Sentences in \(  \) with scores below a predefined threshold \( T \) are deemed irrelevant and removed, resulting in a refined set \(  \). The sentence-level re-ranking is formally defined as follows:
\[
=(q,;R)=\{s'_1,,s'_m\}
\]
where each $s'_k$ is a sentence from $$ whose relevance score exceeds $T$. Figure~ demonstrates the reordering of sentences, highlighting the exclusion of $s'_3$ due to its insufficient relevance score.

Note that this step of the  framework utilizes off-the-shelf ranking models, which are identical to those used in passage-level re-ranking.

==================================================

Chunk 8:
tilizes off-the-shelf ranking models, which are identical to those used in passage-level re-ranking.While the sentence decomposition and re-ranking steps select the top-$m$ relevant sentences for the query $q$, these sentences may lack contextual relationships to one another, as these steps can disrupt the original contextual flow of the passage by discarding some sentences.
Instead of following a widely used approach of simply concatenating these sentences in descending order of their relevance scores, we propose to reconstruct them into the contextually organized set, $^*$, to reflect the order in which they were originally positioned before being decomposed from passages, ensuring the original coherence and logical flow:
\[
^* = (,)=\{s^*_1,,s^*_m\}
\]
where $s^*_i$ is the sentence included in $S'$ and $i$ denotes the relative position of $s^*_i$ within $$. 
As shown in Figure~, the remaining two sentences are reconstructed in their original order by switching their positions to preserve the context before the sentence re-ranking step.
Then, LLM $M$ generates the answer $a$ for a given query $q$ with $^*$ formalized as: $a=(q,^*;M)$.

==================================================

