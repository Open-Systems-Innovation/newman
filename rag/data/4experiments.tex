\section{Experiment Setups}

In this section, we describe the experimental setup for evaluating \textit{DSLR} across various scenarios. We provide additional details in Appendix \ref{sec:Experimental_Setups}.

% In this section, we outline the experimental setups used to evaluate our novel retrieval and reconstruction approach within the RAG framework. We focus on comparing sentence-level reranking with traditional passage-level reranking across various scenarios to ascertain their relative effectiveness. 



\subsection{Models}

\noindent\textbf{Retriever.} We use BM25~\cite{bm25} as a passage-level retriever, which is a widely used sparse retriever due to its notable performance with high efficiency. The retriever fetches the \textbf{top-1} passage-level query-relevant document from an external corpus, which serves as the baseline document.
% Note that the documents consist of coarse-grained passage-level text.
% For our experimental setup, retrieval step retrieves the top-20 and top-100 documents for subsequent reranking across all datasets. 


\noindent\textbf{Re-ranker.} 
We operationalize a variety of ranking models as re-rankers, including off-the-shelf retrievers, fine-tuned re-rankers, and LLMs.
\textbf{1) Sparse Retriever:} We use \textbf{BM25}~\cite{bm25} as a sentence-level re-ranker. Note that BM25 is only applied at the sentence level, as it is primarily utilized in the retrieval step.
\textbf{2) Dense Retriever:} We utilize two representative dense retrievers, \textbf{Contriever}~\cite{contriever} and \textbf{DPR}~\cite{DPR-karpukhin}, which are better at capturing the semantic similarity between documents and queries than sparse retrievers.
\textbf{3) Supervised Re-ranker\footnote{It is important to note that the terms `supervised' and `unsupervised' in this context refer to the models being trained on document ranking tasks, and not on document refinement tasks.\label{footnote}}:} We employ two supervised re-ranking models based on T5~\cite{T5}, \textbf{MonoT5}~\cite{monot5} and \textbf{RankT5}~\cite{rankt5}. These models are specifically trained for pointwise document ranking tasks.
\textbf{4) Unsupervised Re-ranker\footref{footnote}:}
We explore \textbf{Relevance Generation (RG)}~\cite{holistic}, a pointwise ranking method using the inherent ranking ability of LLMs, validating its effectiveness in scenarios lacking extensive labeled data.
We use LLama2-13b-chat~\cite{Llama2} as a ranking model for \textbf{RG}.
% Our reranking framework incorporates both supervised and unsupervised approaches. \textbf{Hybrid Retrieval:} We investigate a hybrid retrieval approach that merges the advantages of lexical features (sparse) and dense retrieval techniques, employing models like DPR and Contriever. We initially retrieve using BM25, followed by reranking with dense retrievers. \textbf{Supervised Reranking:} We employ MonoT5 and RankT5, which are models specifically trained for pointwise document ranking tasks. \textbf{Unsupervised Reranking:} Relevance Generation(RG), which is the pointwise ranking prompting using the open-sourced LLama2-13b-chat, is explored to validate effectiveness in scenarios devoid of extensive labeled data. To address the computational cost associated with the decomposition into sentences for reranking, we employ pointwise rankers with a complexity of \textit{O(n)}. To maintain consistency across different retrieval setups and enable a direct comparison of various reranking strategies, we standardize the length of the documents(\textit{L}) used for answer generation post-reranking at either 100 or 500 words.


\noindent\textbf{Reader.}
We use the instruction-tuned, open-source LLM \textbf{LLama2-13b-chat} as our reader. To generate the final answer, the document is prepended to the system prompt.

\subsection{Datasets}

We evaluate our \textit{DSLR} across 6 open-domain QA datasets, including both general and specific domains.
First, we conduct our experiment using the development set of \textbf{Natural Questions (NQ)}~\cite{NQ}, \textbf{TriviaQA (TQA)}~\cite{TQA}, and \textbf{SQuAD (SQD)}~\cite{SQD}, consisting of queries with general topics.
%These datasets are often exploited to evaluate the performance of RAG systems in previous work.
Additionally, we incorporate specialized datasets such as \textbf{RealtimeQA (RQA)}~\cite{Realtime}, \textbf{SciQ (SQ)}~\cite{sciq}, and \textbf{BioASQ (BASQ)}~\cite{bioasq, bioasq2} for evaluating the generalizability of our proposed method.
In detail, RQA includes questions that are updated periodically to test our system's ability to handle ever-evolving knowledge. 
In addition, SQ and BASQ are domain-specific datasets in science and biology, respectively.
Specifically, for BASQ, we selectively use the questions from the BioASQ6 challenge (task b) that are suitable for yes/no and factoid responses.
% We further extend our evaluation to domain-specific datasets, including \textbf{SciQ (SQ)} in the science domain and \textbf{BioASQ (BASQ)} in the biology domain, to verify our system's efficacy in specialized contexts.
We report the effectiveness of our framework with \textbf{Accuracy (Acc)}, which determines whether the prediction contains golden answers, following~\citet{selfrag}. 

% sentence-level re-ranking and reconstruction system across a diverse array of open-domain QA datasets to assess the generalizability of our approach under various informational and query contexts. 
% Our experiments are designed to compare our system against existing research using well-established datasets such as Natural \textbf{Questions (NQ)}, \textbf{TriviaQA (TQA)}, and \textbf{SQuAD (SQD) }which is dev-set following prior work\cite{}. Additionally, we incorporate specialized datasets such as \textbf{RealtimeQA (RQA)}, which includes questions that are updated periodically to test our system's ability to handle knowledge that changes over time. We further extend our evaluation to domain-specific datasets, including \textbf{SciQ (SQ)} in the science domain and \textbf{BioASQ (BASQ) }in the biology domain, to verify our system's efficacy in specialized contexts. 
% We employ the evaluation metric, \textbf{accuracy (acc)}, which determines whether the answers generated contain golden answers following~\citet{selfrag}. 


% \noindent\textbf{Reader} LLM을 reader로써 사용하였고 open sourced LLM인 LLama2-13b-chat을 사용하였다. 
% We compare the effectiveness of traditional passage-level reranking with our proposed sentence-level reranking approach. Our retrieval method employs \textbf{BM25} for initial retrieval, allowing for comparisons with more advanced reranking techniques such as hybrid approach \textbf{DPR} and \textbf{Contriever}. we evaluate supervised reranking models, including \textbf{RankT5} and \textbf{MonoT5}, to enhance relevance. An unsupervised reranker \textbf{Pointwise LLM ranker(RG)} LLama2-13b-chat을 사용하여 is also included in our experiments to assess performance in scenarios lacking extensive labeled training data. Our experiments focus on contrasting the efficacy of passage-level reranking against our novel sentence-level reranking method, which incorporates a reconstruction process. We employ open-source \textbf{LLama2-13b-chat} as reader.




% \subsection{Experiment 2: Handling Long Contexts}

% In our second experiment, we tackle the challenge of processing long-context QA using the RAG framework, following configurations from previously established benchmarks. We specifically employ datasets from the LongBench benchmark, which include NarrativeQA(NQA), Qasper, and MultiFieldQA(MFQA). These datasets are characterized by their long-context QA settings, which present significant challenges for effective retrieval and ranking.

% In our second experiment, we address scienario that the challenge of handling long-context QA using the RAG framework, configured according to established benchmarks. We utilize datasets from the LongBench benchmark, specifically NarrativeQA (NQA), Qasper, and MultiFieldQA (MFQA). These datasets are specifically curated for their relevance to long-context QA scenarios, featuring contexts that surpass the maximum length limitations typical of standard LLMs.

% To manage the long contexts inherent in these datasets with retrieval, we segment the contexts into 100-word chunks. These segments are then subjected to both passage-level and sentence-level reranking processes. The reranking models and the reader LLM utilized in this experiment are consistent with those applied in our first experiment.
\input{fig/various}
\subsection{Implementation Details}

% In the conducted experiments, the generation of language model outputs was standardized using greedy decoding to ensure consistency across all tests. For the retrieval process, we use pyserini framework for retrieval, we employed preprocessed Wikipedia datasets for the NQ, TQA, SQD, and SQ datasets, following methodologies from previous studies. For the BASQ dataset, we utilized the BEIR (v1.0.0) BioASQ corpus, specifically curated for bio-medical information retrieval.
% MonoT5랑 RankT5로 T5-base에 finetuned된 것을 사용했다. For the RQA dataset, we utilized the Google Cloud Search (GCS) API to retrieve documents that adhere to the dataset's guidelines. This approach ensures the retrieval of documents contemporaneously updated to match the period when the queries were generated, thus reflecting the state of knowledge relevant to that time. The QA prompts used for the open-domain QA were sourced from the publicly available llama-index, whereas prompts for datasets from the LongBench benchmark adhered strictly to those specified in the LongBench publication. The experiments were powered by clusters equipped with NVIDIA A100 GPUs. 

% For building a retrieval system for RAG, we implement BM25 using the Pyserini~\footnote{\url{https://github.com/castorini/pyserini}} to fetch the documents from the retrieval corpus.
The threshold \( T \), used to remove irrelevant content, was determined empirically by sampling 1,000 random entries from each of the NQ, TQA, and SQD training sets and setting \( T \) to the relevance score at the 90th percentile. Detailed values of \( T \) for various models are provided in Table \ref{tab:public_threshold}.
The retrieval corpus for NQ, TQA, and SQD is a pre-processed Wikipedia dump from Dec. 20, 2018 following~\citet{DPR-karpukhin}, and for BASQ and RQA, we use their own retrieval corpora.
To be specific, BASQ used the BEIR (v1.0.0)~\footnote{\url{https://github.com/beir-cellar/beir}} BioASQ corpus, specializing in biomedical information retrieval.
For the RQA dataset, spanning from 2022 to 2023, we use the search documents provided at the time of dataset creation through the Google Cloud Search (GCS) API to align the periods of the queries and answers. 
% For retrieval, we employed the Pyserini framework~\footnote{\url{https://github.com/castorini/pyserini}} with preprocessed Wikipedia datasets for NQ, TQA, SQD, and SQ, following established methodologies. 
% The BASQ dataset used the BEIR (v1.0.0) BioASQ corpus, specializing in biomedical information retrieval. 
% MonoT5 and RankT5, fine-tuned on T5-base, were deployed for supervised re-ranking. 
% For the RQA dataset, spanning 2022 to 2023, we utilized search documents provided at the time of dataset creation through the Google Cloud Search (GCS) API to ensure alignment between the periods of the queries and answers.
% For BASQ, we selectively employed questions from the BioASQ6 challenge (task b) that are suitable for yes/no and factoid responses, evaluated using accuracy metrics.
When implementing each component in \textit{DSLR}, we decompose passage-level documents into sentences using the Sentencizer from Spacy\footnote{\url{https://spacy.io/}}. All predictions in our experiments are generated via greedy decoding. 

% When implementing our proposed method, \textit{DSLR}, we decompose passage-level documents into sentences via Sentencizer of Spacy~\footnote{\url{https://spacy.io/}}.
% Also, we enhanced document-based answer generation for all datasets by including titles to contextualize the information for the LLM following~\citet{selfrag}. 
% We exploit QA prompts for open-domain queries from the publicly available llama-index~\footnote{\url{https://www.llamaindex.ai/}}.
% All predictions in our experiments are generated via greedy decoding.
% For sentence-level re-ranking, we decomposed sentences using spacy from nltk. 
% In our experiments, we standardized language model output generation across all tests using greedy decoding. 
