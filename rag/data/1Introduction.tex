\section{Introduction}

Recent advancements in Large Language Models (LLMs) \cite{fewshotlearner, GPT-4_technical_report, Llama2} have significantly expanded their capabilities across diverse knowledge-intensive tasks in Natural Language Processing (NLP), such as Question Answering (QA) \cite{NQ, TQA, SQD}. 
However, despite these capabilities, LLMs still face challenges such as generating plausible yet non-factual responses, known as hallucination, due to their reliance on limited parametric memory \cite{DBLP:conf/acl/MallenAZDKH23}. 
Also, it is noted that this parametric memory is static, as LLMs can learn knowledge only up to the specific date on which the training was completed.
Therefore, these limitations restrict their adaptability to long-tailed or ever-evolving domains \cite{Realtime} and to unseen knowledge outside their training data \cite{kalmv}.


%\input{fig/motivation}
\input{fig/dslr}

Retrieval-Augmented Generation (RAG) \cite{DBLP:conf/iclr/KhandelwalLJZL20, RAG, DBLP:conf/icml/BorgeaudMHCRM0L22, replug} has been introduced as an effective solution to address such problems.
Specifically, RAG enhances LLMs by integrating non-parametric memories fetched from external knowledge bases using a retrieval module, which helps LLMs' responses grounded on factual evidence and makes them more up-to-date. 

While the efficacy of RAG depends on the performance of the retrieval module, the instability of LLMs in incorporating the retrieved knowledge is also a critical challenge to RAG.
To be specific, retrieved documents sometimes contain irrelevant information~\cite{das}, and LLMs often struggle to effectively filter out such redundant details and focus on the most query-relevant knowledge~\cite{distractor, distractor2, lostinthemiddle, distractor3}, which leads to the failure of the overall RAG systems.
Therefore, it is crucial to investigate how to effectively refine retrieved documents before augmenting them with LLMs, ensuring that the LLMs are not distracted by irrelevant information within retrieved documents.
%Therefore, delivering accurate and refined knowledge focusing on solving the given query remains a critical challenge for RAG systems.
% Such inefficiencies can misguide the generation process, resulting in less accurate or potentially misleading responses.

% Retrieval-Augmented Generation (RAG) systems have been introduced as an effective solution for mitigating existing limitations.
% These systems enhance LLMs by integrating non-parametric memories fetched by the retriever module from external knowledge bases, strengthening the generation to be more informed and accurate by being grounded on factual evidence.
% The efficacy of RAG systems is largely contingent upon the performance of the retriever module, specifically its ability to retrieve relevant knowledge that helps answer the given query. 
% However, the imperfection of the current retrieval systems counties to face significant challenges that 
% However, as current retrieval systems don't always search for the optimal knowledge for the given query, these challenges may lead to generating inaccurate outputs even though exploiting RAG systems. 
% Furthermore, the subsequent inability of large language models (LLMs), in their role as readers, to effectively filter and prioritize the most pertinent information exacerbates these issues. Such inefficiencies can misguide the generation process, resulting in less accurate or potentially misleading responses.
% strengthening the models' ability to produce informed and accurate outputs. 

% Recent advancements in Large Language Models (LLMs) have remarkably extended their capabilities across various NLP tasks, particularly in question answering (QA).
% However, LLMs still struggle with generating plausible yet non-factual responses, called hallucination issues, due to their limited parametric memory~\cite{}.
% These memories are static, storing the knowledge only up to a specific date of the training procedure. 
% This limitation restricts their adaptability to long-tailed or ever-evolving knowledge. Also, it increases their propensity to produce incorrect or non-factual responses when faced with queries outside their training data.

% To mitigate existing limitations, Retrieval-Augmented Generation (RAG) systems have been introduced as an effective solution. 
% These systems enhance LLMs by integrating non-parametric memories fetched by the retriever from external knowledge bases during the response generation, thereby strengthening the models' ability to produce informed and accurate outputs. 
% The efficacy of RAG systems is largely contingent upon the performance of the underlying retrieval mechanisms. 
% Current retrieval systems, however, continue to face significant challenges that limit their efficacy. These challenges originate from the imperfect nature of the technologies employed, which may lead to the retrieval of irrelevant information. Furthermore, the subsequent inability of Large Language Models (LLMs), in their role as readers, to effectively filter and prioritize the most pertinent information exacerbates these issues. Such inefficiencies can misguide the generation process, resulting in less accurate or potentially misleading responses.


% To mitigate existing limitations, Retrieval-Augmented Generation (RAG) systems have been introduced. These systems enhance LLMs by integrating external, non-parametric knowledge bases during the response generation process, thereby strengthening the models' ability to produce informed and accurate outputs. The efficacy of RAG systems is largely contingent upon the performance of the underlying retrieval mechanisms. Current retrieval systems, however, continue to face significant challenges that limit their efficacy. These challenges originate from the imperfect nature of the technologies employed, which may lead to the retrieval of irrelevant information. Furthermore, the subsequent inability of Large Language Models (LLMs), in their role as readers, to effectively filter and prioritize the most pertinent information exacerbates these issues. Such inefficiencies can misguide the generation process, resulting in less accurate or potentially misleading responses.


% Historically, retrieval systems have progressed from keyword-based methods such as BM25 to more sophisticated semantic-based models that employ dense vectors for document retrieval. These advanced models strive to align more closely with the semantic intricacies of user queries, thereby improving the relevance of the retrieved documents. Despite these advancements, the systems often retrieve a mixture of relevant and irrelevant information, highlighting the need for further advancements to refine the retrieval process.
% Reranking the order of  retrieved documents based on their relevance to a given query or refining them by removing irrelevant contents is considered an effective solution to address these issues.

Re-ranking the order of the retrieved document set~\cite{monot5, llm-rerank} or refining them into new documents~\cite{filco,recomp} can be considered as solutions. 
%Document reranking methodologies reorder documents using more effective but complex ranking models after retrieving them with efficient yet straightforward models from an external knowledge base.
%To be specific, \citet{bert-rerank}, \citet{monot5}, and \citet{llm-rerank} introduced some document reranking strategies to reorder documents to locate the most relevant document at the top of the retrieved document set.
%Furthermore, \citet{recomp}, \citet{filco}, \citet{REAR}, and \citet{bider} recently focus on refining the retrieved document set into newly generated documents by filtering out irrelevant information.
However, they generally require high computational costs for training additional re-ranking or refining models.
% This enhances the retrieval system's precision by ensuring that the most relevant documents are placed at the top of the set. 
% However, since prior approaches only change the order within the sets 그 들의 단위는 fixed length의 such as 100 words, passage로 주로 다뤄졌고 여전히 query와 완전히 relevant한 문장만 있는 것은 아니고, it is limited in handling irrelevant knowledge within the documents, which can distract the LLMs \cite{distractor}. 그래서 retrieval unit의 such as phrase, sentence로 granularity를 낮추는 방식으로 해결할 수도 있다. 하지만 sentence는 앞 뒤 내용이 다 사라져서 context가 유지되지 않는 문제가 발생한다.
%Traditional methods have primarily managed this within fixed-length units such as 100-word passages. However, these units still often contain irrelevant sentences, which can introduce noise and distract LLMs \cite{distractor}. A potential solution is to reduce the granularity of the retrieval units to smaller segments like phrases or sentences. Although this approach can mitigate the inclusion of irrelevant content, using sentences as units risks losing contextual information, as it eliminates surrounding content that may be crucial for understanding.
Another proposed solution is to reduce the retrieval granularity from passage-level to sentence-level which can help eliminate redundant information within passages \cite{phrase_retriever, denseXretrieval}.
However, this might also inadvertently remove important contextual information, which is crucial for accurately answering the given queries \cite{decontextualization}.
Therefore, we should explore a novel method that can effectively and efficiently filter out irrelevant information while maintaining the necessary contextual details.
%Alternatively, several approaches \cite{recomp, filco, REAR, bider} focus on refining the retrieved document set into newly generated documents, filtering out non-relevant information. 
%Alternatively, \citet{recomp}, \citet{filco}, \citet{REAR}, and \citet{bider} recently focus on refining the retrieved document set into newly generated documents by filtering out irrelevant information. 
%These methodologies typically involve specialized additional training designed to minimize the inclusion of irrelevant content in the retrieval results. 
%Refining might be more effective than reranking as it aims to create documents containing only relevant information, though it requires additional training.
%While refining strategies might be more effective than reranking ones because they aim to generate documents containing only relevant information, they generally require additional training.
%Effectively filtering the most relevant information without additional training remains underexplored despite its importance for efficiency.

%In this work, we introduce an unsupervised refinement approach consisting of three steps: dividing the document into sentence-level granularity, filtering the most relevant sentences for a given query, and reconstructing these sentences into a single document. 
In this work, we introduce an unsupervised \textbf{\textit{DSLR}} (\textbf{D}ocument Refinement with \textbf{S}entence-\textbf{L}evel \textbf{R}e-ranking and Reconstruction) framework that consists of three steps: 1) decomposition, 2) re-ranking, and 3) reconstruction.
%decomposing the retrieved document into sentence-level granularity, filtering out the irrelevant sentences based on the re-ranking score, and reconstructing the remaining sentences into a coherent document to preserve the original contextual information. 
%Specifically, our method integrates passage-level retrieval with subsequent sentence-level reranking and reconstruction. 
%Initially, passages are retrieved to serve as a coarse relevance filter and are then subjected to sentence-by-sentence reranking based on their relevance to the query.
Specifically, after retrieving the passage-level document, the \textit{DSLR} framework operates by first decomposing the retrieved document into sentences for finer granularity and then filtering out the irrelevant sentences based on their re-ranking scores from the ranking models, including off-the-shelf retrievers and re-rankers. 
Finally, the remaining sentences are reconstructed into a single document to preserve the original contextual information. 
Note that \textit{DSLR} is an unsupervised refinement framework, which does not require any additional training for re-ranking or reconstruction steps.
The overall \textit{DSLR} framework is illustrated in Figure~\ref{fig:dslr}.
%Specifically, the process begins by retrieving query-relevant documents at the passage level to serve as an initial relevance filter. Subsequently, the sentences within these passages are reranked based on their relevance to the given query. Lastly, the remaining query-relevant sentences are reordered to follow their original sequence, reconstructing the passage to ensure it preserves the contextual information.
%Following this reranking, the most pertinent sentences are reordered to reconstruct the passage, maintaining coherence and alignment with the query’s intent. 
%This approach aims to enhance the precision of the retrieval process and preserve the contextual integrity of the retrieved passages, ensuring that the resulting content remains both relevant and contextually coherent.
%Therefore, our method aims to enhance the precision of the retrieval step while preserving the contextual integrity of the retrieved passages, ensuring that the resulting document remains relevant and coherent.

% In this work, we introduce an unsupervised refinement approach consisting of three steps: dividing the document's granularity into sentence levels, filtering the most relevant sentences for a given query, and reconstructing the sentences into a single document.
% In detail, our method integrates passage-level retrieval with subsequent sentence-level reranking and reconstruction. Initially, passages are retrieved to act as a coarse relevance filter and then subjected to a sentence-by-sentence reranking based on their relevance to the query. Following this reranking, the most pertinent sentences are reordered to reconstruct the passage in a way that maintains coherence and alignment with the query’s intent. This approach is aimed not only at enhancing the precision of the retrieval process but also at preserving the contextual integrity of the retrieved passages, ensuring that the resulting content remains both relevant and contextually coherent.

We validate our framework across a diverse range of open-domain QA benchmarks, which include three general QA datasets and three specific QA datasets that require domain-specific or ever-evolving knowledge.
Our experimental results show that \textit{DSLR} significantly enhances the overall RAG performance and is comparable to, or even outperforms, the supervised baseline approaches.
Specifically, when evaluated with specific QA datasets, \textit{DSLR} shows high robustness in realistic settings.
Furthermore, a detailed analysis demonstrates the effectiveness of each proposed step and how it contributes to the overall performance.
%These findings reveal the effectiveness of our method in enhancing the relevance and granularity of information retrieval in complex QA tasks.

Our contributions in this work are threefold:
\vspace{-0.075in}
\begin{itemize}
    % Motivation
  \item We point out that recent RAG systems are largely vulnerable to redundant knowledge within fixed-size passage-level retrieved documents and that the existing refining strategies generally require additional training steps.
  %We propose a novel retrieval framework that combines coarse-grained passage retrieval with fine-grained sentence-level reranking to enhance the accuracy and relevance of information sources for RAG systems.
   % 제안한 방법론
  \item We propose a \textit{DSLR} framework that incorporates sentence-level re-ranking and reconstruction to effectively remove redundant knowledge that negatively affects the RAG system.
  %We introduce a sentence reordering that reconstructs the passage context, ensuring that the flow and coherence of information are maintained, even after intensive reranking.
  % 실험 결과 잘 나옴
  \item We show that \textit{DSLR} is highly effective and efficient even without additional training steps in both general and specific scenarios.
  %We have conducted extensive evaluations on multiple open-domain QA datasets, which include domain-specific datasets. We demonstrate that our approach consistently outperforms existing methods.
\end{itemize}

% Alternatively, several approaches focus on refining the retrieved document set into newly generated documents, filtering out non-relevant information. These strategies typically involve specialized additional training designed to minimize the inclusion of irrelevant content in the retrieval results. This method is more effective than re-ranking because it aims to create documents containing only relevant information, but it has the disadvantage of requiring additional training.
% Effectively filtering the most relevant information without additional training has been underexplored.

% Re-ranking the order of the retrieved document set or refining them into new documents is considered an effective solution to address these issues. 
% Document reranking methodologies reorder documents using more effective but complex retriever models after retrieving them with efficient yet straightforward retriever models from an external knowledge base.
% This enhances the retrieval system's precision by ensuring that the most relevant documents are placed at the top of the set. 
% However, since this approach only changes the order within the sets, it is limited in handling irrelevant knowledge within the documents.
% In another line of work, several approaches refine the retrieved document set into a newly generated document, filtering out non-relevant information within documents.
% These strategies typically involve specialized additional training designed to minimize the inclusion of irrelevant content in the retrieval results. 
% This is more effective than re-ranking because it targets to create new documents containing only relevant information but has the disadvantage of requiring additional training.
% Therefore, research that leaves only the most relevant information possible without additional training has not yet been proposed.
% Moreover, there is a growing interest in refining the granularity of retrieval to smaller textual units, such as phrases or sentences, to increase the precision of the content provided. 
% However, this approach introduces significant challenges, as sentences indexed separately from their larger textual context can lead to a loss of contextual information. Such decontextualization can severely impair retrieval systems, especially when the information's relevance is intricately linked with its broader narrative or argumentative structure.
% Additionally, the rise of unsupervised approaches that leverage the intrinsic capabilities of LLMs marks a significant advancement toward more effective retrieval processes. 
% These methods capitalize on the broad applicability of modern language models to enhance ranking precision without relying on extensive labeled data sets.

% In response to these limitations, innovative approaches have been developed. Hybrid approaches and document reranking strategies, which integrate traditional retrieval methods with modern semantic models or sophisticated supervised reranking techniques, are enhancing the accuracy of information retrieval. Additionally, the rise of unsupervised approaches that leverage the intrinsic capabilities of Large Language Models (LLMs) marks a significant advancement toward more effective retrieval processes. These methods capitalize on the broad applicability of modern language models to enhance ranking precision without relying on extensive labeled data sets.

% RAG에서 문서를 retrieval할 때  imperfect retrieval system에 많은 challenge들이 있다. 이러한 시스템들은 query에 대해서 관련 있는 문서, 관련 없는 문서를 혼합하여 반환하게 되며, 관련있는 문서라고 하더라도 retrieval unit이 고정된 길이(e.g. 100 words)의 paragraph 단위로 가져오기 때문에 반드시 query에 관련된 내용만 있는 것은 아니며, 이는 Reader로 쓰이는 LLMs이 discern and prioritize the most pertinent information from the retrieved documents한 능력이 부족하여 모델을 mislead하여 inaccurate하거나 hallucinations한 output을 유도할 수 있다. 또한 이를 극복하고 싶어 문서의 양을 늘리는 경우 이는 중간에 정보들이 제대로 활용되지 못하는 문제가 있다. 

% Recent research has handled the challenges associated with passage-level retrieval, where content is often retrieved in fixed-length units, such as 100-word passages, that may not precisely align with the user queries. This misalignment necessitates enhanced methods for improving the precision and relevance of retrieved information. In the context of RAG systems, various strategies have been developed to refine the content of retrieved documents. These strategies typically involve specialized training methods designed to minimize the inclusion of irrelevant content in the retrieval results. Moreover, there is a growing interest in refining the granularity of retrieval to smaller textual units, such as phrases or sentences, to increase the precision of the content provided. However, this approach introduces significant challenges, as sentences indexed separately from their larger textual context can lead to a loss of contextual information. Such decontextualization can severely impair retrieval systems, especially when the relevance of the information is intricately linked with its broader narrative or argumentative structure.


% Moreover, sentence-level indexing requires maintaining a vast index with each sentence cataloged individually, resulting in significantly larger indexes than passage-based systems and increasing the complexity and cost of index maintenance and querying. Such detailed indexing may impede the scalability and efficiency of retrieval systems. Efforts to refine retrieval granularity to smaller units like phrases or sentences present notable challenges. Sentence-level retrieval can lead to decontextualization, where sentences are extracted and indexed separately from their larger textual context, losing critical nuances. This can mislead retrieval systems, especially when the relevance of information is tightly linked to its broader narrative or argument structure. 

% Moreover, sentence-level indexing requires maintaining a vast index with each sentence cataloged individually, resulting in significantly larger indexes than passage-based systems and increasing the complexity and cost of index maintenance and querying. Such detailed indexing may impede the scalability and efficiency of retrieval systems.


% Nevertheless, much of those studies predominantly deal with passage-level retrieval and reranking, where content is often retrieved in fixed-length units, such as 100-word passages, that may not always directly pertain to the query at hand. Concurrently, other studies are focusing on enhancing the RAG systems by increasing the precision of retrieved content and removing the presence of irrelevant context. These studies require task-specific additional training designed to filter out superfluous content, further fine-tuning the retrieval process for optimal performance.


% retrived 된 document의 성능을 높이기 위해서 reranking이라는 것이 등장했다.

% Despite the potential of RAG, many challenges persist due to the imperfections of current retrieval systems. These systems frequently retrieve a mix of relevant and irrelevant documents in response to queries. Moreover, because these documents are typically retrieved in fixed-length units, such as 100-word passages, they may not exclusively contain content pertinent to the query. This often leads to scenarios where the LLM, serving as the reader, lacks the capability to discern and prioritize the most relevant information from the retrieved documents. Such limitations can mislead the model, resulting in inaccurate outputs or further hallucinations. Attempts to overcome these shortcomings by increasing the volume of retrieved documents often fail to utilize the intermediary information effectively, highlighting that more content is not necessarily beneficial.

% 위 방법으로 relevance를 증가시켰지만 여전히 리트리버의 한계로 인한 RAG 성능 저하..
% 최근 RAG 성능을 높이기 위한 document refine 전략들...


% Furthermore, efforts to refine the granularity of retrieval units to smaller entities such as phrases or sentences introduce significant challenges. Sentence-level retrieval often results in decontextualization, where sentences extracted and indexed independently from their broader textual environment lose the nuanced information provided by adjacent sentences or paragraphs. This decontextualization can mislead retrieval systems, especially when the relevance of the information is deeply intertwined with a broader narrative or argumentative structure. Additionally, the fine granularity of sentence-level indexing necessitates the maintenance of a massive index where each sentence is cataloged as a separate entry. This approach leads to exponentially larger indexes compared to passage-based indexing and escalates the complexity and cost of maintaining and querying these indexes. The overhead associated with such detailed indexing can significantly hinder the scalability and efficiency of retrieval systems. 

% Building on the challenges highlighted, the current research explores innovative approaches to optimize retrieval systems for Retrieval-Augmented Generation (RAG) by refining the granularity of retrieval and enhancing the contextual precision of the content that feeds into LLMs. Specifically, this paper proposes a novel method that combines the robustness of passage-level retrieval with the precision of sentence-level analysis and reconstruction. This approach aims to mitigate the shortcomings of both retrieval scopes by employing a two-step process: initially retrieving relevant passages using retrieval methods and subsequently enhancing the precision of these results through targeted sentence-level reranking and reconstruction.

% This methodology addresses critical issues in traditional RAG systems. First, by starting with passage-level retrieval, the system maintains the contextual richness essential for understanding complex queries and generating coherent answers. This is particularly important as it preserves the narrative or argumentative structures necessary for a comprehensive understanding of the subject matter, which are often lost in sentence-level retrieval. Then, by employing sentence-level reranking and reconstruction within these passages, the system can focus on and enhance the relevance of the most crucial information, thereby reducing noise and improving the overall quality of the content used for generating answers.
% While sentence-level retrieval may achieve high precision, it often suffers from poor recall. This occurs because, although the relevance of isolated sentences to a query might be high, without the broader context, crucial information that could be pertinent to a user’s query may be overlooked. Moreover, this high precision is frequently undermined by the increased likelihood of retrieving sentences that, despite closely matching the query terms, may prove to be contextually irrelevant.


% Furthermore, there is also adjusting the granularity of retrieval units to smaller entities such as phrases or sentences. However, sentence level retrieval can cause significant decontextualization. Sentences extracted and indexed independently of their larger textual environment often lose the nuanced information provided by adjacent sentences or paragraphs. This decontextualization can mislead retrieval systems, particularly when the relevance of the information depends on a broader narrative or argumentative structure.The fine granularity of sentence-level indexing requires maintaining a massive index where each sentence is a separate entry. This not only leads to exponentially larger indexes compared to passage-based indexing but also increases the complexity and cost of maintaining and querying these indexes. The overhead associated with such detailed indexing can hinder the scalability and efficiency of retrieval systems. While sentence-level retrieval can provide high precision, it often suffers from poor recall. This is because the relevance of isolated sentences to a query might be high, but without the broader context, crucial information that could be pertinent to a user's query might be overlooked. Additionally, the high precision is frequently offset by the increased likelihood of retrieving sentences that, although closely matching the query terms, may be contextually irrelevant.
% when considered in isolation, smaller granularity units, such as phrases and sentences. However, when sentences are retrieved individually—referred to in studies as "decontextualization"—they often lose the broader narrative or factual coherence, leading to context fragmentation issues.



% 이와 같은 문제를 극복하기 위해서 retrieval system 측면에서 다양한 연구들이 진행되고 있다. 한 가지 방법은 document reranking 방식이다. 이는 우선 BM25와 같은 비교적 가벼운 retriever로 고정된 길이의 passage 단위로 문서를 가져오게 된다. 그리고 

% 하지만 여전히 RAG에서 imperfect retrieval systems 때문에 많은 challenge들이 있다. 이러한 retrieval systems는 query에 대해서 관련 있거나, 관련 없는 paragraph들을 혼합하여 반환하며 이는 output에 inaccurate하거나 hallucations을 유발할 수 있다. 게다가 retrieval unit이 보통 고정된 단어수의 paragraph인데 이때 
% This issue is exacerbated by the models' inability to effectively discern and prioritize the most pertinent information from the retrieved documents. 






% Among these, FilCo emerges as a notable solution by proposing a dual strategy that involves identifying useful context through lexical and information-theoretic approaches and employing context filtering models to sift through retrieved contexts at test time. This method has demonstrated superiority over existing techniques across multiple knowledge-intensive tasks, evidencing its capacity to enhance the relevance and utility of the context fed into generation models. Similarly, RECOMP advances the field by compressing retrieved documents into concise summaries before their in-context integration, significantly reducing computational costs while maintaining performance levels. This approach not only alleviates the challenges posed by the voluminous nature of retrieved documents but also aids in selectively augmenting the language model input, depending on the relevance of the documents.

% In response to these challenges, several innovative approaches have been developed. For instance, FilCo introduces a method for improving the quality of context by identifying useful content through lexical and information-theoretic methods and filtering out irrelevant passages at test time. Similarly, RECOMP focuses on compressing retrieved documents into concise summaries before their integration, thereby reducing computational costs and enhancing the models' ability to identify relevant information. LongLLMLingua proposes a solution through prompt compression, optimizing the models' perception of key information and addressing the challenges of high computational cost, latency, and performance in long context scenarios. Additionally, various efforts have been made to rerank passages, select evidential content, and employ selective retrieval strategies to further refine the quality of context provided to generation models. These approaches represent critical strides toward overcoming the limitations of current RAG models, underscoring the ongoing pursuit of more reliable and efficient information retrieval and processing mechanisms within natural language processing.
% In response to the limitations inherent in current retrieval-augmented generation (RAG) models, a variety of innovative strategies have been proposed, aiming to refine the retrieval process and improve the overall effectiveness of these models. The primary challenge centers around the models' tendency to process and incorporate irrelevant or distracting content within the retrieved documents, leading to inaccuracies and inefficiencies in generated outputs. Prior works have made strides toward addressing this issue through approaches such as reranking of passages to emphasize more relevant content (Wang et al., 2018; Nogueira and Cho, 2020; Mao et al., 2021), the selection of evidential passages only (Asai et al., 2022), and retrieval of passages strictly when necessary for the generation model (Mallen et al., 2023; Jiang et al., 2023). Additionally, efforts like those by Choi et al. (2021) to decontextualize sentences and integrate surrounding context, despite requiring extensive human annotation, signify attempts to mitigate the impact of distracting content.

% Recently, several work capture existence of noise in the retrieved documents and they emphasize refining the retreived documents. For instance, RECOMP presents a method of compressing retrieved documents into concise summaries, employing both extractive and abstractive techniques, to reduce computational load and improve the model's focus on relevant information. Similarly, FilCo proposes a method to refine the quality of context by identifying useful content and employing context filtering models that operate at test time, demonstrating improvements across various tasks. LongLLMLingua introduces prompt compression to address the challenges faced by LLMs in long context scenarios, optimizing the model's performance by enhancing its focus on key information.

% 현재 접근들의 한계 : Plugin-Model 훈련으로 대응... + Passage Level

% However, while these approaches mark significant progress, they also underscore persistent limitations. Many solutions focus on optimizing at the passage level, potentially overlooking the nuances of sentence-level relevance that can critically influence model performance. Moreover, the reliance on additional plugin models or specialized training regimens introduces complexity and may limit the scalability and adaptability of RAG models across diverse tasks and datasets. As such, there remains a pressing need for more holistic and flexible approaches that can dynamically adapt to the specificities of the task at hand, ensuring the effective and efficient use of retrieved information in enhancing model output.







% 제안하고자 하는 방향성 : LLM 만의 성능으로도 충분히 이를 대응 가능하다고 생각 + Sentence Level
% starsuzi43: 이 부분은 다음 단락으로 넘겨서 설명해도 좋을듯합니다

% Mar 12, 2024 8:49 PM
% starsuzi43: In this work,  어쩌고

% 따라서, 우리는 "LLM Ranking Model"을 가지고 Sentence Level refinement를 통해 RAG 시스템이 가지고 있는 한계점에 대응하고자 함.
% concept figure 처럼 우리꺼가 돌아간다.
% This work addresses the critical problem of filtering out irrelevant sentence-level context from documents retrieved by RAG models. Traditional approaches have predominantly focused on passage-level refinement, which, while useful, may overlook the nuances at the sentence level that contribute to the overall noise in the retrieved data. Our objective is to enhance the precision of context used for QA tasks by developing a novel approach that employs Large Language Models (LLMs) as re-rankers. This methodology aims to refine the retrieved context on a sentence-by-sentence basis, employing zero-shot learning techniques to eliminate the need for extensive task-specific training data.

%LLMs의 일반화 성능을 사용하여..


% This innovative method is designed to strategically filter and refine the context provided by RAG models, ensuring that only the most relevant and supportive sentences are utilized for the generation tasks. By employing LLMs as re-rankers in a zero-shot learning framework, our approach circumvents the need for extensive task-specific datasets, making it both efficient and scalable across various QA tasks. The conceptual framework, illustrated in our proposed figure, demonstrates the operational flow of our approach, highlighting how LLMRanking effectively streamlines the refinement process. This methodology not only significantly reduces the noise and irrelevancy in the retrieved context but also enhances the overall precision and reliability of the RAG model outputs, marking a substantial improvement over traditional passage-level refinement techniques.

% Consequently, we introduce the Large Language Model Ranking (LLMRanking) approach, leveraging zero-shot learning to address the limitations of current RAG systems through sentence-level refinement. This innovative method is designed to strategically filter and refine the context provided by RAG models, ensuring that only the most relevant and supportive sentences are utilized for the generation tasks. By employing LLMs as re-rankers in a zero-shot learning framework, our approach circumvents the need for extensive task-specific datasets, making it both efficient and scalable across various QA tasks. The conceptual framework, illustrated in our proposed figure, demonstrates the operational flow of our approach, highlighting how LLMRanking effectively streamlines the refinement process. This methodology not only significantly reduces the noise and irrelevancy in the retrieved context but also enhances the overall precision and reliability of the RAG model outputs, marking a substantial improvement over traditional passage-level refinement techniques.



% 실험 셋팅, 실험 결과, 분석에 대한 설명.

% We validate our ~~~ using ~~~ benchmark datasets. 실험 결과 높다는 것을 보였음.
% Our contributions are manifold and significant. We introduce a novel application of LLMs for context refinement, leveraging their capabilities to improve the granularity and precision of the information retrieval process. Through extensive validation across diverse QA datasets, including Open Domain QA datasets and specialized datasets for long contexts, we demonstrate the robustness and versatility of our approach. Furthermore, our work provides valuable insights into the performance of different LLM architectures in the context of zero-shot learning for information retrieval, offering practical guidance for future research and applications.

% Contributions are three folds~

% Previous work in this area has explored various strategies for context retrieval and refinement, including the development of sophisticated retriever and reranker models. However, these approaches often require substantial task-specific data for training and may not efficiently handle the nuances of sentence-level context. Our work addresses these limitations by proposing a zero-shot learning approach that significantly enhances the adaptability and scalability of context refinement processes.

% This paper is organized as follows: Section 2 provides a review of related work, highlighting the advancements and limitations in the field of RAG models and context refinement. Section 3 details our methodology, including the design and implementation of the LLM reranker for sentence-level context refinement. Section 4 presents a comprehensive evaluation of our approach across various datasets and benchmarks, followed by a discussion of the results in Section 5. Finally, Section 6 concludes the paper with a summary of our findings and suggestions for future research directions.

