\section{Related Work}


% 최근에 LLM이 처리할 수 있는 maximum tokens가 4096으로 제한되어 있어 그 이상의 long context를 Retrival을 통해서 다루려는 연구도 이루어지고 있다. 기본적으로 RAG는 retrieval를 통해 fetch된 정보를 reader가 사용하게 되어 Retriever의 성능에 크게 전체적인 성능이 의존된다. 하지만 retrieval은 imperfection 하기 때문에 관련있는 문서와 없는 문서가 동시에 들어오게 되는데 LLM은 reader로써 irrlevant한 문서나 noise에 취약하여 정답을 생성하는데 어려움을 겪는 문제가 발생한다. 게다가 LLM에게 많은 정보를 context를 줘서는 중간 정보를 제대로 활용할 수 없는 문제가 발생하여 본질적으로 주어지는 문서의 precision을 높이는 방법이 연구되어야 한다. 따라서 본 연구는 기존 retrieval system을 단계를 개선하여 RAG의 전체적인 퍼포먼스를 올리는 방법을 연구한다.

% However, RAG faces challenges such as retrieving irrelevant information. 
% Previous works have attempted to solve the 
% Our work specifically focuses on enhancing the retriever, which is a critical component of RAG models. Current retrieving strategies often face limitations such as retrieving overly broad or irrelevant information, failing to 

%RAG combines retrieval-based and generative mechanisms, allowing models to access relevant external information for given queries.

% lost in the middle

\paragraph{Information Retrieval.}
Information Retrieval (IR) is the task of searching for query-relevant documents from a large corpus \cite{IR}, which has been widely applied for both search systems and various NLP tasks such as open-domain QA ~\cite{KILT}.
% IR is essential not only for developing search systems but also for various NLP tasks such as open-domain QA.
IR models can be categorized into sparse retrievers \cite{tfidf, bm25}, which use lexical metrics to calculate relevance scores between queries and documents, and dense retrievers \cite{DPR-karpukhin, contriever}, which embed queries and documents into a dense space that captures semantic relationships but requires significant computational resources \cite{dar}.
 
% IR models can be categorized into either sparse or dense retrieval models, depending on their representations.
% Specifically, sparse retrievers \cite{tfidf, bm25}, which represent text lexically and calculate relevance based on lexical metrics such as unique word counts, are widely utilized for their efficiency, but they often fail to capture semantic relationships.
% On the other hand, dense retrievers, which embed query-document pairs into a learnable dense embedding space, have recently gained significant attention for their ability to capture semantic information \cite{DPR-karpukhin, contriever}, but training them requires substantial computing resources~\cite{dar}.

In order to further enhance retrieval performance, additional strategies have been proposed.
Specifically, the re-ranking strategy improves retrieval performance by recalculating relevance scores using an additional re-ranking model \cite{bert-rerank, monot5, rankt5}, and then reordering the documents based on these scores.
Recently, LLMs have shown remarkable re-ranking performance by generating relevance labels without requiring further fine-tuning \cite{holistic, prp}.


While the aforementioned work on IR \cite{100words, DPR-karpukhin} generally assumes fixed-size, 100-word passages as the document length, some work has explored an optimal level of retrieval granularity~\cite{phrase, phrase_retriever,proconvqa,denseXretrieval}.
These approaches validate that a fine-grained level of granularity, containing only the knowledge needed to answer the query, can enhance the overall performance by excluding redundant details in the lengthy retrieved documents. 
However, reducing retrieval granularity to the sentence level can disrupt the original context and result in a loss of the document’s coherence \cite{decontextualization}.
In addition, sentence-level retrieval generally requires a much larger index size compared to passage-level retrieval \cite{DBLP:conf/emnlp/LeeWC21}.
By contrast, we investigate a novel framework for effectively re-ranking sentences within retrieved passage-level documents and then reconstructing the re-ranked sentences to preserve contextual integrity.
% Information Retrieval (IR) involves searching for and retrieving relevant documents for a given query from large corpora. 
% IR is not only essential for search systems but also crucially applied to various NLP tasks such as Open-Domain Question Answering (QA) and Retrieval-Augmented Generation (RAG). 
% Traditionally, sparse retriever models, which calculate relevance based on lexical values like unique word counts, have been widely utilized for their effectiveness and efficiency in IR. 
% Recently, dense retriever models, which embed query-document pairs into a dense embedding space to capture semantic similarities, have begun to replace sparse models.
% However, developing an excellent dense retriever model requires a significant amount of training data and computing resources. It also has the fatal limitation of slower latency compared to sparse retrievers.


% Instead of improving the performance of dense retrievers, studies have been proposed to increase the precision of retrieval results in other ways while using existing retrievers.
% Re-ranking the retrieved documents using specialized ranking models is an effective solution, enhancing retrieval performance compared to the original order. Several approaches propose supervised re-rankers fine-tuned with question-document pairs to generate rich relevance scores. Recently, large language models (LLMs) have shown the potential for unsupervised ranking models without additional fine-tuning by generating specific labels to distinguish relevancy between pairs. Furthermore, exploring the optimal level of retrieval granularity has shown significant improvements in retrieval precision, especially when retrieved results are prepended to reader models. These approaches validate that a fine-grained level of granularity, containing only the knowledge needed to answer the query, is effective by excluding extraneous details in documents.

% Relying on single retrieval models has limitations in improving the precision of the retrieved documents.
% Re-ranking the retrieved documents using specialized ranking models is one effective solution by enhancing the retrieval performance compared to the original order.
% Several approaches proposed the supervised re-rankers fine-tuned with the question-document pairs to generate rich relevance scores between them.
% Recently, LLMs have shown the possibility of unsupervised ranking models without additional fine-tuning by generating specific labels distinguishing relevancy between pairs.
% In other lines of work, exploring the optimal level of retrieval granularity shows remarkable enhancements in retrieval precision, especially when retrieved results are prepending to the reader models. 
% These approaches validate that a fine-grained level of granularity, containing only the knowledge needed to answer the query, is effective via excluding extraneous details in documents.



% The effectiveness of the RAG systems depends heavily on the effectiveness of their retrieval component. Over time, retrieval techniques have evolved from traditional lexical methods like BM25 to sophisticated dense retrieval systems that capture semantic meanings. 
% Despite advancements, challenges in pinpointing the most relevant documents persist, leading to the adoption of hybrid retrieval methods that merge sparse and dense techniques. 
% Recent innovations in document reranking—evaluating and refining document relevance post initial retrieval—have notably improved accuracy. 
% However, these systems typically handle fixed-length units, such as 100-word passages, which may contain irrelevant information. 
% This limitation has spurred research into reducing granularity to phrases or sentences, aiming to enhance relevance and specificity. 
% While passage-level retrieval targets smaller text segments within documents, enhancing focus and reducing unrelated content, it may still include extraneous details or miss crucial context. 
% Conversely, sentence-level retrieval offers higher granularity by focusing on individual sentences, thus enhancing specificity but potentially disrupting contextual continuity. 
% This finer granularity increases index sizes, raising computational costs and storage requirements. 
% To address these challenges, our methodology integrates coarse-grained passage retrieval with fine-grained sentence-level reranking, aiming to balance specificity with contextual coherence effectively.

\paragraph{Retrieval-Augmented Generation.} RAG has emerged as a promising solution for addressing LLMs' hallucination issues by leveraging external knowledge fetched by the retrieval module. 
Specifically, RAG incorporates retrieval modules that reduce the need to update the parameters of LLMs and help them generate accurate and reliable responses~\cite{DBLP:conf/iclr/KhandelwalLJZL20, RAG, DBLP:conf/icml/BorgeaudMHCRM0L22, replug}. 
%RAG has demonstrated superior performance in diverse knowledge-intensive tasks such as Open-Domain Question Answering~\cite{KILT}.
Additionally, various real-world applications integrate RAG as a core component when deploying LLM-based services~\cite{plugin, langchain, Toolllm}. % real world에 쓰는 것, tool
However, they still have limitations due to the imperfections of the retrieval module within RAG, where the retrieved documents containing query-irrelevant information can negatively lead the LLMs to generate inaccurate answers.
% how is it imperfect?
%which can negatively affect the overall performance of RAG.%which lead to distractions that affect the performance of LLMs. %retriever's imperfections within RAG systems and the resulting distractions to LLMs caused by these imperfections.

% Retrieval-augmented generation (RAG) has emerged as a promising solution for handling LLMs' hallucination issue by exploiting the external knowledge fetched by the retriever module.
% RAG has shown its superior performance across not only diverse knowledge-intensive tasks~\cite{} but also long-context understanding tasks~\cite{}.
% Also, various real-world applications integrate RAG as a core component when deploying LLM-based services~\cite{}.
% However, they come with limitations arising from the imperfection of the retriever within RAG systems and the distraction of LLMs to such imperfection.

%To address them, several approaches have attempted to leverage the capabilities of LLMs to enhance their resilience to irrelevant knowledge. 
To address them, several studies have attempted to leverage the capabilities of LLMs to enhance their resilience against irrelevant knowledge. 
These approaches include crafting specialized prompts~\cite{self-ask, das}, training plug-in knowledge verification models~\cite{kalmv}, adaptively retrieving the required knowledge~\cite{adaptive-rag, selfrag, ReFeed}, and augmenting knowledge using the capabilities of the LLM itself \cite{gen_read}.
%, and prepending fine-grained contexts to LLMs~\cite{recomp}. 
%Among the promising solutions, refining the initially retrieved documents into fine-grained knowledge has improved performance and efficiency without requiring multiple query iterations~\cite{recomp, REAR, filco, bider}.
Among the promising solutions, recent studies show that further refining the retrieved documents into fine-grained knowledge can improve the RAG performance~\cite{recomp, REAR, filco, bider}.
However, such refinement strategies generally require additional fine-tuning on a specific dataset, which might result in limited generalizability and high computational cost. 
By contrast, our proposed refinement framework removes irrelevant information with unsupervised sentence-level re-ranking and reconstruction steps by using off-the-shelf ranking models without requiring additional training costs.

% more efficient methods based on unsupervised approaches could still be proposed.

% To address these issues, several approaches have attempted to leverage the capabilities of large language models (LLMs) to enhance their resilience to irrelevant knowledge. These approaches include crafting specialized prompts, training plug-in models, employing adaptive retrieval techniques, and prepending fine-grained contexts to LLMs~\cite{}. 
% Among the promising solutions, refining initially retrieved documents into fine-grained knowledge has been demonstrated to be effective in performance improvement and efficient without requiring multiple query iterations~\cite{}. 
%fine grained 해서 전달해주는 VIPER
% This approach is particularly efficient as it necessitates only a single processing turn by the LLM rather than multiple query iterations.
% However, since additional data collection and model training are still required, more efficient methods based on unsupervised approaches could still be proposed.

% ICLR나왔던 , ,adaptive retrieavl
% Occasional failures by retriever models to obtain relevant documents can lead to the inclusion of irrelevant knowledge, misleading LLMs and resulting in responses that are not accurately grounded in relevant information.

% Recent advancements in QA systems have benefited from integrating Retrieval-Augmented Generation (RAG) systems. 
% These systems incorporate retrieval processes that alleviate the need for frequent updates to LLMs and help overcome their token processing limitations. 
% A typical RAG system comprises a retriever and a reader, with LLMs often serving as readers. 
% The efficiency of the retriever is crucial as it fetches the information utilized by the reader. However, retrieval processes often yield relevant and irrelevant documents, posing significant challenges. 
% The presence of irrelevant documents can lead LLMs to generate inaccurate answers due to the noise in the retrieved data. 
% Furthermore, providing LLMs with extensive context can lead to the ineffective use of intermediary information, underscoring the necessity for focused research on improving document precision. 
% Consequently, efforts have been made to reduce irrelevant content in retrieved documents while preserving pertinent information. 
% However, these often require additional training of additional models or readers tailored to specific tasks. 
% This study aims to enhance the overall performance of RAG systems by utilizing existing document reranking models to refine the retrieval system, thereby addressing the retrieval of overly broad or irrelevant information prevalent in current strategies.


% The RAG systems heavily depends on the effectiveness of their retrieval component. Retrieval techniques have evolved from traditional methods such as BM25, which primarily utilize lexical features, to more sophisticated dense retrieval systems capable of capturing semantic meanings. Despite these advancements, challenges remain in accurately identifying the most relevant documents, prompting the adoption of hybrid approaches that combine sparse and dense retrieval methods. Moreover, Recent innovations have introduced document rerankers that evaluate and refine the relevance of documents after an initial lightweight retrieval phase, significantly enhancing the retrieval system's accuracy. This evolution in retrieval strategies has improved system capabilities, but most still handle fixed-length units, such as 100-word passages. Notably, passages may contain irrelevant information, motivating research into reducing granularity to phrases or sentences to enhance relevance and specificity in retrieved content. These retrieval units, ranging from entire documents to smaller text segments like sentences, significantly influence the quality of the generated output. Passage-level retrieval targets smaller text segments within a document, providing a more focused approach than whole-document retrieval and reducing the inclusion of unrelated information while maintaining coherence. However, this approach can still include irrelevant details or omit essential context outside the retrieved passage. To address these shortcomings, sentence-level retrieval has been explored, offering the highest granularity by concentrating on individual sentences, which allows for the retrieval of specific and relevant information. Nevertheless, this method often results in a lack of contextual continuity, leading to fragmented or incomplete information retrieval. Moreover, the finer granularity of retrieval increases the index size, thus raising computational costs and storage requirements, and necessitates additional preprocessing even using models, further increasing expenses. To overcome the limitations of passage and sentence-level retrieval, our methodology focuses on an integrated approach that combines coarse-grained passage retrieval with fine-grained sentence-level reranking. This strategy leverages the strengths of both passage and sentence-level retrieval to achieve an optimal balance between specificity and coherence.

% In RAG, retrieval의 성능에 크게 의존하게 된다. Retriever는 lexical feature를 쓰는 tradition한 BM25부터 semantic meaning을 captuer할 수 있는 Dense retrieval까지 발전해왔다. 하지만 이러한 발전에도 여전히 더 relevance document를 찾아내는 것에 한계가 있어 hybrid apporach로 sparse와 dense retriever를 함께 고려하는 방법을 쓰거나 query와의 비교적 가벼운 inital retrieval 이후에 query와의 relevance를 평가하여 이를 ranking할 수 있는 document reranker들이 등장하였다. 이들의 등장으로 retrieval system이 발전하였으나 여전히 이들은 fixed length, such as 100 words의 passage unit만 다뤘다.  하지만 passage에는 반드시 query와 관련된 내용만 포함되어있는 것은 아니다. 그래서 retrieval units의 phrase나 sentence로 granalarity를 더 낮추는 연구들이 진행되었다.\cite{}These retrieval units can range from entire documents to smaller text segments such as passages or sentences \cite{}. The choice of retrieval units impacts the relevance and specificity of the retrieved information, which in turn affects the quality of the generated output. Passage-level retrieval focuses on smaller segments of text within a document. This unit provides a more targeted approach than the whole document, reducing the inclusion of unrelated information while maintaining the coherence. However, it may still include irrelevant information or miss critical context that lies outside the retrieved passage \cite{}. To avoid from the irrelevant information being fetched, sentence-level retrieval also researched. Sentence-level retrieval offers the highest granularity by focusing on individual sentences, allowing for specific and relevant information retrieval. Yet, sentence-level retrieval often results in a lack of contextual continuity, leading to fragmented or incomplete information retrieval \cite{choi-etal-2021-decontextualization}. 그리고 이런 작은 단위의 retrieval은 Index가 커서 computational cost와 storage space가 늘어나게 되고 전처리 하는 과정에서 추가적인 모델에 의존하여 cost가 발생한다. To overcome limitations of the passage and sentence-level retrieval, our methodology focuses on integrated coarse-grained passage retrieval and fine-grained sentence-level reranking (reranking in reranking subsection?). This approach combines the strengths of passage and sentence-level retrieval to balance specificity and coherence.

% \noindent\textbf{Passage Reranking and Refinement}
% 부족한 retrieval 능력을 
% \cite{qin-etal-2023-large}

