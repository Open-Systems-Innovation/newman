Chunk 1:
\@ACM@checkaffil{
    @ACM@instpresent
    {No institution present for an affiliation}

@ACM@citypresent
    {No city present for an affiliation}

@ACM@countrypresent
        {No country present for an affiliation}

}

{
  {{
     B-0.5em{ i-0.25em b}-0.8em}}}

{

}
    @nvidia.com}

==================================================

Chunk 2:
try present for an affiliation}

}

{
  {{
     B-0.5em{ i-0.25em b}-0.8em}}}

{

}
    @nvidia.com}Enterprise chatbots, powered by generative AI, are rapidly emerging as the most explored initial applications of this technology in the industry, aimed at enhancing employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), Langchain/Llamaindex types of LLM orchestration frameworks serve as key technological components in building generative-AI based chatbots. However, building successful enterprise chatbots is not easy. They require meticulous engineering of RAG pipelines. This includes fine-tuning semantic embeddings and LLMs, extracting relevant documents from vector databases, rephrasing queries, reranking results, designing effective prompts, honoring document access controls, providing concise responses, including pertinent references, safeguarding personal information, and building agents to orchestrate all these activities. In this paper, we present a framework for building effective RAG-based chatbots based on our first-hand experience of building three chatbots at NVIDIA: chatbots for IT and HR benefits, company financial earnings, and general enterprise content. Our contributions in this paper are three-fold. First, we introduce our FACTS framework for building enterprise-grade RAG-based chatbots that address the challenges mentioned. FACTS mnemonic refers to the five dimensions that RAG-based chatbots must get right - namely content reshness (F), rchitectures(A), ost economics of LLMs  (C), esting (T), and ecurity (S). Second, we present fifteen control points of RAG pipelines and techniques for optimizing chatbots’ performance at each stage. Finally, we present empirical results from our enterprise data on the accuracy-latency tradeoffs between large LLMs vs small LLMs.  To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.

==================================================

Chunk 3:
s a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.Chatbots are increasingly becoming an extension of search tools in companies for finding relevant information. Whether it's HR benefits, IT help, sales queries, or engineering issues, enterprise chatbots are now go-to productivity tools. Before the debut of OpenAI's Chat-GPT~ in November 2022, companies relied on internally developed chatbots  based on dialog flows. Such bots required extensive training for intent understanding and meticulous orchestration for response generation and yet could only provide extractive answers at best. These early bots, built on dialog management systems paired with information retrieval and question answering (IRQA) solutions were fragile and limited in capability. While previous generation language models and GPT models existed, they lacked the accuracy, robustness, and reliability needed for broad enterprise use~.

==================================================

Chunk 4:
els existed, they lacked the accuracy, robustness, and reliability needed for broad enterprise use~.Chat-GPT's release, the emergence of vector databases, and the wide-spread use of retrieval augmented generation (RAGs)~ marked the beginning of a new era in Chatbot domain. Now, LLMs can understand user intents with simple prompts in natural language, eliminating the need for complex intent variant training, synthesize enterprise content coherently, thereby empowering chatbots with conversational capability beyond scripted intent recognition. While LLMs bring their generative capabilities to construct coherent, factual, and logical responses to user queries, vector database-powered information retrieval (IR) systems augment LLMs ability to retrieve fresh content. Tools like LangChain~ and Llamaindex~ facilitate chatbot construction, and orchestration of complex workflows including memory, agents, prompt templates, and overall flow. Together, vector-search based IR systems, LLMs, and LangChain-like frameworks form core components of a RAG pipeline and are powering generative AI chatbots in post Chat-GPT era.

==================================================

Chunk 5:
form core components of a RAG pipeline and are powering generative AI chatbots in post Chat-GPT era.[tp!]

{c|c|c|c|c|c|c}

Chatbot & Domain & {Data Sources}  & {Data Types}  &   {Access Control}  & {Sample Queries}  & {State}  \\  
 &    & }  &  & Yes &  &   \\  
  &   &   &    & Yes &    &  Production\\  
 &  Financial Earnings &   &   & No &  & Production \\

==================================================

Chunk 6:
&   &   &    & Yes &    &  Production\\  
 &  Financial Earnings &   &   & No &  & Production \\At NVIDIA, our main motivation was to improve our employee productivity by building enterprise chatbots. Our initial enthusiasm quickly met with the reality of addressing numerous challenges. We learned that crafting a successful enterprise chatbot, even in post Chat-GPT era, while promising, is not easy. The process demands meticulous engineering of RAG pipelines, fine-tuning LLMs, and engineering prompts, ensuring relevancy and accuracy of enterprise knowledge, honoring document access control permissions, providing concise responses, including pertinent references, and safeguarding personal information. All of these require careful design, skillful execution, and thorough evaluation, demanding many iterations. Additionally, maintaining user engagement while optimizing for speed and cost-efficiency is essential. Through our journey, we learned that getting an enterprise conversational virtual assistant right is akin to achieving a perfect symphony where every note carries significance!

==================================================

Chunk 7:
rtual assistant right is akin to achieving a perfect symphony where every note carries significance!In this paper, we share our experiences and strategies in building effective, secure, and cost-efficient chatbots. We answer the following questions from a practitioner perspective:

We present our findings from trying to deliver }resh content (F) with flexible }rchitectures (A) that are }ost-efficient (C), }ested well (T), and }ecure (S) - (FACTS).  
  We present the fifteen control points of RAG pipelines and techniques for optimizing each control point and the overall RAG pipeline.

==================================================

Chunk 8:
ints of RAG pipelines and techniques for optimizing each control point and the overall RAG pipeline.Our company's content landscape includes both authoritative knowledge and unauthoritative content. Authoritative content encompasses IT help articles, HR resources in platforms like ServiceNow, and project documentation on Confluence, SharePoint, Google Drive, and engineering tools like NVBugs and GitHub. Employee-generated content complements these sources on platforms such as Slack and MS Teams. In this paper, we present three bots that we have built at NVIDIA using RAGs and LLMs. These bots are briefly introduced below. All three bots are built on our in-house built generative-AI chatbot platform called NVBot platform. Some of the queries that our bots are capable of answering are shown in Table~.

==================================================

Chunk 9:
lled NVBot platform. Some of the queries that our bots are capable of answering are shown in Table~.answers questions about enterprise content (approx. 500M documents of size > 7 TB), complementing intranet search. It manages diverse data formats and enforces document access controls. The tech stack includes LangChain, a vendor vector database for retrieval and to handle document access controls, LLM model (multiple LLM models can be selected), and a custom web-UI.
      Bot focuses on IT help and HR benefits (approx. 2K multi-modal documents containing text, tables, images, pdfs, and html pages), using a similar tech stack to NVInfo bot with a smaller data volume.
      handles questions about financial earnings from public sources, managing structured and unstructured data (approx. 4K multi-modal documents containing text, tables, pdfs, and html pages). The tech stack includes an Open source Vector DB, LangChain, Ragas evaluation, selectable LLM models, and a custom web-UI.

==================================================

Chunk 10:
s an Open source Vector DB, LangChain, Ragas evaluation, selectable LLM models, and a custom web-UI.In the remainder of the paper, we present our FACTS framework that summarizes the challenges experienced and the learnings gained in building the aforementioned three chatbots. We first start with the challenge of dealing with delivering fresh enterprise content in each of the chatbots.

[ht]

[h]

Ensuring the freshness of enterprise data in LLM-powered chatbots presents several challenges. Foundation models, although powerful, often fall short as they lack domain-specific and enterprise-specific knowledge. Once trained, these models are essentially frozen in time and may hallucinate, providing undesired or inaccurate information when used on enterprise content that they are not trained on.

==================================================

Chunk 11:
ng undesired or inaccurate information when used on enterprise content that they are not trained on.Retrieval Augmented Generation (RAG) is a process where relevant information is retrieved from vector databases through semantic matching and then fed to LLMs for response generation. In a RAG pipeline, vector databases and LLMs collaboratively ensure the delivery of up-to-date enterprise knowledge. However, RAG pipelines have many control points, each of which when not tuned well can lead to lower accuracy, hallucinations, and irrelevant responses by Chatbots. Additionally, document access control permissions complicate the search and retrieval process, requiring careful management to ensure data security and relevance. Furthermore, multi-modal content necessitates the use of multi-modal retrievers to handle structured, unstructured, and semi-structured data, including presentations, diagrams, videos, and meeting recordings. Addressing these challenges is critical for maintaining the accuracy and reliability of enterprise chatbots. Inspired by , we identify fifteen control points of RAG from our case studies visualized in Figure~. Each control point is labeled with a number. In the remainder of this section, we present our insights and learnings for addressing RAG control points.

==================================================

Chunk 12:
remainder of this section, we present our insights and learnings for addressing RAG control points.In figure~, we present a summary description of the fifteen control points of RAG pipelines, challenges associated with each control point, and our suggested approaches for optimizing each control point. Each control point is labeled as RAG-C[num] and RAG-Op[num] for RAG and RAGOps flows, respectively. Below, we present a few key learnings and insights to manage the fresh enterprise content.

==================================================

Chunk 13:
ectively. Below, we present a few key learnings and insights to manage the fresh enterprise content.: We noticed that metadata enrichment, chunking, query rephrasal and query re-ranking stages of RAG pipeline have the most impact on the quality of Chatbot responses. LLM response generation quality is highly dependent on retrieval relevancy. Retrieval relevancy is, in turn, highly dependent on document metadata enrichment, chunking, and query rephrasal. We implemented grid search-based auto-ML capabilities to find the right configurations of chunk token-sizes, experimented with various prompt variations, and explored different chunk reranking strategies to find optimal settings for each. While we have made significant improvements in retrieval relevancy and answer quality and accuracy, 
we believe, we still have more work to do to optimize the full pipeline.

==================================================

Chunk 14:
swer quality and accuracy, 
we believe, we still have more work to do to optimize the full pipeline.: We noticed that Vector databases are not so good at handling matching entities (e.g., people names, places, company names etc.). Using a combination of Lexical search (e.g., elastic search) and vector search provided better retrieval relevancy and more coverage. Setting up an infrastructure that supports hybrid search capabilities, which combines the strengths of both lexical and vector-based searches, can enhance the accuracy and speed of the retrieval process.

[tp!]

==================================================

Chunk 15:
xical and vector-based searches, can enhance the accuracy and speed of the retrieval process.

[tp!]:  Questions such as ‘compare the revenue of NVIDIA from Q1 through Q4 of FY2024 and provide an analytical commentary on the key contributing factors that led to the changes in revenues during this time’ require complex agents that are capable of query decomposition and orchestration. Figure~ shows one mechanism we had implemented to deal with such questions in Scout bot. From our experience of building the three bots, we have realized that IR systems and LLMs are insufficient for answering complex queries. Complex agents and multi-agent architectures are needed to handle complex queries.

==================================================

Chunk 16:
complex queries. Complex agents and multi-agent architectures are needed to handle complex queries.A key decision is whether to fine-tune LLMs, balancing the use of foundational models with domain-specific customizations. One size doesn’t fit all when it comes to LLMs. Some use cases may work well with foundational models, while others require customization. When considering customization, several options are available, including prompt engineering, P-tuning, parameter-efficient fine-tuning (PEFT), and full fine-tuning (FT). Fine-tuning requires significant investment in data labeling, training, and evaluations, each of which can be time-consuming and costly. Automating testing and quality evaluation processes become critical to ensuring efficiency and accuracy when customizing LLMs. Figure~ shows the accuracy vs latency tradeoff evaluations we have done comparing OpenAI’s GPT-4 model with some of the open-source models on about 245 queries from NVHelp bot domain. Our results show that the Llama3-70B model excels in several aspects of answer quality while maintaining acceptable latency.

==================================================

Chunk 17:
e Llama3-70B model excels in several aspects of answer quality while maintaining acceptable latency.[tp!]

: Enterprise data is multi-modal. Handling structured, unstructured, and multi-modal data is crucial for a versatile RAG pipeline. From our experience, if the structure of the document is consistent and known apriori (like those found in EDGAR databases for SEC filings data in financial earnings domain that Scout bot was handling), implementing section-level splitting, using the section titles and subheadings and incoporating those in the context of chunks improves retrieval relevancy. We also found solutions like Unstructured.io, which specialize in extracting and structuring content from PDFs, helpful in parsing and chunking unstructured documents with context.

==================================================

Chunk 18:
structuring content from PDFs, helpful in parsing and chunking unstructured documents with context.: Effective health monitoring of RAG pipelines is essential once they are deployed. When answer quality is poor, a thorough error analysis is required to determine whether the issue lies in retrieval relevancy or LLM response generation. To debug retrieval relevancy, developers need detailed information on which chunks were stored in vector databases with their associated metadata, how queries were rephrased, which chunks were retrieved, and how those chunks were ranked. Similarly, if an LLM response is incorrect, it is crucial to review the final prompt used for answer generation. For issues with citations, developers must trace back to the original document links and their corresponding chunks. RAGOps/LLMOps and evaluation frameworks, such as Ragas, are critical for providing the necessary automation to enable rapid iteration during accuracy improvement cycles in RAG pipelines.

==================================================

Chunk 19:
necessary automation to enable rapid iteration during accuracy improvement cycles in RAG pipelines.More details on each control point are presented in Figure~. In summary, while promising, implementing RAG systems for chatbots demands meticulous planning and continuous evaluation to ensure secure and accurate data retrieval.

[t]

[t]

[h]

Keeping up with rapid progress in AI is like navigating a fast-moving river. Every aspect, from vector databases and embedding models to LLMs, agentic architectures, low-code/no-code platforms, RAG evaluation frameworks, and prompting techniques, is evolving rapidly. Concurrently, departments within companies are exploring generative AI by building their own chatbots and AI copilots.

==================================================

Chunk 20:
rtments within companies are exploring generative AI by building their own chatbots and AI copilots.In this dynamic environment, building common, flexible, and adaptive platforms are crucial. At NVIDIA, our chatbot ecosystem has grown significantly, reflecting a trend likely seen in many companies. From building three initial chatbots, we realized the importance of a common platform to avoid duplicated efforts in security, guardrails, authentication, prompts, user interfaces, feedback mechanisms, usage reporting, monitoring, and evaluations.

==================================================

Chunk 21:
cation, prompts, user interfaces, feedback mechanisms, usage reporting, monitoring, and evaluations.To address this, we developed the NVBot platform (Figure~), a modular platform with a pluggable architecture. It allows developers to select LLMs, vector databases, embedding models, agents, and RAG evaluation frameworks that best suit their use case. It also provides common components for essential features like security, guardrails, authentication, authorization, user experience, and monitoring. Additionally, the platform supports citizen development, allowing multiple teams to contribute their tested prompts, workflows, guardrails, and fine-tuned models for collective use.

==================================================

Chunk 22:
to contribute their tested prompts, workflows, guardrails, and fine-tuned models for collective use.As our ecosystem of bots expanded, we faced a critical question: should organizations build many domain-specific bots, a single enterprise bot, or go with a hybrid approach? Domain-specific chatbots excel in tailored environments, while nterprise-wide chatbots act as generalists, providing a centralized knowledge base for all employees. Through our experience, we realized that there is no need to choose one over the other.

==================================================

Chunk 23:
l employees. Through our experience, we realized that there is no need to choose one over the other.Novel architectural patterns are emerging where enterprise-wide chatbots act as `switchboards', directing inquiries to specialized bots tuned with domain-specific data. This multibot architecture allows for the concurrent development of specialized chatbots while providing users with a unified interface. Our NVBot platform supports the coexistence and orchestration of multiple chatbots within an enterprise. The debate over a single bot or multiple specialized bots is ongoing. We envision a landscape where domain-specific bots coexist with a centralized information bot, supported by 'copilots'—generative AI capabilities integrated into workplace environments like programming IDEs and collaboration tools. At NVIDIA, we're actively exploring all three chatbot variations—domain-specific, enterprise-wide, and copilot as generative AI reshapes workplace efficiency and information accessibility.

==================================================

Chunk 24:
rise-wide, and copilot as generative AI reshapes workplace efficiency and information accessibility.[ht]

{Cost Economics of Chatbot deployments (C)}

Understanding the cost economics of generative AI-based chatbots involves several critical factors. The high costs of major and commercial LLMs can be unsustainable, with expenses adding up significantly across multiple use cases. Additionally, unseen expenses often accumulate as teams test various LLMs to meet specific needs. Moreover, when using commercial LLM vendor APIs, securing sensitive enterprise data requires guardrails to detect and prevent sensitive data leakage, as well as gateways for audit and legally permitted learning. There are also cost versus latency trade-offs to consider, as large LLMs with long context lengths typically have slower response times, impacting overall efficiency.

==================================================

Chunk 25:
e LLMs with long context lengths typically have slower response times, impacting overall efficiency.: Larger, commercial LLMs, smaller open source LLMs are increasingly becoming viable for many use cases, thereby offering cost effective alternatives to companies. As opensource models are catching up with larger, commercial models, they are increasingly offering close-comparable accuracy, as demonstrated in our NVHelp bot emperical evaluation in Figure~, and generally have better latency performance compared to larger models. Additionally, GPU optimization of inference models can further speed up processing times. Open-source models optimized with NVIDIA’s Tensor RT-LLM inference libraries, for instance, have shown faster performance than non-optimized models. These strategies help balance the need for cost-efficiency with maintaining high performance and security standards.

==================================================

Chunk 26:
help balance the need for cost-efficiency with maintaining high performance and security standards.: If you must use a vendor LLM API, it is better to implement an internal company LLM Gateway for audit, subscription and cost management across the company. Implementing an internal company LLM Gateway can streamline LLM usage, subscriptions, and data tracking for security audits. This central hub simplifies management and ensures efficient resource allocation. At NVIDIA IT, we have implemented an LLM Gateway that logs the inbound and outbound payloads for audit purposes and this data is guarded with access control permissions. Our LLM Gateway helps manage the subscriptions and costs of LLM API invocations.

==================================================

Chunk 27:
ontrol permissions. Our LLM Gateway helps manage the subscriptions and costs of LLM API invocations.In summary, developing a hybrid and balanced LLM strategy is essential for managing costs and enabling innovation. This involves using smaller and customized LLMs to manage expenses while allowing responsible exploration with large LLMs via an LLM Gateway. It's crucial to measure and monitor ROI by keeping track of LLM subscriptions and costs, as well as assessing Gen-AI feature usage and productivity enhancements. Ensuring the security of sensitive enterprise data in cloud-based LLM usage requires implementing guardrails to prevent data leakage and building an LLM Gateway for audits and legally permitted learning. Finally, be aware of the trade-offs between cost, accuracy, and latency, customizing smaller LLMs to match the accuracy of larger models while noting that large LLMs with long context lengths tend to have longer response time.

==================================================

Chunk 28:
ger models while noting that large LLMs with long context lengths tend to have longer response time.Testing generative AI solutions can be a lengthy process due to the need for human response validation. LLMs are increasingly being employed using ‘LLM-as-a-judge’ approach. However, it is advisable to use caution when using LLMs as human proxy, as using LLMs as judges can lead to self-fulfilling prophecy type of scenarios reinforcing their inherent biases in evaluations as well.

}: Automating security testing is critical for maintaining development velocity without compromising safety. A strong security framework and regression test datasets ensure that the chatbot remains resilient to potential threats. We are collaborating with our internal RED teams in security to prepare a set of datasets that can be tested with each major iteration of the chatbot.

==================================================

Chunk 29:
n security to prepare a set of datasets that can be tested with each major iteration of the chatbot.}: Generative AI models can be highly sensitive to prompt changes. To maintain accuracy, full regression testing is needed with each prompt alteration.

}: Incorporating feedback gathered and the RLHF cycle is pivotal for continuous improvement. It allows LLM models to refine both our solutions and Language Models over time, ensuring that the chatbot becomes increasingly proficient. However, if the chosen foundational models don’t offer customization, then it becomes difficult to align the models to human feedback. If the feedback is significant and comes in many areas, then model customization may be an option. As of now, we have begun gathering user feedback but haven’t built our continuous learning pipelines using RLHF yet. Having tools to make this automated is critical to pos-production life cycle management of these chatbots.

==================================================

Chunk 30:
tools to make this automated is critical to pos-production life cycle management of these chatbots.: Effective testing of RAG-based chatbots requires anticipation of lengthy test cycles. Begin by focusing on automating tests and enhancing accuracy assessments to streamline this essential phase.

{}: It is crucial to construct comprehensive ground truth datasets that reflect full spectrum of targeted solution strengths. This ensures that the chatbot is tested against scenarios that it will encounter in actual use.

}: While leveraging LLMs as evaluators can provide scalable testing options, remember that the quality of human evaluations is unmatched. Automated tools should be used where feasible to supplement but not replace human oversight.

{}: Establish mechanisms that allow for  human feedback and systematic error analysis. Prioritize iterative improvements based on this feedback to continually refine chatbot performance and adaptability.

{Securing RAG-based Chatbots (S)}

==================================================

Chunk 31:
dback to continually refine chatbot performance and adaptability.

{Securing RAG-based Chatbots (S)}Building trust is paramount when deploying generative AI chatbots. To mitigate risks, guardrails for hallucinations, toxicity, fairness, transparency, and security are critical. Strong foundational models are increasingly getting better at these guardrails. However, there are still many possibilities of jail breaks, adversarial attacks, and other security issues. Apart from these security risks, generative AI-based chatbots are susceptible to derivative risks (explained below). Since our bots are all internal enterprise chatbots, our focus has been more on the enterprise content security and guardrailing for sensitive data. Below we summarize our learnings and insights for securing RAG-based chatbots based on our experience. Addressing these challenges is imperative to maintaining the integrity and security of RAG-based chatbots within corporate environments.

==================================================

Chunk 32:
ative to maintaining the integrity and security of RAG-based chatbots within corporate environments.{}: Enterprise documents are protected by access controls, requiring RAG-based chatbots to comply with Access Control Lists (ACLs) during response generation. To ensure this compliance, we specifically selected an IR product known for its capability to honor these document ACLs effectively.

}: Chatbots might generate responses that lack context from their original data sources, potentially leading to misinterpretations. Additionally, enhanced search methods could inadvertently elevate the risk of exposing sensitive data if enterprise content is inappropriately secured. As part of our NVInfo bot journey, we implemented sensitive data guardrails in addition to leveraging sensitive data filtering and classification capabilities provided by the vector search solution we used to automatically filter out sensitive data during the retrieval.

==================================================

Chunk 33:
the vector search solution we used to automatically filter out sensitive data during the retrieval.}: Efficient knowledge access can increase sensitive data leakage risks. Thus, it's essential to prioritize data governance before deployment to safeguard against unauthorized access and data breaches. At NVIDIA, we embarked on an enterprise content security initiative for  document sensitivity classification and exclusion of sensitive content from chatbots.

==================================================

Chunk 34:
nitiative for  document sensitivity classification and exclusion of sensitive content from chatbots.{}: Implementing guardrails that align generative AI responses with specific enterprise policies and rules is essential. These guardrails help mitigate risks by ensuring that Chatbot-generated content adheres to established norms and ethical guidelines, preventing potential legal and reputational damage. In NVInfo bot, we implemented many guardrails in LLM prompts initially. However, later realized that not all LLMs follow these prompts consistently. Therefore, we implemented these guardrails during pre and post processing of queries and responses respectively using Nemo Guardrails~.

==================================================

Chunk 35:
rdrails during pre and post processing of queries and responses respectively using Nemo Guardrails~.Our work can be compared with RAG papers on various topics dealing with RAG quality along all the FACTS dimensions we presented (freshness, architecture, costs, testing and security). Due to lack of space, we contrast our work with selective works. Barnett~~ presented seven failure points when engineering RAG systems. In their work, they highlight the challenges of getting retrieval augmented generation right by presenting their findings from having built three chatbots. Wenqi Glantz~ elaborated 12 RAG pain points and presented solutions. We experienced similar challenges first-hand when building our chatbots. However, none of these works discuss the challenges with complex queries, testing, dealing with document security, and the need for flexible architectures. In our work, we not only build on failure/pain points of RAGs as mentioned above, but also present our 15 control points in RAG pipelines and offer specific solutions for each stage. Also, we extend our insights and present practical techniques for handling complex queries, testing, and security. We present a reference architecture for one of the implementations of agentic architectures for complex query handling, strategies for testing and evaluating subjective query responses, and raised awareness for dealing with document ACLs and security. Furthermore, we present a reference architecture for a flexible generative-AI based Chatbot platform.

==================================================

Chunk 36:
urthermore, we present a reference architecture for a flexible generative-AI based Chatbot platform.ChipNemo~  presents evidence for using a domain adapted language model for improving RAG’s performance on domain specific questions. They finetuned the e5-small-unsupervised model with 3,000 domain specific auto-generated samples. We tried fine-tuning e5-large embeddings model in Scout Bot. Our results did not demonstrate significant improvements. We are presently collecting high quality human-annotated data to repeat the experiments. This could be an important direction to explore in the future for our work. Another interesting technique was presented by Setty~~, in improving RAG performance using Hypothetical Document Embeddings (HYDE) technique.  HyDE uses an LLM to generate a theoretical document when responding to a query and then does the similarity search with both the original question and hypothetical answer. This is a promising approach  but might make the architecture complex.

==================================================

Chunk 37:
tion and hypothetical answer. This is a promising approach  but might make the architecture complex.Active Retrieval augmented generation (FLARE)~ iteratively synthesizes a hypothetical next sentence. If the generated sentence  contains low-probability tokens,  FLARE would use the sentence as the new query for retrieval and regenerate the sentence. Mialon~~ reviews works for advanced augmented generation methods in language model. Self-refine  builds an agent to improve the initial answer of RAG through iterative feedback and refinement. ReAct~ Agent is widely used for handling the complex queries in a recursive manner.  On the RAG evaluation front, RAGAS~ and ARES~ utilize LLMs as judges and build automatic RAG benchmark to evaluate the RAG system.  Zhu~~ overview the intensive usages of LLM in a RAG pipeline including retriever, data generation, rewriter, and reader. We believe that our work provides a unique perspective on building secure enterprise-grade chatbots via our FACTS framework.

==================================================

Chunk 38:
provides a unique perspective on building secure enterprise-grade chatbots via our FACTS framework.In this paper, we presented our approach to developing effective RAG-based chatbots, highlighting our experiences of building three chatbots at NVIDIA. We outlined our FACTS framework, emphasizing the importance of content freshness (F), architecture (A), LLM cost (C) management, planning for testing (T), and security (S) in creating robust, secure, and enterprise-grade chatbots. We also identified and elaborated on fifteen critical control points within RAG pipelines, providing strategies to enhance chatbot performance at each stage. Furthermore, our empirical analysis reveals the trade-offs between accuracy and latency when comparing large and small LLMs. This paper offers a holistic perspective on the essential factors and practical solutions for building secure and efficient enterprise-grade chatbots, making a unique contribution to the field. 
More work is needed in several areas to build effective RAG-based chatbots. This includes developing agentic architectures for handling complex, multi-part, and analytical queries; efficiently summarizing large volumes of frequently updated enterprise data; incorporating auto-ML capabilities to optimize various RAG control points automatically; and creating more robust evaluation frameworks for assessing subjective responses and conversations.

==================================================

Chunk 39:
and creating more robust evaluation frameworks for assessing subjective responses and conversations.//The NAMES, AFFILIATIONS, and SHORT BIOS of all the organizers
//The MAIN CONTACT organizer’s e-mail and telephone number; and please make the main contact is chosen as the correspondence author in the proposal submission site. 
//The TITLE of the workshop
    ==A maximum of three paragraphs that describe the TOPIC of the workshop, its target AUDIENCE, and its RELEVANCE to SIGKDD.
    ==One paragraph MOTIVATING the workshop (why we should organize it NOW in conjunction with KDD 2024)
//Names of invited speakers, reviewers, and panelists (names can be tentative, but the list is highly desired for proposal consideration)
//The desired LENGTH of the workshop: full-day (~6 hours) or half-day (~3 hours), and estimated number of attendees
//Tentative PROGRAM SKETCH, including tentative descriptions of all workshop components (panels, discussion sessions, poster sessions, invited talks, etc.)
//If you wish to deviate from the suggested deadlines above (see “Important Dates”), a tentative TIMELINE and motivation for why the suggested deadlines are not suitable
//For workshops previously held at KDD or other conferences, details on venue, attendance and number of submissions/accepted papers from past editions
//For new workshops, a list of possible attendees/submissions and/or a justification of the expected attendees/submissions
//A list of possible venues (e.g., mailing lists, social media) to advertise the proposed workshop
//Include the link to the CFPs for paper submission from past editions (for workshops previously held at KDD or other conferences).

==================================================

Chunk 40:
=1

{
 , Xiaojing Li, Binzhu Wang, Yueyang Zhou}, \\
 , Hong Chen Jinshi Zhang, Fei Yu, Zewei Zhao, Song Jin, Renji Gong, Wanqing Xu} \\
 AntGroup
}

{} These authors contributed equally to this work.}
Corresponding author: jihan.hanji@antgroup.com}

==================================================

Chunk 41:
{} These authors contributed equally to this work.}
Corresponding author: jihan.hanji@antgroup.com}In domain-specific applications, GPT-4, augmented with precise prompts or Retrieval-Augmented Generation (RAG), shows notable potential but faces the critical  of performance, cost, and data privacy. High performance requires sophisticated processing techniques, yet managing multiple agents within a complex workflow often proves costly and challenging. To address this, we introduce the PEER (lan, xecute, xpress, eview) multi-agent framework. This systematizes domain-specific tasks by integrating precise question decomposition, advanced information retrieval, comprehensive summarization, and rigorous self-assessment. Given the concerns of cost and data privacy, enterprises are shifting from proprietary models like GPT-4 to custom models, striking a balance between cost, security, and performance. We developed industrial practices leveraging online data and user feedback for efficient model tuning. This study provides best practice guidelines for applying multi-agent systems in domain-specific problem-solving and implementing effective agent tuning strategies. Our empirical studies, particularly in the financial question-answering domain, demonstrate that our approach achieves 95.0\

==================================================

Chunk 42:
articularly in the financial question-answering domain, demonstrate that our approach achieves 95.0\Advanced LLMs like GPT-4, enhanced with engineered prompts or Retrieval-Augmented Generation (RAG), show great potential in handling complex tasks across various domains . However, deploying these models involves a critical  of performance, cost, and data privacy.

While domain-specific applications benefit from meticulously fine-tuned models , this approach incurs high costs due to the extensive resources needed for training and data acquisition. Alternatively, multi-agent systems have proven effective , especially in complex tasks with distinct and conflicting role requirements that challenge even advanced models. However, current implementations often involve dynamic and complex workflows, increasing costs and complicating reproducibility. Consequently, enterprises are shifting from proprietary models like GPT-4 to custom models that better balance cost, security, and performance.

==================================================

Chunk 43:
proprietary models like GPT-4 to custom models that better balance cost, security, and performance.To address these challenges, we introduce the PEER (lan, xecute, xpress, eview) multi-agent framework. This framework incorporates precise question decomposition, advanced information retrieval, comprehensive summarization, and rigorous self-assessment, aiming to streamline workflows and enhance problem-solving efficacy. Additionally, our research addresses enterprise demands for private deployment and stringent data privacy by developing industrial best practices that leverage online data and user feedback for effective model tuning. These practices are crucial for optimizing custom model performance while ensuring cost-efficiency and robust data privacy.

==================================================

Chunk 44:
cial for optimizing custom model performance while ensuring cost-efficiency and robust data privacy.The main contributions of this study include:
1. Providing and open-sourcing the PEER framework, characterized by its conciseness, effectiveness, and cost-efficiency, for effectively tackling domain-specific tasks. In experiments, it demonstrates superior performance compared to BabyAGI.\\
2. Proposing a customized agent tuning strategy for 10-billion-parameter models, achieving performance comparable to GPT-4. \\
3. Constructing and open-sourcing a dataset for use within the PEER framework, applicable to agent training, pre-training, and supervised fine-tuning in various financial analysis scenarios.

==================================================

Chunk 45:
to agent training, pre-training, and supervised fine-tuning in various financial analysis scenarios.With the advent of large models, we simulate the collaborative processes of human experts (e.g. financial) using multiple agents, achieving comparable interpretative results. This approach is encapsulated in the Plan, Execute, Express and Review (PEER) framework, where domain specific (e.g. financial) tasks are divided into these four steps. Each agent specializes in a single task, working together to accomplish the overall objective. The prompt for this section is attached in .

The "Plan" agent uses a model to generate multiple related sub-questions from users’ domain specific (e.g. financial) queries. These sub-questions serve as an interpretation framework, breaking down the original query into specific and actionable criteria, and expanding it for a comprehensive analysis.

==================================================

Chunk 46:
original query into specific and actionable criteria, and expanding it for a comprehensive analysis.The "Execute" agent gathers information for each sub-question identified by "Plan". Using these sub-questions as search criteria, it finds relevant information from news, domain specific (e.g. financial) data, reports, and articles, enhancing accuracy, efficiency, and comprehensiveness. This information forms the foundation for interpreting domain events and answering questions.

The "Express" agent synthesizes collected information to perform comprehensive large-model reasoning, forming final conclusions. It emphasizes integrative reasoning and delivers professional descriptions tailored to the user's requirements.

The "Review" agent evaluates whether the "Express" agent's answer meets pre-established criteria. If satisfied, the final answer is delivered; if not, it provides modification suggestions and initiates another PEER iteration, enhancing answer quality through feedback.

==================================================

Chunk 47:
ication suggestions and initiates another PEER iteration, enhancing answer quality through feedback.The PEER multi-agent cooperation framework’s strong reasoning and analysis abilities stem from its efficient task allocation, cooperation, and the feedback loop and self-optimization enabled by the "Review" agent. This ensures that the answers continuously improve towards the optimal solution. If an answer does not meet user requirements, the "Review" agent suggests modifications for the "Plan," "Execute," or "Express" agents. The relevant agent then adjusts its process to better meet expectations. For some simple tasks, one or more agents in PEER process can be skipped to simplify the procedure. For complex tasks, a nested pattern can be used, designing each agent to perform an isolate PEER process to enhance entire performance.For a more comprehensive understanding of the PEER framework, refer to Figure , which illustrates how these four agents synergize.

[htbp]

\\

==================================================

Chunk 48:
the PEER framework, refer to Figure , which illustrates how these four agents synergize.

[htbp]

\\Supervised fine-tuning typically employs the cross-entropy loss:
\[
(y, ) = - _{i=1}^{N} _{j=1}^{C} y_{ij} (_{ij}),
\]
where \( N \) is the number of training examples, \( C \) is the number of classes, \( y_{ij} \) is the ground truth one-hot encoded vector, and \( _{ij} \) is the predicted probability for class \( j \). We used a robust model to generate an offline training dataset $_{off}$, which was then refined and validated by human annotators for quality assurance.

==================================================

Chunk 49:
ng dataset $_{off}$, which was then refined and validated by human annotators for quality assurance.Rejection sampling, as used in LLaMA2 , involves generating samples from a pre-trained model and filtering based on quality criteria to retain only high-quality examples. Unlike direct offline supervised fine-tuning (SFT), rejection sampling automates initial filtering to reduce low-quality samples before human annotation. In our iterative training process, rejection sampling boosts performance post offline dataset training.

Direct Preference Optimization (DPO), has emerged as efficient alternatives to RLHF, eliminating the need for a separate reward model . The loss function for DPO is defined as follows:

\[

_{}(_ ; _{}) 
= -_{(x, y_w, y_l)  } [ &  (   {_{} (y_w  x)} \\
&-   {_{} (y_l  x)} ) ]

\]

==================================================

Chunk 50:
ows:

\[

_{}(_ ; _{}) 
= -_{(x, y_w, y_l)  } [ &  (   {_{} (y_w  x)} \\
&-   {_{} (y_l  x)} ) ]

\]where $_$ is the language model being optimized and $_{}$ refers to the model after SFT ($^{}$). The scaling factor $$ measures errors in ranking results and accounts for the KL constraint. In vanilla/offline Direct Preference Optimization (DPO), the model is optimized using  \((x, y_w, y_l)  \), where the dataset-generating model and the optimized model are .

When optimizing DPO models, offline preference datasets and off-policy updates can cause generalization issues with out-of-distribution (OOD) queries. These issues can be mitigated by incorporating online preference datasets and using on-policy learning approaches. .

We follow the experimental setup of , utilizing a batch size of $m$ in online setting. Our methodology integrates the LLM-as-a-Judge approach for real-time feedback, as introduced by , to refine the model progressively.

==================================================

Chunk 51:
LM-as-a-Judge approach for real-time feedback, as introduced by , to refine the model progressively.Algorithm  outlines our iterative training process, starting with the initial dataset $D_$. The agent processes each batch iteratively, involving model evaluation, data generation, and refinement. It generates multiple candidate responses per input, using a reward model (GPT-4o) to select the optimal response and compare it with the ground truth. If the model-generated response exceeds the quality threshold, it replaces the original training sample. For DPO, the lowest-ranked response is identified as a negative example. The updated dataset is then used to refine the model via SFT or DPO techniques. After multiple iterations, the algorithm outputs the best-performing model variant based on predefined metrics. This iterative process continuously enhances response quality, creating a self-refining training paradigm that progressively improves model performance. Figure  illustrates this process.

==================================================

Chunk 52:
g training paradigm that progressively improves model performance. Figure  illustrates this process.[1]
{}\\

{$D_$, $D_$, $T$, $N_$, $M_$}

Initialize $M_0^$ and $M_0^$ using $D_$ 
     $T$}
         Evaluate $M_{i-1}^$ and $M_{i-1}^$ on $D_$
         \{SFT, DPO\}}
             $D_i^  $
            )$  $D_$}
                 $R  (N_, M_{i-1}^(q))$

$r_  _{r  R} M_(q, r)$
(q, r_) > M_(q, r_)$}
     $r_  r_$

$r_  r_$

$r_  _{r  R} M_(q, r)$
                     Add $(q, r_, r_)$ to $D_i^$

Add $(q, r_)$ to $D_i^$

Train $M_i^$ using $D_i^$

Best $M_N^$, $M_N^$, and evaluation results

$: offline training dataset, 
$D_$: evaluation dataset, 
$T$: number of iterations, 
$N_$: number of candidate responses generated each time, 
$M_$: reward model}

[htbp]

==================================================

Chunk 53:
f iterations, 
$N_$: number of candidate responses generated each time, 
$M_$: reward model}

[htbp]is trained on offline data. This model then generates two sets of predictions: one to create training data for the next iteration (upper section) and another to provide evaluation results for the current iteration (lower section). This cycle is repeated iteratively across subsequent training phases.}

We conduct experiment on a real-word industry financial QA dataset to validate the PEER framework discussed in section  and evaluate the agent tuning methods discussed in section .

Since the main usage scenario of PEER framework is the interpretation and analysis of domain events and problems, we mainly tested and compared the performance of PEER on the dataset of financial QA. We sampled hundreds of professional questionsfrom our business scenarios and divided them into nine categories. Details of the dataset distribution are shown in Table .

==================================================

Chunk 54:
rios and divided them into nine categories. Details of the dataset distribution are shown in Table .We conducted experiments using two base models, GPT-3.5 turbo (16k) and GPT-4o, with Python for execution. For FinQA datasets, we compared with the BabyAGI multi-agent framework due to its similar task creation, organization, and execution capabilities to PEER.

To assess the impact of the "Review" agent in the PEER framework, we designed self-ablation experiments with and without the "Review" agent. We set the maximum rounds for both BabyAGI and PEER (with "Review") to 5 and used Google for information retrieval. Under GPT-3.5 turbo (16k), we recalled the top 2 search results with a token limit of 13,000. For GPT-4o, we increased the parameters to the top 6 and 125,000 tokens, leveraging the model's enhanced performance.

==================================================

Chunk 55:
creased the parameters to the top 6 and 125,000 tokens, leveraging the model's enhanced performance.Despite GPT-4’s widespread use for evaluations, its confidence can be influenced by position and verbosity biases . To mitigate these issues, we have developed two evaluation methodologies based on GPT-4:

GPT-4 scores all answers across various dimensions, and we calculate the average score for each dimension. Detailed scoring dimensions, rules, and their meanings are provided in Table  in the Appendix.
     GPT-4 selects the best answer between those provided by PEER and the control group. For this evaluation, we use the win rate as the metric, with selection criteria outlined in Table .

[htbp]

[!h]

{1.5}
    {c|c|c|c|c|c|c|c|c|c|c}

{*}{ Experimental Setup} &{*}{ Model Type} &{*}{Framework Type} &{c}{Evaluation Dimension} \\

& & & Integrity & Relevance & Compactness & Factuality & Logic & Structure & Comprehensiveness & Average\\

==================================================

Chunk 56:
Integrity & Relevance & Compactness & Factuality & Logic & Structure & Comprehensiveness & Average\\{*}{ Comparison Experiment}&{*}{ GPT-3.5-turbo-16K}& BabyAGI & 3.49 & 3.79 & 3.55 & 3.84 & 3.94 & 3.76 & 3.47 & 3.69 \\

& & PEER &  &  &  &  &  &  &  &  \\

& {*}{ GPT-4o}& BabyAGI & 3.16 & 3.32 & 3.32 & 3.98 & 3.78 & 3.86 & 3.14 & 3.51 \\

& & PEER &  &  &  &  &  &  &  &  \\

{*}{ Ablation Experiment}&{*}{ GPT-3.5-turbo-16K}& PEE & 3.91 & 4.30 &  & 4.26 & 4.30 & 4.14 & 3.78 & 4.09 \\

& & PEER &  &  & 3.67 &  &  &  &  &  \\

& {*}{ GPT-4o}& PEE & 4.81 & 4.94 &  & 4.72 & 4.92 & 4.90 & 4.81 & 4.73 \\

& & PEER &  & 4.94 & 3.83 &  & 4.92 &  &  &  \\

==================================================

Chunk 57:
.81 & 4.94 &  & 4.72 & 4.92 & 4.90 & 4.81 & 4.73 \\

& & PEER &  & 4.94 & 3.83 &  & 4.92 &  &  &  \\In the comparative experiment with BabyAGI, as depicted in Table  and Figure , PEER consistently surpasses BabyAGI in both average score and win rate, irrespective of the base model employed. PEER demonstrates superior performance in dimensions such as integrity, relevance, logic, structure, and comprehensiveness, often by a margin exceeding one point. Specifically, under the GPT-3.5 turbo (16k) model, PEER achieves a win rate of 83\

In the ablation experiment, as illustrated in Table  and Figure 3, PEER scores higher in most dimensions and attains a 64\

==================================================

Chunk 58:
ment, as illustrated in Table  and Figure 3, PEER scores higher in most dimensions and attains a 64\We conducted two categories of experiments: one focusing on individual agents and the other on the entire workflow. Dataset sizes are provided in Table , with the data being open-sourced. The test set for individual agents is derived from the intermediate results of the evaluation set detailed in Table , whereas the test set for the entire workflow corresponds directly to Table .

As in Section , for the evaluation of individual agents and the entire workflow, we also employed the LLM-as-a-Judge approach. Specifically, for individual agents, we used scoring and pairwise comparison to evaluate the performance of each iteration. For the entire workflow, we used GPT-4o to score and compare the results of GPT-4 + PEER, the SFT results using offline data, and the best model obtained through iterative training.

==================================================

Chunk 59:
+ PEER, the SFT results using offline data, and the best model obtained through iterative training.Figure  illustrates the win, tie, and loss rates across different iterations for three agents involved in planning, execution, and expression. Both DPO and SFT show progress with each iteration. For example, for the planning agent, the first iteration of SFT achieves a win rate of 43.15\

[htbp]

{1.5}
{c|c|c|c|c|c|c|c|c}

{*}{ Model Type} & {c}{Evaluation Dimension} \\

& Integrity & Relevance & Logic & Comprehensiveness & Compactness & Factuality & Structure & Average \\

QWEN1.5-14B (sft-offline)+ PEER      & 4.09      & 4.58      & 3.34  & 4.23              & 4.32        & 4.22       & 4.03      & 4.12    \\

QWEN1.5-14B (iter-best-model) + PEER & 4.4       & 4.63      & 3.42  & 4.35              & 4.61        & 4.77       & 4.28      & 4.35    \\

GPT4 + PEER &  &  &  &  &  &  &  &  \\

==================================================

Chunk 60:
& 4.61        & 4.77       & 4.28      & 4.35    \\

GPT4 + PEER &  &  &  &  &  &  &  &  \\{Evaluation of the entire workflow: the model after iterative training(QWEN1.5-14B (iter-best-model) + PEER), shows improvements across all metrics compared to the single-round SFT model(QWEN1.5-14B (sft-offline)) and it ultimately reaches 95.0\

[htbp]

[H]

{1.5}
    {l|l|l|l}

& plan & execute & express \\ 
    training datasize  & 5000 & 6847    & 6193    \\ 
    test datasize      & 100  & 456     & 100     \\

==================================================

Chunk 61:
aining datasize  & 5000 & 6847    & 6193    \\ 
    test datasize      & 100  & 456     & 100     \\Table  presents the results of the end-to-end (the entire workflow) evaluation. We conducted experiments on three models: QWEN1.5-14B (sft-offline), QWEN1.5-14B (iter-best-model) and GPT-4, all combined with the PEER framework. QWEN1.5-14B (sft-offline) refers to the QWEN1.5 model fine-tuned with an offline SFT dataset, while QWEN1.5-14B (iter-best-model) indicates the best model obtained through iterative training. We can observe that the QWEN1.5-14B model, after iterative training, shows improvements across all metrics compared to the single-round SFT model. When combined with PEER, it ultimately reaches 95.0\

==================================================

Chunk 62:
metrics compared to the single-round SFT model. When combined with PEER, it ultimately reaches 95.0\Despite pioneering projects in this field, such as AutoGPT, BabyAGI, CAMEL, MetaGPT, and AutoGen , demonstrating their potential, achieving fully autonomous AI agents remains a significant challenge. These dynamic process agents, also known as autonomous intelligent agents, can autonomously perceive the environment, make decisions based on observations, and take actions. Subsequently, they reflect on the outcomes of their actions and plan their next steps accordingly. While theoretically generalizable to any scenario, they face issues such as poor controllability, instability, reproducibility problems, and low task completion rates in specialized domains . PEER strikes a balance between model flexibility and controllability through effective pattern design, considering practical industrial needs, including efficiency, cost-effectiveness, and operational simplicity, making it more suitable for industrial applications.

==================================================

Chunk 63:
cost-effectiveness, and operational simplicity, making it more suitable for industrial applications.Many research efforts aim to transform past experience into usable knowledge and apply it in new reasoning processes to drive continuous model evolution . However, these studies often place high demands on the model's ability to follow instructions, which is particularly challenging for models with fewer parameters. To overcome this challenge, our research adopts an iterative training approach. Specifically, we use both successful and failed cases from previous steps as new training data to promote the model's evolution.

==================================================

Chunk 64:
ccessful and failed cases from previous steps as new training data to promote the model's evolution.In this work, we introduced the PEER framework to address the tri-lemma of performance, cost, and data privacy in domain-specific applications. The framework balances flexibility and controllability through effective pattern design, meeting industrial demands for efficiency and cost-effectiveness. We also developed industrial practices that use online data and user feedback for effective model tuning, promoting continuous model evolution. Our empirical studies, particularly in the financial question-answering domain, demonstrate that this approach achieves 95.0\

Despite our progress in using multi-agent systems to address domain-specific tasks (e.g.finacial), several areas remain ripe for further exploration and improvement:

==================================================

Chunk 65:
in-specific tasks (e.g.finacial), several areas remain ripe for further exploration and improvement:Long-term Learning and Memory Mechanisms: Explore ways to equip the model to accumulate and utilize knowledge over extended periods.
     User Interaction and Feedback Mechanisms: Study how user interactions and feedback can further guide and optimize the model's behavior, achieving a more user-friendly agent design.
     Enhancing Generalization Capability: Investigate methods to further improve the model's generalization ability, enabling agents to tackle other financial problems such as factor-based stock selection or other quantitative trading strategies.

[!h]

{1.5}
    {c|c}

Category & Example\\

==================================================

Chunk 66:
ock selection or other quantitative trading strategies.

[!h]

{1.5}
    {c|c}

Category & Example\\Information Query & 12\
         General Financial QA & 11\
         Report Interpretation & 8\
         Target Analysis & 12\
         Strategy Advice & 12\
         Major Events Interpretation & 9\
         Macro Analysis & 12\
         Market Analysis & 12\
         Policy Interpretation & 12\

{1.2}
{!}{
{ll|c|c|c|c|c|c|c|c}

==================================================

Chunk 67:
Market Analysis & 12\
         Policy Interpretation & 12\

{1.2}
{!}{
{ll|c|c|c|c|c|c|c|c}&
   &
  Integrity &
  Relevance &
  Logic &
  Comprehensiveness &
  Compactness &
  Factuality &
  Structure &
  Average \\ 
{c|}{{*}{Plan}} &
  sft-offline &
  3.36 &
  3.88 &
  3.96 &
  3.24 &
   &
  - &
  - &
  3.61 \\  
{c|}{} &
  sft-iter-1 &
  3.5 &
  4.04 &
   &
   &
  - &
  - &
  - &
  3.76 \\  
{c|}{} &
  sft-iter-2 &
  3.51 &
   &
  4.12 &
   &
  - &
  - &
  - &
   \\  
{c|}{} &
  dpo-iter-1 &
   &
   &
   &
  3.35 &
  - &
  - &
  - &
  3.77 \\  
{c|}{} &
  dpo-iter-2 &
   &
  4.04 &
  4.12 &
  3.33 &
  - &
  - &
  - &
  3.75 \\ 
{l|}{{*}{Execute}} &
  sft-offline &
  3.95 &
  4.68 &
  4.53 &
  3.65 &
  4.24 &
  4.64 &
  4.19 &
  4.27 \\  
{l|}{} &
  sft-iter-1 &
  4.01 &
   &
  4.55 &
  3.76 &
   &
  4.73 &
  4.25 &
  4.32 \\  
{l|}{} &
  sft-iter-2 &
   &
  4.73 &
  4.59 &
   &
  4.11 &
   &
  4.29 &
  4.33 \\  
{l|}{} &
  dpo-iter-1 &
  4.02 &
  4.73 &
   &
  3.74 &
   &
  4.7 &
   &
   \\  
{l|}{} & dpo-iter-2 & 4.03          &  & 4.58          & 3.77          & 4.22       &  &  &  \\ 
{l|}{{*}{Express}} &
  sft-offline &
  4.08 &
  4.79 &
  4.53 &
  3.88 &
   &
  4.76 &
  4.31 &
  4.34 \\  
{l|}{} &
  sft-iter-1 &
  4.09 &
  4.7 &
  4.62 &
  3.96 &
  4.04 &
  4.71 &
  4.39 &
  4.36 \\  
{l|}{} &
  sft-iter-2 &
  4.19 &
  4.75 &
   &
  4.06 &
  3.95 &
  4.77 &
  4.41 &
  4.4 \\  
{l|}{} & dpo-iter-1 & 4.39          &  & 4.71          &  &  & 4.83          &   &  \\  
{l|}{} & dpo-iter-2 &  & 4.79          &  &  & 3.98       &  &   &  \\

==================================================

Chunk 68:
& 4.83          &   &  \\  
{l|}{} & dpo-iter-2 &  & 4.79          &  &  & 3.98       &  &   &  \\}

In this study, we experimented with several different iterative modeling approaches. The SFT-OFFLINE model refers to the SFT model trained exclusively on offline data. The DPO-ITER-1 model is obtained by further training the SFT-OFFLINE model using DPO. Similarly, the DPO-ITER-2 model is derived by continuing the iterative training on DPO-ITER-1 using DPO. The SFT-ITER-1 and SFT-ITER-2 models follow the same iterative training process.

==================================================

Chunk 69:
O-ITER-1 using DPO. The SFT-ITER-1 and SFT-ITER-2 models follow the same iterative training process.As shown in Table , in the three tasks (Plan, Execute, Express), the dpo-iter-2 model stands out with its exceptional performance, particularly in the Express task, where it leads significantly with an average score of 4.56. Meanwhile, the sft-iter-2 model also demonstrates its strength in the Plan task, achieving an average score of 3.78. In the Execute task, the dpo-iter-2 model again takes the top spot with an average score of 4.34.
Overall, the dpo-iter-2 model shows advantages in various metrics, indicating its adaptability and effectiveness across different tasks. Additionally, an increase in iteration count seems to positively impact model performance, though the extent of improvement depends on the specific model used (sft or dpo) and the particular requirements of the downstream tasks.

Table  and table  shows the prompt used for LLM Evaluation.

[t]

{10pt}
{1.2}
{p{}}

==================================================

Chunk 70:
nstream tasks.

Table  and table  shows the prompt used for LLM Evaluation.

[t]

{10pt}
{1.2}
{p{}}You will play a crucial role as a quality evaluator for large language model responses. Your task is to assess and analyze the answers provided by the model for a text-based question answering task, and score the model's responses based on the standard answers and scoring criteria:

\{
  "User Question": "The specific question posed by the user;",\\
  "Context": "Sufficient contextual information provided to answer the user's question;",\\
  "Standard Answer": "The standard answer, i.e., the ideal or reference answer;",\\
  "Model Answer": "Since question answering is an open-ended task, sometimes the model's answer may be better than the standard answer;"
\}
\\
\\
The scoring criteria are as follows:
\\

==================================================

Chunk 71:
l's answer may be better than the standard answer;"
\}
\\
\\
The scoring criteria are as follows:
\\Does the answer form a logical and coherent whole, directly addressing the core requirement of the question?\\
1 = Very Incomplete
2 = Incomplete
3 = Partially Complete
4 = Fairly Complete
5 = Very Complete
\\

Assess the degree to which the answer is related to the question posed.\\
1 = Completely Irrelevant
2 = Largely Irrelevant
3 = Somewhat Relevant
4 = Fairly Relevant
5 = Very Relevant
\\

Evaluate whether the answer is concise, avoiding redundancy or irrelevant information.\\
1 = Very Lengthy
2 = Quite Lengthy
3 = Moderate
4 = Fairly Concise
5 = Very Concise
\\

Assess whether the information in the answer is accurate and fact-based.\\
1 = Completely Inaccurate
2 = Mostly Inaccurate
3 = Partially Accurate
4 = Fairly Accurate
5 = Very Accurate
\\

==================================================

Chunk 72:
ely Inaccurate
2 = Mostly Inaccurate
3 = Partially Accurate
4 = Fairly Accurate
5 = Very Accurate
\\Evaluate whether the answer is logically coherent, reasonable, and aids in understanding.\\
1 = Completely Incoherent
2 = Not Very Coherent
3 = Partially Coherent
4 = Fairly Coherent
5 = Very Coherent
\\

Assess whether the answer is well-structured, with clear paragraph divisions and logical order.\\
1 = Completely Unstructured
2 = Poorly Structured
3 = Moderately Structured
4 = Well Structured
5 = Very Well Structured
\\

Evaluate whether the answer covers all relevant aspects of the question without significant omissions.\\
1 = Very Incomplete
2 = Incomplete
3 = Partially Complete
4 = Fairly Complete
5 = Very Complete
\\
\\
Please return your evaluation results strictly in the following JSON format, adhering to the criteria outlined above:

==================================================

Chunk 73:
r evaluation results strictly in the following JSON format, adhering to the criteria outlined above:\{
  "Analysis Process": "Explanation of the reasoning and process for scoring each dimension;",\\
  "Integrity": "Score;",\\
  "Relevance": "Score;",\\
  "Compactness": "Score;",\\
  "Factuality": "Score;",\\
  "Logic": "Score;",\\
  "Structure": "Score;",\\
  "Comprehensiveness": "Score;"
\}
\\

[]

{p{0.95}}

You will play an important role as a quality evaluator of answers provided by large language models. Your task is to assess and analyze the answers generated by the model for a given text-based question.

==================================================

Chunk 74:
ur task is to assess and analyze the answers generated by the model for a given text-based question.\{\\
  "User Question": "The specific question asked by the user",\\
  "Context": "Sufficient contextual information provided to answer the user's question",\\
  "Expected Answer": "The standard answer, representing an ideal or optimal response",\\
  "Model Answers": "This is a list comprising two answers. Each item in the list is numbered, and each answer may be correct or incorrect. Since the question-answering task is open-ended, sometimes a correct answer may be better than the standard answer"\\
\}

Your tasks are as follows:

Carefully read the two answers provided by the model.
 Compare the two answers and select the better one.
 If both answers are equally good or equally bad, respond with "equally good" or "equally bad," respectively.

During the evaluation, you need to consider the following key aspects:

==================================================

Chunk 75:
"equally bad," respectively.

During the evaluation, you need to consider the following key aspects:: Does the answer accurately address the user's question and align with the provided context?
 : Does the answer exhibit professional knowledge consistent with the standard answer?
 : Compared to the standard answer, does it adhere to current facts or time-sensitive information?
 : Is the answer succinct, avoiding redundancy or irrelevant information?
 : Is the information in the answer accurate and fact-based?
 : Is the answer logically coherent, reasonable, and does it aid understanding?
 : Does the answer cover all relevant aspects of the question without significant omissions?

Based on the analysis above, please strictly adhere to and return your evaluation results in the following JSON format:

==================================================

Chunk 76:
is above, please strictly adhere to and return your evaluation results in the following JSON format:\{\\
  "Reason for Choice": "Please provide detailed reasons for your choice here. Explain why you think one answer is better than the other, or why both are equally good or equally bad.",\\
  "Evaluation Result": "Choose one of the following options: 1 (if answer 1 is better), 2 (if answer 2 is better), equally good (if both answers are equally good), equally bad (if both answers are equally bad)."\\
\}
\\

[t]

{10pt}
{1.2}
{p{}}

From now on, your role is

Name: Research Assistant

Responsibilities:

Skilled at analyzing issues from various perspectives to help users quickly obtain information.
Determine what information to search for based on the context and the user’s question to provide the best possible answer.

First, identify the relevant timeframe based on the context and question, which could be a specific date or general terms like "latest," "recent," or "upcoming."

==================================================

Chunk 77:
nd question, which could be a specific date or general terms like "latest," "recent," or "upcoming."Then, decide what information needs to be searched to answer the question, ensuring a multi-dimensional and multi-angled approach.

Finally, provide clear and unambiguous search conditions, each as a complete sentence.
Rules to Follow:

Sub-questions must have clear and detailed descriptions of the subject, event, and timeframe. Avoid vague terms like "similar," "related," and replace them with specific details from the context.

Unless the question or context specifies a timeframe, default to "latest" or "recent," not "today."

If the question includes a timeframe, the search conditions must reflect this, converting terms like "today" to a specific date.

Each search condition must directly aim to find answers from that angle, without including terms like "search" or "query."

Each search condition must be a complete sentence with no ambiguity.

==================================================

Chunk 78:
erms like "search" or "query."

Each search condition must be a complete sentence with no ambiguity.Do not expand on the question; only answer the question asked.

When breaking down the question, adhere to the context’s requirements.
\\
\\

:
From now on, your role is

Name: Research Assistant

Responsibilities:

When a user searches for information on a particular issue and obtains several relevant pieces of information, you need to integrate, correct, and answer the user’s question.
Rules to Follow:

Do not use incorrect information.

Do not use information with inconsistent subjects. For example, if the question is about "John Doe from XYZ Company," but the information only mentions "John Doe" without confirming he is from "XYZ Company," do not use it.

Do not make vague speculations.

Follow the requirements specified in the context.

Avoid using ambiguous terms like XXX or ABC.

Ensure the content is detailed and emphasizes key points.
\\
\\

:
From now on, your role is

==================================================

Chunk 79:
r ABC.

Ensure the content is detailed and emphasizes key points.
\\
\\

:
From now on, your role isName: Research Assistant

Responsibilities:

Skilled at answering user questions from different angles in an organized manner.
You need to use only the provided knowledge to answer user questions professionally and in detail.
Rules to Follow:

Do not use incorrect information.

Do not use information with inconsistent subjects. For example, if the question is about "John Doe from XYZ Company," but the information only mentions "John Doe" without confirming he is from "XYZ Company," do not use it.

Do not make vague speculations.

Follow the requirements specified in the context.

Avoid using ambiguous terms like XXX or ABC.

Ensure the content is detailed and emphasizes key points.

Use only the provided knowledge and answer the questions step by step.
\\
\\

:
From now on, your role is

Name: Research Assistant

Background:

==================================================

Chunk 80:
he questions step by step.
\\
\\

:
From now on, your role is

Name: Research Assistant

Background:I am an editor for a financial news agency, writing a report for my client.
The client has provided specific requirements for the content and format of the report and desires a higher quality response.

My report includes two parts:

[Analysis Dimensions]: Rewriting or breaking down the user’s question into 3 to 5 queries as search conditions. Each question must be related to the original question’s theme.
[Answer]: Answering the user’s question with a complete and coherent response. The answer should be logically structured, clear, and concise.

Responsibilities:

==================================================

Chunk 81:
coherent response. The answer should be logically structured, clear, and concise.

Responsibilities:As a senior expert with 30 years of experience in the investment field, you need to check if my report meets the client’s requirements and provide feedback for improvement.
Determine whether my analysis dimensions and answers meet the user’s requirements. If the report meets all requirements, set Qualified to True; otherwise, set it to False.
If the report needs improvement, identify which part needs enhancement, either "Analysis Dimensions" or "Answer."
Provide detailed suggestions for improvement based on the user’s requirements. Describe your suggestions in detail without providing the complete revised answer.
Provide only relevant and high-impact feedback, avoiding unnecessary information.
Do not add redundant information or supplement reference sources. Only include highly relevant key information.

Your response structure must be:

Draft: Your thought process

==================================================

Chunk 82:
lude highly relevant key information.

Your response structure must be:

Draft: Your thought processQualified: Whether the user’s requirements are fully met, True or False

Role: The part that needs modification, either "Plan" or "Express"

Suggestion: Your suggested modifications. If the report meets all requirements, this can be empty. Only suggest modifications, without providing the complete revised answer

==================================================

Chunk 83:
In this section, we describe a novel framework  for enhancing the precision of retrieval results through sentence-level ranking and reconstruction, integrated into the RAG system. Note that  does not require additional training.

We first introduce the general RAG system, which consists of three steps: the retrieval step, the re-ranking step, and the generation step.
Note that all steps focus on passage-level documents.

==================================================

Chunk 84:
the re-ranking step, and the generation step.
Note that all steps focus on passage-level documents.The retrieval step searches for a potentially relevant document set $$ to the given query $q$ from a retrieval corpus $$ consisting of millions of documents. 
This retrieval step is conventionally performed using a sparse retriever $S$, such as BM25, which is widely used for processing large corpora due to its low latency. 
The sparse retriever $S$ fetches the relevant documents having high relevant scores based on lexical values such as document length or unique word count.
Formally, we define the retrieval step as:
\[
 = (q, ; S) = \{d_1, d_2, ..., d_n\}
\]
where $d_k$ represents a document having the top-$k$ score among the retrieval corpus $$ for a given query $q$, and $n$ denotes the size of $$, generally ranging from tens to hundreds.

==================================================

Chunk 85:
s $$ for a given query $q$, and $n$ denotes the size of $$, generally ranging from tens to hundreds.While the sparse retriever $S$ can efficiently handle a large corpus, it cannot consider semantic similarities, thereby limiting its retrieval performance for lexically different but semantically relevant pairs. 
To address this, the re-ranking step aims for more precise retrieval results by reordering the retrieved document set $$ using the ranking model $R$. 
This model transforms $$ into a newly ordered document set $$ based on relevance scores with a query $q$, capturing semantic meanings that could not be addressed in the retrieval step with $S$.
Formally, we define the re-ranking step as:
\[
=(q,;R)=\{d'_1,,d'_m\}
\]
where $d'_k$ represents the document that has top-$k$ relevance score among $$ and \(m  n\), indicating that the subset $$ contains significantly fewer documents than the original set $$.

==================================================

Chunk 86:
n\), indicating that the subset $$ contains significantly fewer documents than the original set $$.After the re-ranking step, the document set $$ is augmented to the LLM $M$ with the supporting documents to generate the correct answer $a$ for the given query $q$. The generation step can be formalized as:

\[
 a=(q, ; M)
\]

In RAG systems, the three key steps are designed to retrieve the most query-relevant knowledge for LLMs, typically at the passage level. However, this fixed granularity can overlook finer relevance between queries and individual sentences.

Therefore, in this work, we introduce a fine-grained, sentence-level ranking strategy in the re-ranking step, aiming to reduce distractions from irrelevant information and enhance answer accuracy.

==================================================

Chunk 87:
ranking step, aiming to reduce distractions from irrelevant information and enhance answer accuracy.We propose a novel unsupervised refinement framework, ocument Refinement with entence-evel e-ranking and Reconstruction (), designed to assess the fine-grained relevance of individual sentences within a passage and reconstruct to preserve the original contextual coherence. 
Figure  illustrates examples generated by each step in our  framework.

==================================================

Chunk 88:
iginal contextual coherence. 
Figure  illustrates examples generated by each step in our  framework.After the retrieval step (§), we conduct sentence-level re-ranking for the documents within the retrieved set $$. First, each document $d_i  $ is decomposed into a sentence set $_i = \{s_j\}_{j=1}^l$, where $s_j$ represents the $j$-th sentence in document $d_i$ and $l$ is the number of sentences in $d_i$. Then, the passage-level retrieved set $$ is redefined to the sentence-level retrieved set $ = _{i=1}^n _i$. For instance, as illustrated in Figure , a passage retrieved for a query ``How many episodes in "Grace and Frankie" Season 1?" is decomposed into three sentences \( s_1 \), \( s_2 \), and \( s_3 \) during the sentence decomposition step.

==================================================

Chunk 89:
sed into three sentences \( s_1 \), \( s_2 \), and \( s_3 \) during the sentence decomposition step.To extract sentences containing relevant information for a query \( q \), we initially perform re-ranking to assess relevance scores at the sentence level. Sentences in \(  \) with scores below a predefined threshold \( T \) are deemed irrelevant and removed, resulting in a refined set \(  \). The sentence-level re-ranking is formally defined as follows:
\[
=(q,;R)=\{s'_1,,s'_m\}
\]
where each $s'_k$ is a sentence from $$ whose relevance score exceeds $T$. Figure~ demonstrates the reordering of sentences, highlighting the exclusion of $s'_3$ due to its insufficient relevance score.

Note that this step of the  framework utilizes off-the-shelf ranking models, which are identical to those used in passage-level re-ranking.

==================================================

Chunk 90:
tilizes off-the-shelf ranking models, which are identical to those used in passage-level re-ranking.While the sentence decomposition and re-ranking steps select the top-$m$ relevant sentences for the query $q$, these sentences may lack contextual relationships to one another, as these steps can disrupt the original contextual flow of the passage by discarding some sentences.
Instead of following a widely used approach of simply concatenating these sentences in descending order of their relevance scores, we propose to reconstruct them into the contextually organized set, $^*$, to reflect the order in which they were originally positioned before being decomposed from passages, ensuring the original coherence and logical flow:
\[
^* = (,)=\{s^*_1,,s^*_m\}
\]
where $s^*_i$ is the sentence included in $S'$ and $i$ denotes the relative position of $s^*_i$ within $$. 
As shown in Figure~, the remaining two sentences are reconstructed in their original order by switching their positions to preserve the context before the sentence re-ranking step.
Then, LLM $M$ generates the answer $a$ for a given query $q$ with $^*$ formalized as: $a=(q,^*;M)$.

==================================================

Chunk 91:


==================================================

Chunk 92:
Recent advancements in Large Language Models (LLMs) have significantly improved their performance across various Natural Language Processing (NLP) tasks.
However, LLMs still struggle with generating non-factual responses due to limitations in their parametric memory.
Retrieval-Augmented Generation (RAG) systems address this issue by incorporating external knowledge with a retrieval module.
Despite their successes, however, current RAG systems face challenges with retrieval failures and the limited ability of LLMs to filter out irrelevant information.
Therefore, in this work, we propose } (ocument Refinement with entence-evel e-ranking and Reconstruction), an unsupervised framework that decomposes retrieved documents into sentences, filters out irrelevant sentences, and reconstructs them again into coherent passages.
We experimentally validate  on multiple open-domain QA datasets and the results demonstrate that  significantly enhances the RAG performance over conventional fixed-size passage.
Furthermore, our  enhances performance in specific, yet realistic scenarios without the need for additional training, providing an effective and efficient solution for refining retrieved documents in RAG systems.

==================================================

Chunk 93:
Recent advancements in Large Language Models (LLMs)  have significantly expanded their capabilities across diverse knowledge-intensive tasks in Natural Language Processing (NLP), such as Question Answering (QA) . 
However, despite these capabilities, LLMs still face challenges such as generating plausible yet non-factual responses, known as hallucination, due to their reliance on limited parametric memory . 
Also, it is noted that this parametric memory is static, as LLMs can learn knowledge only up to the specific date on which the training was completed.
Therefore, these limitations restrict their adaptability to long-tailed or ever-evolving domains  and to unseen knowledge outside their training data .

==================================================

Chunk 94:
ility to long-tailed or ever-evolving domains  and to unseen knowledge outside their training data .Retrieval-Augmented Generation (RAG)  has been introduced as an effective solution to address such problems.
Specifically, RAG enhances LLMs by integrating non-parametric memories fetched from external knowledge bases using a retrieval module, which helps LLMs' responses grounded on factual evidence and makes them more up-to-date.

==================================================

Chunk 95:
val module, which helps LLMs' responses grounded on factual evidence and makes them more up-to-date.While the efficacy of RAG depends on the performance of the retrieval module, the instability of LLMs in incorporating the retrieved knowledge is also a critical challenge to RAG.
To be specific, retrieved documents sometimes contain irrelevant information~, and LLMs often struggle to effectively filter out such redundant details and focus on the most query-relevant knowledge~, which leads to the failure of the overall RAG systems.
Therefore, it is crucial to investigate how to effectively refine retrieved documents before augmenting them with LLMs, ensuring that the LLMs are not distracted by irrelevant information within retrieved documents.

Re-ranking the order of the retrieved document set~ or refining them into new documents~ can be considered as solutions.

However, they generally require high computational costs for training additional re-ranking or refining models.

==================================================

Chunk 96:
ey generally require high computational costs for training additional re-ranking or refining models.Another proposed solution is to reduce the retrieval granularity from passage-level to sentence-level which can help eliminate redundant information within passages .
However, this might also inadvertently remove important contextual information, which is crucial for accurately answering the given queries .
Therefore, we should explore a novel method that can effectively and efficiently filter out irrelevant information while maintaining the necessary contextual details.

In this work, we introduce an unsupervised } (ocument Refinement with entence-evel e-ranking and Reconstruction) framework that consists of three steps: 1) decomposition, 2) re-ranking, and 3) reconstruction.

==================================================

Chunk 97:
ion) framework that consists of three steps: 1) decomposition, 2) re-ranking, and 3) reconstruction.Specifically, after retrieving the passage-level document, the  framework operates by first decomposing the retrieved document into sentences for finer granularity and then filtering out the irrelevant sentences based on their re-ranking scores from the ranking models, including off-the-shelf retrievers and re-rankers. 
Finally, the remaining sentences are reconstructed into a single document to preserve the original contextual information. 
Note that  is an unsupervised refinement framework, which does not require any additional training for re-ranking or reconstruction steps.
The overall  framework is illustrated in Figure~.

==================================================

Chunk 98:
l training for re-ranking or reconstruction steps.
The overall  framework is illustrated in Figure~.We validate our framework across a diverse range of open-domain QA benchmarks, which include three general QA datasets and three specific QA datasets that require domain-specific or ever-evolving knowledge.
Our experimental results show that  significantly enhances the overall RAG performance and is comparable to, or even outperforms, the supervised baseline approaches.
Specifically, when evaluated with specific QA datasets,  shows high robustness in realistic settings.
Furthermore, a detailed analysis demonstrates the effectiveness of each proposed step and how it contributes to the overall performance.

Our contributions in this work are threefold:

We point out that recent RAG systems are largely vulnerable to redundant knowledge within fixed-size passage-level retrieved documents and that the existing refining strategies generally require additional training steps.

==================================================

Chunk 99:
ved documents and that the existing refining strategies generally require additional training steps.We propose a  framework that incorporates sentence-level re-ranking and reconstruction to effectively remove redundant knowledge that negatively affects the RAG system.

We show that  is highly effective and efficient even without additional training steps in both general and specific scenarios.

==================================================

Chunk 100:
[1]{{}}
[1]{{}}

{
    Xiang Li$^{1, 2}$ 
    Haoran Tang$^{2}$ 
    Siyu Chen$^2$ 
    Ziwei Wang$^2$ 
    Ryan Chen$^2$ 
    Marcin Abram$^{1, 3}$\\[4pt]

$^1$Department of Physics and Astronomy, University of Southern California, Los Angeles, CA, USA\\[2pt]
    $^2$Department of Computer Science, University of Southern California, Los Angeles, CA, USA\\[2pt]
    $^3$Information Sciences Institute, University of Southern California, Los Angeles, CA, USA\\[4pt]

@usc.edu}\\
}

==================================================

Chunk 101:
on Sciences Institute, University of Southern California, Los Angeles, CA, USA\\[4pt]

@usc.edu}\\
}We measure the performance of in-context learning as a function of task novelty and difficulty for open and closed questions. For that purpose, we created a novel benchmark consisting of hard scientific questions, each paired with a context of various relevancy. We show that counter-intuitively, a context that is more aligned with the topic does not always help more than a less relevant context. This effect is especially visible for open questions and questions of high difficulty or novelty. This result reveals a fundamental difference between the treatment of close-form and open-form questions by large-language models and shows a need for a more robust evaluation of in-context learning on the variety of different types of questions. It also poses a new question of how to optimally select a context for large language models, especially in the context of Retrieval Augmented Generation (RAG) systems. Our results suggest that the answer to this question can be highly application-dependent and might be contingent on factors including the format of the question, the perceived difficulty level of the questions, and the novelty or popularity of the information we seek.

==================================================

Chunk 102:
rceived difficulty level of the questions, and the novelty or popularity of the information we seek.Despite their indisputable successes , Large Language Models (LLMs) often struggle to answer challenging questions . While they can achieve superhuman accuracy on many benchmarks , they also suffer from hallucinations , lack of coherence , and are prone to cognitive errors .

To make the difficult situation even worse, it is not always easy to detect mistakes committed by LLMs since their responses are often presented in a way that emulates correct and coherent answers .
    For practical reasons, many existing benchmarks only test the ability to answer either closed  or easy-to-verify questions, e.g., regarding common knowledge  or questions that can be algorithmically verified .

==================================================

Chunk 103:
ify questions, e.g., regarding common knowledge  or questions that can be algorithmically verified .Another challenge concerns domain generalization and domain shift problems, resulting in the need to constantly update your machine learning models to account for the evolution of various trends in your data . However, improving the performance of pre-trained LLMs for specific tasks by fine-tuning is both expensive  and technically challenging . While some techniques like Low-Rank Adaptation (LoRa) can reduce the cost of training , it does not solve the main issue, namely, how to allow LLMs to leverage new pieces of information that were not a part of the initial training corpus .

==================================================

Chunk 104:
low LLMs to leverage new pieces of information that were not a part of the initial training corpus .One approach to the issue might be in-context learning , where LLMs effectively learn to solve a given problem leveraging a limited number of examples without updating the model parameters. Namely, in-context learning incorporates question-solution pairs in the input prompt, allowing LLMs to detect the logic and patterns of those examples, subsequently improving the LLMs output accuracy. It enables LLMs to acquire new knowledge in the inference time and utilize it in subsequent responses. This technique significantly reduces the complexity of improving the LLMs performance compared to alternative approaches such as fine-tuning . 
    It should also be noted that the effectiveness of the popular Retrieval-Augmented Generation (RAG) techniques relies heavily on the strength of in-context learning    , as discussed later.

==================================================

Chunk 105:
tion (RAG) techniques relies heavily on the strength of in-context learning    , as discussed later.In this paper, we focused on the question of how various types of context improve the effectiveness of in-context learning when answering challenging questions. We noticed a surprising behavior. Namely, depending on the difficulty and novelty of the question, and depending on the fact whether the question is of the open or closed type, the relation of the measured performance of the model to both the perceived and quantified relevancy of the context . Notably, the measured in-context learning performance of GPT-4 was positively correlated to context relevancy in two benchmarks with closed-form questions but negatively correlated in our benchmark with open-form questions, indicating different utilization of context depending on the form of the received questions.

==================================================

Chunk 106:
stions, indicating different utilization of context depending on the form of the received questions.In the next sections, we introduce our novel dataset, which comprises 160 unique question-response pairs from the fields of physics and computer science with varying levels of difficulty. For the purpose of evaluation, each question is accompanied by one of four types of context (including  to serve as a control group) and paired with a generated answer from GPT-4. In the subsequent sections, we detail our grading scheme and present the results aggregated from each of our graders. Next, we compare our findings with the existing work by , highlighting a notable discrepancy in the measured effectiveness of the context. To elucidate this difference, we delve deeper into the nature of the problem, discovering that the main impact comes from the open or closed form of the questions, with additional effects related to the difficulty or novelty of those queries.

==================================================

Chunk 107:
orm of the questions, with additional effects related to the difficulty or novelty of those queries.To further strengthen our analyses, we then compare the performance improvement associated with in-context learning across a range of context relevancy using two additional close-ended question datasets, MetaICL  and NephSAP  and we contrast the results with our findings harvested with the help of our open-ended question dataset. Following this, in the discussion section, we discuss the impact of our work, especially in the context of the RAG systems, future research directions, and other methods that enhance LLM performance

==================================================

Chunk 108:
ntext of the RAG systems, future research directions, and other methods that enhance LLM performanceLLMs have shown remarkable capabilities in various tasks, including code generation , text summarization , and database query optimization . They demonstrate a surprising ability to perform in-context learning, where an LLM ``learns'' to do a task simply by conditioning on a prompt containing input-output examples, achieving state-of-the-art (SOTA) results on various benchmarks. However, there has been little understanding of how the model leverages the context and what makes in-context learning work.
    In addition, their performance significantly depends on the contextual information provided and, as discussed in this paper, on the form and type of the queries.

==================================================

Chunk 109:
ontextual information provided and, as discussed in this paper, on the form and type of the queries.In-context learning has been a focal point in recent research. Unlike tra\-ditional fine-tuning methods, in-context learning adapts models to unseen tasks by incorporating examples directly into the input context, as highlighted by .  discussed how in-context learning can be understood as implicit Bayesian inference, where models infer latent concepts to generate coherent responses. Techniques such as chain-of-thought prompting  have shown significant improvements in reasoning tasks. Recent frameworks like OpenICL  have further streamlined the implementation of in-context learning by providing unified and flexible tools for integrating various retrieval and inference methods.

==================================================

Chunk 110:
ing by providing unified and flexible tools for integrating various retrieval and inference methods.Many recent research focuses on the example selection strategies of in-context learning. One of the most common strategies is to select examples for demonstration based on similarity in the embedding space . In-context learning seems robust to label-noise, as indicated by work of , in which authors show that demonstrations, even one with randomly shuffled labels, can still significantly improve LLM's performance in the MetaICL dataset.

==================================================

Chunk 111:
randomly shuffled labels, can still significantly improve LLM's performance in the MetaICL dataset.Benchmarking is essential for understanding LLM performance across different domains. Existing benchmarks like AGIEval , ChenLLMBench , SCIEval , PIXIU , and MME  provide comprehensive datasets for evaluating LLMs. While these benchmarks are useful for understanding the general capabilities of LLMs, they do not capture the complexity of more open-ended and context-sensitive queries. Here, the added value of our work, as we believe the novel open-question validation set we created, fills that gap.

==================================================

Chunk 112:
value of our work, as we believe the novel open-question validation set we created, fills that gap.In this paper, we argue that closed questions, such as multiple-choice or fill-in-the-blank formats, do not adequately reflect the challenges posed by open questions that require deep understanding and synthesis of information from diverse contexts. While  have shown that context significantly affects LLM performance, they have not quantified how different levels of context relevancy impact responses to different types of questions. Our research addresses this gap by creating a novel benchmark that focuses on open, challenging questions. These questions are paired with various types of contexts to systematically evaluate how context affects LLM performance.

==================================================

Chunk 113:
aired with various types of contexts to systematically evaluate how context affects LLM performance.Furthermore, our work suggests areas for improving the performance of Retrieval-Augmented Generation (RAG). Current RAG studies focus on providing context during model inference. Given our observation of the inconsistent relationship between the relevance of context and model performance for different question types (open-form and closed-form), we believe that the context retrieved by comparing vector similarity using RAG may not always correlate with the most useful context for enhancing LLM inference performance and does not mitigate issues such as hallucinations and logic errors. We propose that the type of context selected should be tailored to the attributes of the type of questions with several practical propositions of the retrieval regions outlined in the discussion.

==================================================

Chunk 114:
f questions with several practical propositions of the retrieval regions outlined in the discussion.To investigate the relationship between the relevance of context and the performance of large language models (LLMs), we created an open-form questions dataset comprising physics and computer science questions of varying difficulty levels and originality. Next, we prepared contexts with four different levels of relevancy for each question in our dataset.

==================================================

Chunk 115:
Next, we prepared contexts with four different levels of relevancy for each question in our dataset.The selected questions cover the following areas: quantum mechanics, physics for life science, electromagnetism, classical mechanics, and computer science. Solutions usually involve a combination of complex calculations and the application of conceptual knowledge. Each question is categorized under one of the three different difficulty levels: easy, medium, and hard. The difficulty of the question is defined by the grader according to their perceived complexity of the question. Additionally, each question is also categorized under one of three originality categories: known, paraphrased, and original. Known questions can be found online or in textbooks, paraphrased questions are modified versions of known questions, and original questions were handcrafted by the authors of this paper.

==================================================

Chunk 116:
d versions of known questions, and original questions were handcrafted by the authors of this paper.For each question, we created a ground truth answer for scoring reference and four context types with different levels of relevance. The four context types are: (1) ``no context'' to serve as a control group, (2) ``irrelevant context'', which consists of text on topics that do not match the subject of the question, (3) ``vague context'', which incorporates some topics or keywords related to the question, and (4) ``relevant context'', which provides reasoning context for the question, or answer to a highly related question. Next, for each unique pair of question-context, we generated a response employing the OpenAI's\  model.

After retrieving the responses, we constructed 160 question-response pairs, each accompanied by the corresponding ground truth.

==================================================

Chunk 117:
ses, we constructed 160 question-response pairs, each accompanied by the corresponding ground truth.Aware that human grading can be subjective, we decided that each question would be evaluated by six independent graders using a pre-defined scoring sheet. This gave us 960 evaluation responses in total.

The Supplementary Material includes examples of the questions and context types, as well as the evaluation sheets.

Our evaluation system comprised three main categories, 
     (5 points),
     (5 points), and
     (5 points).

==================================================

Chunk 118:
tion system comprised three main categories, 
     (5 points),
     (5 points), and
     (5 points).In addition, graders had the option to identify specific problems in the responses, such as , , , , and . They could also highlight portions of the responses as , , or . An open response section was provided for graders to give comments and feedback about the generated responses. Finally, graders were asked to rate their confidence in their own grading. These options allowed us to gain deeper insights into the grading process and to assess the quality of the generated responses in detail.
    A screenshot of the scoring interface can be found in the Supplementary Material.

==================================================

Chunk 119:
ses in detail.
    A screenshot of the scoring interface can be found in the Supplementary Material.Each grader may have different biases and varying levels of expertise. To enhance the accuracy and reliability of our evaluation, we ensured that all graders assessed all 160 questions. This approach was essential for obtaining consistent and accurate results. By having multiple graders evaluate each response, we mitigated individual biases and ensured a more comprehensive assessment. This method captured a broader range of perspectives and expertise, leading to a more robust and reliable evaluation of the generated responses. As demonstrated later, this comprehensive grading significantly improved the accuracy and consistency of our findings.

==================================================

Chunk 120:
ter, this comprehensive grading significantly improved the accuracy and consistency of our findings., , , and ) evaluated for , , and , assessed by six different graders. (B) The process of standardizing raw scores from each grader to calculate the overall standardized average scores. The raw scores are converted to Z-scores, which are then averaged to obtain standardized average scores. (C) Standardized average scores of generated responses for each context type aggregated across all graders.}

To illustrate the correlations between the context types and the quality of the corresponding generated responses, in Fig.  panel A, we show the raw average scores of each context type for each grader. Notably, the results are rather noisy, with each grader having an individual tolerance for different types of errors, resulting in different reference levels for each of them.

==================================================

Chunk 121:
l tolerance for different types of errors, resulting in different reference levels for each of them.By design, each question was evaluated by each grader. This additional redundancy allows us to standardize the scores for each grader and then average them, resulting in reduced variance in the final results. This aggregation procedure is depicted in Fig.  panel B.

As a result, although the raw scores displayed differences in trends and values across all three grading rubrics, a clear trend appeared after we applied the aggregation procedure, as depicted in Fig.~ panel~C. Counter-intuitively, a higher standardized average score was associated with , and the lowest score with the  context.

==================================================

Chunk 122:
y, a higher standardized average score was associated with , and the lowest score with the  context., , , and ), categorized by three levels of question difficulty (, , and ) for correctness, logic errors, and lack of hallucination. (B):~Standardized average scores of generated answers for each context type, subdivided into , , and  categories, evaluated for correctness, logic score, and lack of hallucination.}

==================================================

Chunk 123:
ivided into , , and  categories, evaluated for correctness, logic score, and lack of hallucination.}To investigate how the difficulty of questions affects the quality of generated responses, we compared the results across three difficulty levels (, , and ) for each of the four context types, as shown in Figure , panel A. We can observe a clear trend of decreasing scores as the difficulty of the questions increased from medium to hard, indicating that GPT-4's performance declines with higher question difficulty.
    This also indicates that human-perceived difficulty of the question was in fact, correlated with the factual difficulty experienced by GPT-4, a result interesting on its own.
    For easy and medium-difficulty problems, GPT-4 generated responses with similar scores, indicating that the alignment between the human-perceived and machine-perceived difficulty has its own limits.

==================================================

Chunk 124:
that the alignment between the human-perceived and machine-perceived difficulty has its own limits.In Figure , panel B, we show the comparison between the aggregated standardized average score for the different levels of originality types for each context type. It is evident that GPT-4 scores highest for known questions, likely because these questions were part of its training data, and therefore GPT-4 has a higher chance to answer them correctly. Interestingly, the score for known questions given irrelevant context is twice as high as that for relevant context. This suggests that irrelevant context might be more helpful than relevant context for known questions, at least for the open type of question, as measured here.

==================================================

Chunk 125:
than relevant context for known questions, at least for the open type of question, as measured here.In this section, we combined the standardized scores from all graders and compared them across different context types. Our results indicate that, on average, the responses generated with no additional context or with the help of irrelevant context are of higher quality than the responses generated for queries incorporating highly relevant context.
    This result is in striking difference to results of . To further understand this discrepancy, in the next section, we replicate the key findings of , and we discuss what might cause the difference in the behavior.

==================================================

Chunk 126:
, we replicate the key findings of , and we discuss what might cause the difference in the behavior.demonstrates that in-context learning allows us to achieve significantly better results compared to the ``no context'' case. In addition, the authors show that in-context learning is robust to label noise. Namely, the authors show that context with randomly shuffled labels and ``golden'' context (with correct labels) have similar effects in enhancing the quality of generated responses for closed questions, such as multiple choice and true/false questions.

However, to investigate the striking difference in the observed trends and to eliminate the effect of different versions of ChatGPT playing a potential role here, we decided to replicate the key results from  using precisely the same framework as above and using the same version of the LLM, namely .

==================================================

Chunk 127:
ts from  using precisely the same framework as above and using the same version of the LLM, namely .For the replication, we decided to use two different existing benchmarks, MetaICL  and a dataset from NephSAP . The only significant element, differentiating this study from our previous evaluations, is that both of these datasets contain close-form questions.

Our evaluation of in-context learning of closed-form questions involves two datasets. For the MetaICL dataset, we take a subset of 10 different tasks, each containing multiple-choice questions. For the NephSAP dataset, we take multiple-choice questions within 20 different subjects. Details about tasks, subjects, and sample questions can be found in the Supplementary Materials.

==================================================

Chunk 128:
ts. Details about tasks, subjects, and sample questions can be found in the Supplementary Materials.We conduct an 80-20 train test split for both the MetalCL dataset and the NephSAP dataset. For each multiple-choice question in the test set, we generate a response using the  model. We do it three times: once without any context, once with a randomly sampled demonstration with a different task or subject from the training set of the dataset, and once with a randomly sampled demonstration with the same subject or task from the training set.

We also compute the embedding of the questions and the demonstrations. We bin the embedding similarity of each demonstration/response pair into separate bins. Treating the no-context response as a benchmark, we record the general score improvement of the response within each embedding similarity bin compared to the raw benchmark.

In Fig. , we show the score improvement as a result of different contexts, using the no-context answer as the baseline.

==================================================

Chunk 129:
he score improvement as a result of different contexts, using the no-context answer as the baseline.[b]{0.48}

[b]{0.48}

[b]{0.48}

Note how context similarity is positively correlated with the mean score improvement in both of the closed-question datasets (MetalICL and NephSAP). This result is consistent with the arguments made by  and . Note also that in both closed-question datasets, the context with the lowest levels of similarity scores has a tendency to have a negative mean improvement (meaning, adding context hurts the results). As contexts with low levels of similarities are more likely to be contexts with a different subject or task, this result is consistent with the findings in , where irrelevant demonstrations can hurt the performance of LLM.

==================================================

Chunk 130:
s consistent with the findings in , where irrelevant demonstrations can hurt the performance of LLM.This contrasts the results for the closed-form questions, as depicted in Fig. , panel C. Our open-form question results display a negative correlation between context similarity and mean improvement. The results suggest that, in this case, context with a lower level of similarity can be more helpful in improving the quality of the response, whereas context with a higher level of similarity can hurt the quality of the response.

==================================================

Chunk 131:
he response, whereas context with a higher level of similarity can hurt the quality of the response.Our results have suggested a significant difference between open-form question evaluation and close-form question evaluation, as the relationship between context-similarity and performance improvement is completely reversed in those two cases. The implications of this result are twofold. First, the difference between open-question evaluation and close-question evaluation invokes a new discussion on their different applicability in the context of in-context learning. Second, those mixed results suggest that similarity score might not be the best indicator for context selection in in-context learning, especially in cases that involve open-form questions. This has profound implications, especially in the context of Retrieval Augmented Generation (RAG) applications.
    For example, instead of selecting all points that lie in the vicinity of a certain point in the embedding space representing a query (cf. Fig., panel A), a better choice could be to either exclude or at least diminish the impact of contexts that are too close to that point (cf. Fig., panel B). This would lead to more interesting topologies. Instead of sampling the context from a hypersphere, we could sample from shells of various thicknesses.

==================================================

Chunk 132:
tead of sampling the context from a hypersphere, we could sample from shells of various thicknesses.The different behaviors exhibited in open-form question evaluation and closed-form question evaluation stem from a different treatment of context in those two cases. We provide a hypothetical interpretation of that mechanism. 
    In closed-form multiple-choice questions, the evaluated language model is treated as a classification model. A relevant demonstration provided as a context can improve the LLM's performance by aligning it with the correct choice. 
    In open-form questions, the evaluated language model is treated as a generative model, and the response is open-form. Instead of being either correct or incorrect, an open-form response can be anywhere in between. A relevant context provides alignment with one way of approaching the question, but it can also introduce bias, leading to performance degradation instead of improvement.

==================================================

Chunk 133:
question, but it can also introduce bias, leading to performance degradation instead of improvement.The difference between the relationship between context relevancy and performance in open-form and closed-form questions suggests that the RAG is highly application-dependent. For example, the strategy for context retrieval for open-form applications should be different from the strategy used in closed-form applications. It is also important to be mindful when evaluating RAG, as common closed-form benchmarks might not be good indicators of RAG's performance in open-form applications.
    When designing an RAG, especially in open-form applications, it is important to include some other factors than pure embedding distance or relevancy. Sometimes including a piece of context that is not as close in embedding distance to the question might be helpful as it does not reinforce the hidden bias inside the question.

*{Data and Code Availability}

==================================================

Chunk 134:
helpful as it does not reinforce the hidden bias inside the question.

*{Data and Code Availability}Data and code can be found in the following GitHub repository: {https://github.com/mikelixiang88/context-matters.git}

*{Acknowledgements}

We would like to take this opportunity to thank Professor Stephan Haas for helpful discussions at the early stage of this project and Anurag Maravi for his engagement during the preliminary stage of the work.

*{Author Contributions}

X.L., H.T., and M.A. contributed to the conceptual design,
    X.L. and H.T. developed the Python code and conducted the experiments,
    X.L., H.T., S.C., and M.A. analyzed and interpreted the results.
    All authors equally contributed to the creation of the novel dataset,
    M.A. provided supervision and proposed the experiment measuring the impact of the context.
    All the authors contributed to writing the article.

*{Competing Interests}

The authors declare no competing interests.

*{ Supplementary Material}

==================================================

Chunk 135:
le.

*{Competing Interests}

The authors declare no competing interests.

*{ Supplementary Material}Given the wavelength of an electron is \(0.364  10^{-9} \, \), calculate the speed of the electron.

\\
\( = 0.364  10^{-9} \, \) \\
Mass of electron, \( m = 9.1  10^{-31} \,  \) \\
Planck's Constant, \( h = 6.62607015  10^{-34} \,  \) \\
The de Broglie wavelength is given by \(  = {mv} \) \\
Velocity of the electron, \( v = 2  10^6 \, ^{-1} \)

\\
The De Broglie states that $ = {{mv}}$. The mass of an electron is about $9.109  10^{-31} kg$

\\
Wave-particle duality is the concept in quantum mechanics that quantum entities exhibit particle or wave properties according to the experimental circumstances.

\\
Quantum physics is the study of matter and energy at the most fundamental level. At very small scale, classical theories may not be applicable any more. That is where quantum theories come into play.

==================================================

Chunk 136:
e, classical theories may not be applicable any more. That is where quantum theories come into play.\\
Bird feet can also vary greatly among different birds. Some birds, such as gulls and terns and other waterfowl, have webbed feet used for swimming or floating (Figure below). Other birds, such as herons, gallinules, and rails, have four long spreading toes, which are adapted for walking delicately in the wetlands (Figure below). You can predict how the beaks and feet of birds will look depending on where they live and what type of food they eat. Flightless birds also have long legs that are adapted for running. Flightless birds include the ostrich and kiwi. Some birds, such as gulls and terns and other waterfowl, have what type of feet used for swimming or floating?

\\
webbed

lobed
     quad toed
     bipedal
     webbed

For our task selections from the MetaICL dataset, please visit our GitHub repository, where the task category selections and code are presented.

==================================================

Chunk 137:
aset, please visit our GitHub repository, where the task category selections and code are presented.A 54-year-old man with ESRD is admitted for management of presumed catheter–related bacteremia. He had no pre–ESRD nephrology care and recently started maintenance hemodialysis on an urgent basis for symptomatic uremia. Two days ago, he developed acute onset of fever to 40 C, chills, and rigors during dialysis. After obtaining blood cultures, he received intravenous vancomycin and ceftazidime at the outpatient dialysis center before admission. His tunneled dialysis catheter is removed because of erythema and purulent exudate at the exit site. His fever is now resolved. He does not have back pain, cough, dental complaints, or rash. On physical examination, he is now afebrile. The BP is 136/88 mmHg. There is a 1-cm area of erythema at the tunneled dialysis catheter removal site. He has no stigmata of emboli. He has no obvious dental issues on gross inspection of the oropharynx. The cardiac examination shows a grade 2 early peaking systolic crescendo-decrescendo murmur along the right sternal border without radiation. The remainder of the examination is unremarkable. The blood cultures drawn before implementation of antibiotic therapy grew  within 12 hours. The results of sensitivity testing and repeat blood cultures are pending. Laboratory studies show sodium 138 mEq/L, potassium 3.8 mEq/L, chloride 102, total CO2 25 mmol/L, BUN 45 mg/dl, and creatinine 6.4 mg/dl.

==================================================

Chunk 138:
q/L, potassium 3.8 mEq/L, chloride 102, total CO2 25 mmol/L, BUN 45 mg/dl, and creatinine 6.4 mg/dl.In addition to continued antibiotic therapy, which ONE of the following is the next BEST step in management?

Arrange for placement of a tunneled dialysis catheter today
     Transesophageal echocardiography
     Magnetic resonance imaging of the spine
     Bone scintigraphy (bone scan)
     Examination for an occult dental source of bacteremia

\\
To calculate the speed of the electron, we can utilize de Broglie's equation that relates the momentum of a particle to its wavelength. Here is the formula:
\[
 = {p}
\]
Where:

\(  \) is the wavelength
     \( h \) is Planck's constant
     \( p \) is the momentum.

==================================================

Chunk 139:
\]
Where:

\(  \) is the wavelength
     \( h \) is Planck's constant
     \( p \) is the momentum.If we rearrange the terms to express momentum:
\[
p = {}
\]
The momentum of a particle can also be described as its mass times its velocity:
\[
p = m  v
\]
We know that the mass of an electron (\( m \)) is \( 9.11  10^{-31} \) kg. And Planck's constant (\( h \)) is \( 6.626  10^{-34} \) Js. \\
So the velocity (\( v \)) is given as:
\[
v = {m}
\]
Substituting the known values, we get:
\[
v = }{9.11  10^{-31}  0.364  10^{-9}}
\]
After solving this, the velocity of the electron comes out to be approximately \( 7.24  10^5 \) m/s.

\\
The speed of an electron given its wavelength can be calculated using the de Broglie's equation:
\[
 = {mv}
\]
where:

==================================================

Chunk 140:
lectron given its wavelength can be calculated using the de Broglie's equation:
\[
 = {mv}
\]
where:\(  \) is the wavelength,
     \( h \) is Planck's constant, which is roughly equals to \( 6.626  10^{-34} \,  \),
     \( m \) is the mass of the electron, which is roughly equals to \( 9.109  10^{-31} \,  \),
     \( v \) is the velocity of the electron.

We can rearrange the equation to solve for the velocity:
\[
v = {m}
\]
Substitute the given values into the equation:
\[
v =  \, }{9.109  10^{-31} \,   0.364  10^{-9} \, }
\]
and calculate the velocity.

\\
The wavelength of a matter wave, such as an electron, can be found using De Broglie's equation, which is given by:
\[
 = {p}
\]
where:

\(  \) is the wavelength
     \( h \) is Planck's constant (\( 6.626  10^{-34} \,  \))
     \( p \) is the momentum of the particle, which can be expressed as \( p = mv \)

==================================================

Chunk 141:
0^{-34} \,  \))
     \( p \) is the momentum of the particle, which can be expressed as \( p = mv \)Since we want to find the speed of the electron (\( v \)), we can rearrange this equation in terms of \( v \):
\[
v = {m}
\]
Given that \(  = 0.364  10^{-9} \,  \) and the mass of an electron is \( m = 9.109  10^{-31} \,  \), we can substitute these values into the equation:
\[
v = }{9.109  10^{-31}  0.364  10^{-9}}
\]
Evaluating this will give the speed of the electron in m/s.

\\
The De Broglie equation relates the wavelength of a particle to its momentum. According to De Broglie's principle, every object in motion has a wave character. The De Broglie wavelength is given by the formula:
\[
 = {mv}
\]
where:

\(  \) is the wavelength,
     \( h \) is Planck's constant (\( 6.626  10^{-34} \, ^2   /  \)),
     \( m \) is the mass of the particle,
     \( v \) is the velocity (or speed) of the particle.

==================================================

Chunk 142:
,
     \( m \) is the mass of the particle,
     \( v \) is the velocity (or speed) of the particle.We are given that \(  = 0.364  10^{-9} \,  \), and the mass \( m \) of an electron is \( 9.11  10^{-31} \,  \). We can find the speed \( v \) by rearranging the above formula:
\[
v = {m}
\]
Substitute the known values into the rearranged equation to find \( v \), the speed of the electron:
\[
v =  \, ^2   /  }{(9.11  10^{-31} \, ) (0.364  10^{-9} \, )}
\]
Solving the above expression will give us the speed of the electron.

==================================================

Chunk 143:
} \, ) (0.364  10^{-9} \, )}
\]
Solving the above expression will give us the speed of the electron.To better evaluate the open response to our physics questions, we modified the potato annotation system  and applied it as our evaluation system. Our evaluation system not only allows users to select numeric grades for each response but also enables the user to highlight parts of the response, apply labels, and write descriptions to justify their grading. In addition, the system randomly shuffles the order of the responses for each grader to mitigate any potential bias in grading as a result of the ordering of responses. A short video tutorial is provided at the beginning page to provide guidance and alignment in grading.

A screenshot of the interface of the evaluation system is shown in Fig.~. The system is also accessible via the link: {}.

[b!]

==================================================

Chunk 144:
ce of the evaluation system is shown in Fig.~. The system is also accessible via the link: {}.

[b!]To check whether our context relevancy is well defined, we compute the embedding of the questions and their respective contexts for both our open-form question dataset and the two closed-form question datasets we use. We then calculate the cosine distance between the embedding of each question and the different contexts associated with them. We show the results for the open question dataset in Fig.~.

==================================================

Chunk 145:
different contexts associated with them. We show the results for the open question dataset in Fig.~.We computed the embedding of each question and each context using OpenAI's ``text-embedding-3-large'' model. For the no-context part, we used a space as a placeholder instead of an empty string. 
    As expected, the results show that more relevant contexts, as perceived by us when designing the dataset, receive a higher mean similarity score with their respective questions. Different question types can result in a large standard deviation in similarity scores in different contexts.
    We show the details breakdown of those results in Fig.~.

[b!]

[b!]

All question types except hard paraphrased questions display the same trend, confirming the relationship between context types and embedding similarities.

==================================================

Chunk 146:
isplay the same trend, confirming the relationship between context types and embedding similarities.For the closed datasets, the similarity score between context and question is shown in Table . For both datasets, the same task/subject demonstrations possess a higher mean similarity score than the different task/subject demonstrations. To further verify this relationship, we have also plotted the similarity score of the same task demonstrations and different task demonstrations for each task in the MetaICL dataset in Fig. . The results confirm that the same task demonstration displays higher mean similarity than the different task demonstration in every task in the dataset.

{1.5}
    [h]

{c|c|c}

&  &  \\ 
        MetaICL           & 0.719                                     & 0.787                                 \\ 
        NephSAP           & 0.443                                     & 0.557                                 \\

[h]

==================================================

Chunk 147:
In this section, we show the overall experimental results with in-depth analyses of our framework.

First of all, Table  shows that our -refined top-1 document consistently outperforms the original top-1 document across all datasets and scenarios, despite reduced token counts. This confirms our hypothesis that the redundant information within the fix-sized passages adversely affects the RAG performance and highlights the importance of providing only query-relevant information in RAG with finer-grained sentences.

==================================================

Chunk 148:
hts the importance of providing only query-relevant information in RAG with finer-grained sentences.Furthermore,  also shows performance enhancement over specialized datasets, such as ever-evolving RQA and domain-specific SQ and BASQ datasets.
Specifically, the re-rankers based on pre-trained models such as T5 and the LLM demonstrate remarkable performance improvement.
Given that  requires no additional training, the robust and effective performance suggests its applicability to diverse real-world scenarios, particularly where queries frequently change across different timelines and domains.

in Multiple Passages.}
To assess the effectiveness and efficiency of  in multiple passages, we gradually increased the number of documents \(N\) and compared the performance, token count, and end-to-end (E2E) latency of the original top-\(N\) documents with those refined by .

==================================================

Chunk 149:
oken count, and end-to-end (E2E) latency of the original top-\(N\) documents with those refined by .As shown in the left panel of Figure , both sets of documents show consistent performance improvements as \(N\) increases. However,  consistently outperforms the original documents across all \(N\) levels, with more notable differences at lower \(N\) values. This suggests that  can significantly enhance performance in RAG, even as the number of documents increases.

==================================================

Chunk 150:
gests that  can significantly enhance performance in RAG, even as the number of documents increases.Due to the quadratic increase in memory and time requirements with the number of tokens in transformer-based LLMs, reducing the token count is crucial for improving efficiency . As depicted in the center and right panels of Figure ,  substantially reduces the token count compared to the original documents, with the difference becoming more significant as \(N\) increases. This reduction in tokens also decreases E2E latency in all scenarios except top-1. Notably, at top-10, while the performance difference is minimal (39.6 vs. 39.7), the token count reduction from 1,713 to 577 (nearly 2.97 times) and the corresponding E2E latency reduction from 7.382 seconds to 5.422 seconds (nearly 2 seconds) demonstrate that  can enhance both performance and efficiency in RAG. Detailed results are available in Table .

==================================================

Chunk 151:
that  can enhance both performance and efficiency in RAG. Detailed results are available in Table .To examine the impact of varying \( T \), we adjusted the threshold in increments of 10, starting from the 10th percentile, and measured the resulting performance. Additionally, to explore the theoretical maximum performance of our method, we configured an oracle setting where any correct response, regardless of the threshold setting, was counted as correct.

As shown in Figure , increasing the threshold \( T \) generally improves performance by removing irrelevant content, thus reducing the number of tokens. However, our experimental results revealed that the performance at the 90th percentile threshold was 29.4, while a lower 80th percentile threshold yielded better performance at 29.9. This indicates that an overly stringent threshold can also remove essential information, suggesting that task-specific threshold fine-tuning could improve results.

==================================================

Chunk 152:
ve essential information, suggesting that task-specific threshold fine-tuning could improve results.Furthermore, in the oracle setting, accuracy significantly improved to 34.1, and the token count was reduced to 77. This shows a marked performance improvement over the best performing threshold (80th percentile), with a similar reduction in tokens. This result implies that dynamically adjusting the threshold based on the query could achieve substantial performance improvements with a comparable number of tokens, suggesting an area for future work. Detailed results are available in Table .

==================================================

Chunk 153:
able number of tokens, suggesting an area for future work. Detailed results are available in Table .The left panel of Figure  displays the distribution of token counts in documents refined by . Unlike methods that trim passages to a fixed length,  reduces token counts based on a relevance score threshold, resulting in a wide distribution of token counts, with many instances nearly devoid of external knowledge. The average token count post-refinement is 46. We analyzed performance by comparing this approach with cases where passages are consistently cut to 46 tokens: one where passages are simply truncated at 46 tokens, another using sentence-level re-ranking to select the most relevant sentences up to 46 tokens, and a third where sentences are randomly cut to 46 tokens.

==================================================

Chunk 154:
most relevant sentences up to 46 tokens, and a third where sentences are randomly cut to 46 tokens.As demonstrated in the right panel of Figure , , which trims content based on relevance, significantly outperforms methods that trim to a fixed length, improving scores from 25.3 to 33.7. This suggests that trimming based on relevance score thresholds, rather than a fixed length, is more effective. This method accommodates the variability in the amount of relevant information per query, indicating that non-essential content should be dynamically removed.

==================================================

Chunk 155:
relevant information per query, indicating that non-essential content should be dynamically removed.To assess the effectiveness of sentence-level re-ranking within our framework, we compared it to conventional passage-level re-ranking using the same context length in RAG, under an initial top-100 retrieval setting. Figure  demonstrates that sentence-level re-ranking markedly outperforms passage-level re-ranking by enhancing performance through increased information density at a finer granularity. Additionally, while dense retrievers and fine-tuned ranking
models demonstrate improvements as re-rankers, BM25 as a re-ranker significantly decreases the performance. This highlights the limitations of lexcial-based retrieval for assessing low-granularity, sentence-level relevance, underscoring the necessity for semantic understanding in sentence ranking tasks.
Moreover, off-the-shelf ranking models, originally designed for passage-level relevance assessment, are also effective at determining relevance at the more granular level of individual sentences. Interestingly, even though it is not specifically trained for ranking tasks, the unsupervised re-ranker using LLMs shows remarkable performance in sentence-level re-ranking.

==================================================

Chunk 156:
ks, the unsupervised re-ranker using LLMs shows remarkable performance in sentence-level re-ranking.To see how each step in  contributes to the overall performance, we conduct the ablation studies, the results shown in Table , for the sentence-level re-ranking and reconstruction steps. These studies were uniquely tailored to the variable token counts reduced by , rather than using a fixed length.

First, we examine the impact of removing the sentence-level re-ranking step. In this scenario, after initially retrieving the top-1 passage, the results are decomposed into sentences. Subsequently, these sentences are randomly used as sources for generating answers. The performance drastically drops from 33.7 to 30.6 on the NQ, highlighting the crucial role of sentence-level re-ranking, which helps effectively filter out query-irrelevant information based on relevance scores.

==================================================

Chunk 157:
-ranking, which helps effectively filter out query-irrelevant information based on relevance scores.Furthermore, we explore the effectiveness of the reconstruction step. The performance also drops from 64.1 to 63.8 on the TQA. This finding is similar to those from , which suggests that removing contextual coherence negatively affects the performance. Therefore, in , reconstructing the order of sentences to reflect their original sequence within the retrieved passage is an essential step. Interestingly, the widely used approach of prepending external knowledge in descending order of relevance scores is not effective in our sentence-level refinement framework, showing similar results to a randomly ordered setting.

.}

==================================================

Chunk 158:
our sentence-level refinement framework, showing similar results to a randomly ordered setting.

.}We further compare our  to the concurrent supervised refinement method, RECOMP , which requires additional training steps for refining the retrieved documents.
To be specific, RECOMP is designed to refine the retrieved passages by either abstractively or extractively summarizing them with additional models.
Note that due to significant differences between supervised and unsupervised schemes, directly comparing  with RECOMP on an apples-to-apples basis is difficult.
However, to ensure as fair a comparison as possible, we evaluate both refining methods under the same conditions by adopting a two-sentence extraction context length, following the extractive setting used for RECOMP. 
Additionally, RECOMP's extractive compressor, which requires Contriever to be fine-tuned on specific datasets, shares similarities with our  implementation that also uses Contriever, though ours is not additionally fine-tuned.

==================================================

Chunk 159:
ties with our  implementation that also uses Contriever, though ours is not additionally fine-tuned.Figure  shows the results of the comparison between  and RECOMP in both in-domain and out-of-domain settings. While RECOMP shows robust performance on the in-domain datasets where it is particularly trained, its performance drops drastically for the out-of-domain settings, notably for BASQ from 54 to 47.9. This indicates the challenges of dataset-specific tuning for the supervised refinement methods. On the other hand, our  with RankT5 and RG shows robust performance even without additional training steps for refinement.

We conduct a case study of the  framework in Table .

==================================================

Chunk 160:
hout additional training steps for refinement.

We conduct a case study of the  framework in Table .Specifically, a conventional fixed-size passage may contain distractors, such as unrelated knowledge and irrelevant conceptual details about Nitrogen (highlighted in red).
Note that, although the retrieved passage-level document includes `Oxygen', which is the correct answer to the given query, the LLM used as the reader fails to generate the accurate answer by being distracted by irrelevant information.

On the other hand,  effectively filters out such query-irrelevant sentences. Furthermore,  also helps focus on the information closely related to the query (highlighted in blue), thus correctly generating the answer.

==================================================

Chunk 161:
In this section, we describe the experimental setup for evaluating  across various scenarios. We provide additional details in Appendix .

We use BM25~ as a passage-level retriever, which is a widely used sparse retriever due to its notable performance with high efficiency. The retriever fetches the  passage-level query-relevant document from an external corpus, which serves as the baseline document.

==================================================

Chunk 162:
assage-level query-relevant document from an external corpus, which serves as the baseline document.We operationalize a variety of ranking models as re-rankers, including off-the-shelf retrievers, fine-tuned re-rankers, and LLMs.
 We use ~ as a sentence-level re-ranker. Note that BM25 is only applied at the sentence level, as it is primarily utilized in the retrieval step.
 We utilize two representative dense retrievers, ~ and ~, which are better at capturing the semantic similarity between documents and queries than sparse retrievers.
}:} We employ two supervised re-ranking models based on T5~, ~ and ~. These models are specifically trained for pointwise document ranking tasks.
:}
We explore ~, a pointwise ranking method using the inherent ranking ability of LLMs, validating its effectiveness in scenarios lacking extensive labeled data.
We use LLama2-13b-chat~ as a ranking model for .

==================================================

Chunk 163:
veness in scenarios lacking extensive labeled data.
We use LLama2-13b-chat~ as a ranking model for .We use the instruction-tuned, open-source LLM  as our reader. To generate the final answer, the document is prepended to the system prompt.

We evaluate our  across 6 open-domain QA datasets, including both general and specific domains.
First, we conduct our experiment using the development set of ~, ~, and ~, consisting of queries with general topics.

Additionally, we incorporate specialized datasets such as ~, ~, and ~ for evaluating the generalizability of our proposed method.
In detail, RQA includes questions that are updated periodically to test our system's ability to handle ever-evolving knowledge. 
In addition, SQ and BASQ are domain-specific datasets in science and biology, respectively.
Specifically, for BASQ, we selectively use the questions from the BioASQ6 challenge (task b) that are suitable for yes/no and factoid responses.

==================================================

Chunk 164:
he questions from the BioASQ6 challenge (task b) that are suitable for yes/no and factoid responses.We report the effectiveness of our framework with , which determines whether the prediction contains golden answers, following~.

==================================================

Chunk 165:
of our framework with , which determines whether the prediction contains golden answers, following~.The threshold \( T \), used to remove irrelevant content, was determined empirically by sampling 1,000 random entries from each of the NQ, TQA, and SQD training sets and setting \( T \) to the relevance score at the 90th percentile. Detailed values of \( T \) for various models are provided in Table .
The retrieval corpus for NQ, TQA, and SQD is a pre-processed Wikipedia dump from Dec. 20, 2018 following~, and for BASQ and RQA, we use their own retrieval corpora.
To be specific, BASQ used the BEIR (v1.0.0)~} BioASQ corpus, specializing in biomedical information retrieval.
For the RQA dataset, spanning from 2022 to 2023, we use the search documents provided at the time of dataset creation through the Google Cloud Search (GCS) API to align the periods of the queries and answers.

==================================================

Chunk 166:
creation through the Google Cloud Search (GCS) API to align the periods of the queries and answers.When implementing each component in , we decompose passage-level documents into sentences using the Sentencizer from Spacy}. All predictions in our experiments are generated via greedy decoding.

==================================================

Chunk 167:
Information Retrieval (IR) is the task of searching for query-relevant documents from a large corpus , which has been widely applied for both search systems and various NLP tasks such as open-domain QA ~.

IR models can be categorized into sparse retrievers , which use lexical metrics to calculate relevance scores between queries and documents, and dense retrievers , which embed queries and documents into a dense space that captures semantic relationships but requires significant computational resources .

In order to further enhance retrieval performance, additional strategies have been proposed.
Specifically, the re-ranking strategy improves retrieval performance by recalculating relevance scores using an additional re-ranking model , and then reordering the documents based on these scores.
Recently, LLMs have shown remarkable re-ranking performance by generating relevance labels without requiring further fine-tuning .

==================================================

Chunk 168:
rkable re-ranking performance by generating relevance labels without requiring further fine-tuning .While the aforementioned work on IR  generally assumes fixed-size, 100-word passages as the document length, some work has explored an optimal level of retrieval granularity~.
These approaches validate that a fine-grained level of granularity, containing only the knowledge needed to answer the query, can enhance the overall performance by excluding redundant details in the lengthy retrieved documents. 
However, reducing retrieval granularity to the sentence level can disrupt the original context and result in a loss of the document’s coherence .
In addition, sentence-level retrieval generally requires a much larger index size compared to passage-level retrieval .
By contrast, we investigate a novel framework for effectively re-ranking sentences within retrieved passage-level documents and then reconstructing the re-ranked sentences to preserve contextual integrity.

==================================================

Chunk 169:
ge-level documents and then reconstructing the re-ranked sentences to preserve contextual integrity.RAG has emerged as a promising solution for addressing LLMs' hallucination issues by leveraging external knowledge fetched by the retrieval module. 
Specifically, RAG incorporates retrieval modules that reduce the need to update the parameters of LLMs and help them generate accurate and reliable responses~.

Additionally, various real-world applications integrate RAG as a core component when deploying LLM-based services~. 
However, they still have limitations due to the imperfections of the retrieval module within RAG, where the retrieved documents containing query-irrelevant information can negatively lead the LLMs to generate inaccurate answers.

==================================================

Chunk 170:
containing query-irrelevant information can negatively lead the LLMs to generate inaccurate answers.To address them, several studies have attempted to leverage the capabilities of LLMs to enhance their resilience against irrelevant knowledge. 
These approaches include crafting specialized prompts~, training plug-in knowledge verification models~, adaptively retrieving the required knowledge~, and augmenting knowledge using the capabilities of the LLM itself .

==================================================

Chunk 171:
rieving the required knowledge~, and augmenting knowledge using the capabilities of the LLM itself .Among the promising solutions, recent studies show that further refining the retrieved documents into fine-grained knowledge can improve the RAG performance~.
However, such refinement strategies generally require additional fine-tuning on a specific dataset, which might result in limited generalizability and high computational cost. 
By contrast, our proposed refinement framework removes irrelevant information with unsupervised sentence-level re-ranking and reconstruction steps by using off-the-shelf ranking models without requiring additional training costs.

==================================================

Chunk 172:
In this work, we present , a novel unsupervised document refinement framework that enhances the performance of RAG systems. The  framework aids RAG systems to generate more accurate answers by decomposing passages into sentences, re-ranking them based on each relevance score, and then reconstructing them to preserve the continuity and coherence of the context. Our comprehensive experiments on multiple QA datasets show that  consistently outperforms the conventional approaches of using fixed-size passage in RAG, especially in ever-evolving and domain-specific contexts. Our ablation studies highlight the importance of sentence-level re-ranking and contextual reconstruction for improvement on RAG. We believe that  suggests a promising research direction for refining document retrieval without additional training, together with potential applications across a wide range of knowledge-intensive NLP tasks by integrating more diverse retrieval or ranking models.

==================================================

