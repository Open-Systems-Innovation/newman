Chunk 1:
In this section, we describe the experimental setup for evaluating  across various scenarios. We provide additional details in Appendix .

We use BM25~ as a passage-level retriever, which is a widely used sparse retriever due to its notable performance with high efficiency. The retriever fetches the  passage-level query-relevant document from an external corpus, which serves as the baseline document.

==================================================

Chunk 2:
assage-level query-relevant document from an external corpus, which serves as the baseline document.We operationalize a variety of ranking models as re-rankers, including off-the-shelf retrievers, fine-tuned re-rankers, and LLMs.
 We use ~ as a sentence-level re-ranker. Note that BM25 is only applied at the sentence level, as it is primarily utilized in the retrieval step.
 We utilize two representative dense retrievers, ~ and ~, which are better at capturing the semantic similarity between documents and queries than sparse retrievers.
}:} We employ two supervised re-ranking models based on T5~, ~ and ~. These models are specifically trained for pointwise document ranking tasks.
:}
We explore ~, a pointwise ranking method using the inherent ranking ability of LLMs, validating its effectiveness in scenarios lacking extensive labeled data.
We use LLama2-13b-chat~ as a ranking model for .

==================================================

Chunk 3:
veness in scenarios lacking extensive labeled data.
We use LLama2-13b-chat~ as a ranking model for .We use the instruction-tuned, open-source LLM  as our reader. To generate the final answer, the document is prepended to the system prompt.

We evaluate our  across 6 open-domain QA datasets, including both general and specific domains.
First, we conduct our experiment using the development set of ~, ~, and ~, consisting of queries with general topics.

Additionally, we incorporate specialized datasets such as ~, ~, and ~ for evaluating the generalizability of our proposed method.
In detail, RQA includes questions that are updated periodically to test our system's ability to handle ever-evolving knowledge. 
In addition, SQ and BASQ are domain-specific datasets in science and biology, respectively.
Specifically, for BASQ, we selectively use the questions from the BioASQ6 challenge (task b) that are suitable for yes/no and factoid responses.

==================================================

Chunk 4:
he questions from the BioASQ6 challenge (task b) that are suitable for yes/no and factoid responses.We report the effectiveness of our framework with , which determines whether the prediction contains golden answers, following~.

==================================================

Chunk 5:
of our framework with , which determines whether the prediction contains golden answers, following~.The threshold \( T \), used to remove irrelevant content, was determined empirically by sampling 1,000 random entries from each of the NQ, TQA, and SQD training sets and setting \( T \) to the relevance score at the 90th percentile. Detailed values of \( T \) for various models are provided in Table .
The retrieval corpus for NQ, TQA, and SQD is a pre-processed Wikipedia dump from Dec. 20, 2018 following~, and for BASQ and RQA, we use their own retrieval corpora.
To be specific, BASQ used the BEIR (v1.0.0)~} BioASQ corpus, specializing in biomedical information retrieval.
For the RQA dataset, spanning from 2022 to 2023, we use the search documents provided at the time of dataset creation through the Google Cloud Search (GCS) API to align the periods of the queries and answers.

==================================================

Chunk 6:
creation through the Google Cloud Search (GCS) API to align the periods of the queries and answers.When implementing each component in , we decompose passage-level documents into sentences using the Sentencizer from Spacy}. All predictions in our experiments are generated via greedy decoding.

==================================================

