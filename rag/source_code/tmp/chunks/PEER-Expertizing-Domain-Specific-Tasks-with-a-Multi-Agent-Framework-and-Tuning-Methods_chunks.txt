Chunk 1:
=1

{
 , Xiaojing Li, Binzhu Wang, Yueyang Zhou}, \\
 , Hong Chen Jinshi Zhang, Fei Yu, Zewei Zhao, Song Jin, Renji Gong, Wanqing Xu} \\
 AntGroup
}

{} These authors contributed equally to this work.}
Corresponding author: jihan.hanji@antgroup.com}

==================================================

Chunk 2:
{} These authors contributed equally to this work.}
Corresponding author: jihan.hanji@antgroup.com}In domain-specific applications, GPT-4, augmented with precise prompts or Retrieval-Augmented Generation (RAG), shows notable potential but faces the critical  of performance, cost, and data privacy. High performance requires sophisticated processing techniques, yet managing multiple agents within a complex workflow often proves costly and challenging. To address this, we introduce the PEER (lan, xecute, xpress, eview) multi-agent framework. This systematizes domain-specific tasks by integrating precise question decomposition, advanced information retrieval, comprehensive summarization, and rigorous self-assessment. Given the concerns of cost and data privacy, enterprises are shifting from proprietary models like GPT-4 to custom models, striking a balance between cost, security, and performance. We developed industrial practices leveraging online data and user feedback for efficient model tuning. This study provides best practice guidelines for applying multi-agent systems in domain-specific problem-solving and implementing effective agent tuning strategies. Our empirical studies, particularly in the financial question-answering domain, demonstrate that our approach achieves 95.0\

==================================================

Chunk 3:
articularly in the financial question-answering domain, demonstrate that our approach achieves 95.0\Advanced LLMs like GPT-4, enhanced with engineered prompts or Retrieval-Augmented Generation (RAG), show great potential in handling complex tasks across various domains . However, deploying these models involves a critical  of performance, cost, and data privacy.

While domain-specific applications benefit from meticulously fine-tuned models , this approach incurs high costs due to the extensive resources needed for training and data acquisition. Alternatively, multi-agent systems have proven effective , especially in complex tasks with distinct and conflicting role requirements that challenge even advanced models. However, current implementations often involve dynamic and complex workflows, increasing costs and complicating reproducibility. Consequently, enterprises are shifting from proprietary models like GPT-4 to custom models that better balance cost, security, and performance.

==================================================

Chunk 4:
proprietary models like GPT-4 to custom models that better balance cost, security, and performance.To address these challenges, we introduce the PEER (lan, xecute, xpress, eview) multi-agent framework. This framework incorporates precise question decomposition, advanced information retrieval, comprehensive summarization, and rigorous self-assessment, aiming to streamline workflows and enhance problem-solving efficacy. Additionally, our research addresses enterprise demands for private deployment and stringent data privacy by developing industrial best practices that leverage online data and user feedback for effective model tuning. These practices are crucial for optimizing custom model performance while ensuring cost-efficiency and robust data privacy.

==================================================

Chunk 5:
cial for optimizing custom model performance while ensuring cost-efficiency and robust data privacy.The main contributions of this study include:
1. Providing and open-sourcing the PEER framework, characterized by its conciseness, effectiveness, and cost-efficiency, for effectively tackling domain-specific tasks. In experiments, it demonstrates superior performance compared to BabyAGI.\\
2. Proposing a customized agent tuning strategy for 10-billion-parameter models, achieving performance comparable to GPT-4. \\
3. Constructing and open-sourcing a dataset for use within the PEER framework, applicable to agent training, pre-training, and supervised fine-tuning in various financial analysis scenarios.

==================================================

Chunk 6:
to agent training, pre-training, and supervised fine-tuning in various financial analysis scenarios.With the advent of large models, we simulate the collaborative processes of human experts (e.g. financial) using multiple agents, achieving comparable interpretative results. This approach is encapsulated in the Plan, Execute, Express and Review (PEER) framework, where domain specific (e.g. financial) tasks are divided into these four steps. Each agent specializes in a single task, working together to accomplish the overall objective. The prompt for this section is attached in .

The "Plan" agent uses a model to generate multiple related sub-questions from users’ domain specific (e.g. financial) queries. These sub-questions serve as an interpretation framework, breaking down the original query into specific and actionable criteria, and expanding it for a comprehensive analysis.

==================================================

Chunk 7:
original query into specific and actionable criteria, and expanding it for a comprehensive analysis.The "Execute" agent gathers information for each sub-question identified by "Plan". Using these sub-questions as search criteria, it finds relevant information from news, domain specific (e.g. financial) data, reports, and articles, enhancing accuracy, efficiency, and comprehensiveness. This information forms the foundation for interpreting domain events and answering questions.

The "Express" agent synthesizes collected information to perform comprehensive large-model reasoning, forming final conclusions. It emphasizes integrative reasoning and delivers professional descriptions tailored to the user's requirements.

The "Review" agent evaluates whether the "Express" agent's answer meets pre-established criteria. If satisfied, the final answer is delivered; if not, it provides modification suggestions and initiates another PEER iteration, enhancing answer quality through feedback.

==================================================

Chunk 8:
ication suggestions and initiates another PEER iteration, enhancing answer quality through feedback.The PEER multi-agent cooperation framework’s strong reasoning and analysis abilities stem from its efficient task allocation, cooperation, and the feedback loop and self-optimization enabled by the "Review" agent. This ensures that the answers continuously improve towards the optimal solution. If an answer does not meet user requirements, the "Review" agent suggests modifications for the "Plan," "Execute," or "Express" agents. The relevant agent then adjusts its process to better meet expectations. For some simple tasks, one or more agents in PEER process can be skipped to simplify the procedure. For complex tasks, a nested pattern can be used, designing each agent to perform an isolate PEER process to enhance entire performance.For a more comprehensive understanding of the PEER framework, refer to Figure , which illustrates how these four agents synergize.

[htbp]

\\

==================================================

Chunk 9:
the PEER framework, refer to Figure , which illustrates how these four agents synergize.

[htbp]

\\Supervised fine-tuning typically employs the cross-entropy loss:
\[
(y, ) = - _{i=1}^{N} _{j=1}^{C} y_{ij} (_{ij}),
\]
where \( N \) is the number of training examples, \( C \) is the number of classes, \( y_{ij} \) is the ground truth one-hot encoded vector, and \( _{ij} \) is the predicted probability for class \( j \). We used a robust model to generate an offline training dataset $_{off}$, which was then refined and validated by human annotators for quality assurance.

==================================================

Chunk 10:
ng dataset $_{off}$, which was then refined and validated by human annotators for quality assurance.Rejection sampling, as used in LLaMA2 , involves generating samples from a pre-trained model and filtering based on quality criteria to retain only high-quality examples. Unlike direct offline supervised fine-tuning (SFT), rejection sampling automates initial filtering to reduce low-quality samples before human annotation. In our iterative training process, rejection sampling boosts performance post offline dataset training.

Direct Preference Optimization (DPO), has emerged as efficient alternatives to RLHF, eliminating the need for a separate reward model . The loss function for DPO is defined as follows:

\[

_{}(_ ; _{}) 
= -_{(x, y_w, y_l)  } [ &  (   {_{} (y_w  x)} \\
&-   {_{} (y_l  x)} ) ]

\]

==================================================

Chunk 11:
ows:

\[

_{}(_ ; _{}) 
= -_{(x, y_w, y_l)  } [ &  (   {_{} (y_w  x)} \\
&-   {_{} (y_l  x)} ) ]

\]where $_$ is the language model being optimized and $_{}$ refers to the model after SFT ($^{}$). The scaling factor $$ measures errors in ranking results and accounts for the KL constraint. In vanilla/offline Direct Preference Optimization (DPO), the model is optimized using  \((x, y_w, y_l)  \), where the dataset-generating model and the optimized model are .

When optimizing DPO models, offline preference datasets and off-policy updates can cause generalization issues with out-of-distribution (OOD) queries. These issues can be mitigated by incorporating online preference datasets and using on-policy learning approaches. .

We follow the experimental setup of , utilizing a batch size of $m$ in online setting. Our methodology integrates the LLM-as-a-Judge approach for real-time feedback, as introduced by , to refine the model progressively.

==================================================

Chunk 12:
LM-as-a-Judge approach for real-time feedback, as introduced by , to refine the model progressively.Algorithm  outlines our iterative training process, starting with the initial dataset $D_$. The agent processes each batch iteratively, involving model evaluation, data generation, and refinement. It generates multiple candidate responses per input, using a reward model (GPT-4o) to select the optimal response and compare it with the ground truth. If the model-generated response exceeds the quality threshold, it replaces the original training sample. For DPO, the lowest-ranked response is identified as a negative example. The updated dataset is then used to refine the model via SFT or DPO techniques. After multiple iterations, the algorithm outputs the best-performing model variant based on predefined metrics. This iterative process continuously enhances response quality, creating a self-refining training paradigm that progressively improves model performance. Figure  illustrates this process.

==================================================

Chunk 13:
g training paradigm that progressively improves model performance. Figure  illustrates this process.[1]
{}\\

{$D_$, $D_$, $T$, $N_$, $M_$}

Initialize $M_0^$ and $M_0^$ using $D_$ 
     $T$}
         Evaluate $M_{i-1}^$ and $M_{i-1}^$ on $D_$
         \{SFT, DPO\}}
             $D_i^  $
            )$  $D_$}
                 $R  (N_, M_{i-1}^(q))$

$r_  _{r  R} M_(q, r)$
(q, r_) > M_(q, r_)$}
     $r_  r_$

$r_  r_$

$r_  _{r  R} M_(q, r)$
                     Add $(q, r_, r_)$ to $D_i^$

Add $(q, r_)$ to $D_i^$

Train $M_i^$ using $D_i^$

Best $M_N^$, $M_N^$, and evaluation results

$: offline training dataset, 
$D_$: evaluation dataset, 
$T$: number of iterations, 
$N_$: number of candidate responses generated each time, 
$M_$: reward model}

[htbp]

==================================================

Chunk 14:
f iterations, 
$N_$: number of candidate responses generated each time, 
$M_$: reward model}

[htbp]is trained on offline data. This model then generates two sets of predictions: one to create training data for the next iteration (upper section) and another to provide evaluation results for the current iteration (lower section). This cycle is repeated iteratively across subsequent training phases.}

We conduct experiment on a real-word industry financial QA dataset to validate the PEER framework discussed in section  and evaluate the agent tuning methods discussed in section .

Since the main usage scenario of PEER framework is the interpretation and analysis of domain events and problems, we mainly tested and compared the performance of PEER on the dataset of financial QA. We sampled hundreds of professional questionsfrom our business scenarios and divided them into nine categories. Details of the dataset distribution are shown in Table .

==================================================

Chunk 15:
rios and divided them into nine categories. Details of the dataset distribution are shown in Table .We conducted experiments using two base models, GPT-3.5 turbo (16k) and GPT-4o, with Python for execution. For FinQA datasets, we compared with the BabyAGI multi-agent framework due to its similar task creation, organization, and execution capabilities to PEER.

To assess the impact of the "Review" agent in the PEER framework, we designed self-ablation experiments with and without the "Review" agent. We set the maximum rounds for both BabyAGI and PEER (with "Review") to 5 and used Google for information retrieval. Under GPT-3.5 turbo (16k), we recalled the top 2 search results with a token limit of 13,000. For GPT-4o, we increased the parameters to the top 6 and 125,000 tokens, leveraging the model's enhanced performance.

==================================================

Chunk 16:
creased the parameters to the top 6 and 125,000 tokens, leveraging the model's enhanced performance.Despite GPT-4’s widespread use for evaluations, its confidence can be influenced by position and verbosity biases . To mitigate these issues, we have developed two evaluation methodologies based on GPT-4:

GPT-4 scores all answers across various dimensions, and we calculate the average score for each dimension. Detailed scoring dimensions, rules, and their meanings are provided in Table  in the Appendix.
     GPT-4 selects the best answer between those provided by PEER and the control group. For this evaluation, we use the win rate as the metric, with selection criteria outlined in Table .

[htbp]

[!h]

{1.5}
    {c|c|c|c|c|c|c|c|c|c|c}

{*}{ Experimental Setup} &{*}{ Model Type} &{*}{Framework Type} &{c}{Evaluation Dimension} \\

& & & Integrity & Relevance & Compactness & Factuality & Logic & Structure & Comprehensiveness & Average\\

==================================================

Chunk 17:
Integrity & Relevance & Compactness & Factuality & Logic & Structure & Comprehensiveness & Average\\{*}{ Comparison Experiment}&{*}{ GPT-3.5-turbo-16K}& BabyAGI & 3.49 & 3.79 & 3.55 & 3.84 & 3.94 & 3.76 & 3.47 & 3.69 \\

& & PEER &  &  &  &  &  &  &  &  \\

& {*}{ GPT-4o}& BabyAGI & 3.16 & 3.32 & 3.32 & 3.98 & 3.78 & 3.86 & 3.14 & 3.51 \\

& & PEER &  &  &  &  &  &  &  &  \\

{*}{ Ablation Experiment}&{*}{ GPT-3.5-turbo-16K}& PEE & 3.91 & 4.30 &  & 4.26 & 4.30 & 4.14 & 3.78 & 4.09 \\

& & PEER &  &  & 3.67 &  &  &  &  &  \\

& {*}{ GPT-4o}& PEE & 4.81 & 4.94 &  & 4.72 & 4.92 & 4.90 & 4.81 & 4.73 \\

& & PEER &  & 4.94 & 3.83 &  & 4.92 &  &  &  \\

==================================================

Chunk 18:
.81 & 4.94 &  & 4.72 & 4.92 & 4.90 & 4.81 & 4.73 \\

& & PEER &  & 4.94 & 3.83 &  & 4.92 &  &  &  \\In the comparative experiment with BabyAGI, as depicted in Table  and Figure , PEER consistently surpasses BabyAGI in both average score and win rate, irrespective of the base model employed. PEER demonstrates superior performance in dimensions such as integrity, relevance, logic, structure, and comprehensiveness, often by a margin exceeding one point. Specifically, under the GPT-3.5 turbo (16k) model, PEER achieves a win rate of 83\

In the ablation experiment, as illustrated in Table  and Figure 3, PEER scores higher in most dimensions and attains a 64\

==================================================

Chunk 19:
ment, as illustrated in Table  and Figure 3, PEER scores higher in most dimensions and attains a 64\We conducted two categories of experiments: one focusing on individual agents and the other on the entire workflow. Dataset sizes are provided in Table , with the data being open-sourced. The test set for individual agents is derived from the intermediate results of the evaluation set detailed in Table , whereas the test set for the entire workflow corresponds directly to Table .

As in Section , for the evaluation of individual agents and the entire workflow, we also employed the LLM-as-a-Judge approach. Specifically, for individual agents, we used scoring and pairwise comparison to evaluate the performance of each iteration. For the entire workflow, we used GPT-4o to score and compare the results of GPT-4 + PEER, the SFT results using offline data, and the best model obtained through iterative training.

==================================================

Chunk 20:
+ PEER, the SFT results using offline data, and the best model obtained through iterative training.Figure  illustrates the win, tie, and loss rates across different iterations for three agents involved in planning, execution, and expression. Both DPO and SFT show progress with each iteration. For example, for the planning agent, the first iteration of SFT achieves a win rate of 43.15\

[htbp]

{1.5}
{c|c|c|c|c|c|c|c|c}

{*}{ Model Type} & {c}{Evaluation Dimension} \\

& Integrity & Relevance & Logic & Comprehensiveness & Compactness & Factuality & Structure & Average \\

QWEN1.5-14B (sft-offline)+ PEER      & 4.09      & 4.58      & 3.34  & 4.23              & 4.32        & 4.22       & 4.03      & 4.12    \\

QWEN1.5-14B (iter-best-model) + PEER & 4.4       & 4.63      & 3.42  & 4.35              & 4.61        & 4.77       & 4.28      & 4.35    \\

GPT4 + PEER &  &  &  &  &  &  &  &  \\

==================================================

Chunk 21:
& 4.61        & 4.77       & 4.28      & 4.35    \\

GPT4 + PEER &  &  &  &  &  &  &  &  \\{Evaluation of the entire workflow: the model after iterative training(QWEN1.5-14B (iter-best-model) + PEER), shows improvements across all metrics compared to the single-round SFT model(QWEN1.5-14B (sft-offline)) and it ultimately reaches 95.0\

[htbp]

[H]

{1.5}
    {l|l|l|l}

& plan & execute & express \\ 
    training datasize  & 5000 & 6847    & 6193    \\ 
    test datasize      & 100  & 456     & 100     \\

==================================================

Chunk 22:
aining datasize  & 5000 & 6847    & 6193    \\ 
    test datasize      & 100  & 456     & 100     \\Table  presents the results of the end-to-end (the entire workflow) evaluation. We conducted experiments on three models: QWEN1.5-14B (sft-offline), QWEN1.5-14B (iter-best-model) and GPT-4, all combined with the PEER framework. QWEN1.5-14B (sft-offline) refers to the QWEN1.5 model fine-tuned with an offline SFT dataset, while QWEN1.5-14B (iter-best-model) indicates the best model obtained through iterative training. We can observe that the QWEN1.5-14B model, after iterative training, shows improvements across all metrics compared to the single-round SFT model. When combined with PEER, it ultimately reaches 95.0\

==================================================

Chunk 23:
metrics compared to the single-round SFT model. When combined with PEER, it ultimately reaches 95.0\Despite pioneering projects in this field, such as AutoGPT, BabyAGI, CAMEL, MetaGPT, and AutoGen , demonstrating their potential, achieving fully autonomous AI agents remains a significant challenge. These dynamic process agents, also known as autonomous intelligent agents, can autonomously perceive the environment, make decisions based on observations, and take actions. Subsequently, they reflect on the outcomes of their actions and plan their next steps accordingly. While theoretically generalizable to any scenario, they face issues such as poor controllability, instability, reproducibility problems, and low task completion rates in specialized domains . PEER strikes a balance between model flexibility and controllability through effective pattern design, considering practical industrial needs, including efficiency, cost-effectiveness, and operational simplicity, making it more suitable for industrial applications.

==================================================

Chunk 24:
cost-effectiveness, and operational simplicity, making it more suitable for industrial applications.Many research efforts aim to transform past experience into usable knowledge and apply it in new reasoning processes to drive continuous model evolution . However, these studies often place high demands on the model's ability to follow instructions, which is particularly challenging for models with fewer parameters. To overcome this challenge, our research adopts an iterative training approach. Specifically, we use both successful and failed cases from previous steps as new training data to promote the model's evolution.

==================================================

Chunk 25:
ccessful and failed cases from previous steps as new training data to promote the model's evolution.In this work, we introduced the PEER framework to address the tri-lemma of performance, cost, and data privacy in domain-specific applications. The framework balances flexibility and controllability through effective pattern design, meeting industrial demands for efficiency and cost-effectiveness. We also developed industrial practices that use online data and user feedback for effective model tuning, promoting continuous model evolution. Our empirical studies, particularly in the financial question-answering domain, demonstrate that this approach achieves 95.0\

Despite our progress in using multi-agent systems to address domain-specific tasks (e.g.finacial), several areas remain ripe for further exploration and improvement:

==================================================

Chunk 26:
in-specific tasks (e.g.finacial), several areas remain ripe for further exploration and improvement:Long-term Learning and Memory Mechanisms: Explore ways to equip the model to accumulate and utilize knowledge over extended periods.
     User Interaction and Feedback Mechanisms: Study how user interactions and feedback can further guide and optimize the model's behavior, achieving a more user-friendly agent design.
     Enhancing Generalization Capability: Investigate methods to further improve the model's generalization ability, enabling agents to tackle other financial problems such as factor-based stock selection or other quantitative trading strategies.

[!h]

{1.5}
    {c|c}

Category & Example\\

==================================================

Chunk 27:
ock selection or other quantitative trading strategies.

[!h]

{1.5}
    {c|c}

Category & Example\\Information Query & 12\
         General Financial QA & 11\
         Report Interpretation & 8\
         Target Analysis & 12\
         Strategy Advice & 12\
         Major Events Interpretation & 9\
         Macro Analysis & 12\
         Market Analysis & 12\
         Policy Interpretation & 12\

{1.2}
{!}{
{ll|c|c|c|c|c|c|c|c}

==================================================

Chunk 28:
Market Analysis & 12\
         Policy Interpretation & 12\

{1.2}
{!}{
{ll|c|c|c|c|c|c|c|c}&
   &
  Integrity &
  Relevance &
  Logic &
  Comprehensiveness &
  Compactness &
  Factuality &
  Structure &
  Average \\ 
{c|}{{*}{Plan}} &
  sft-offline &
  3.36 &
  3.88 &
  3.96 &
  3.24 &
   &
  - &
  - &
  3.61 \\  
{c|}{} &
  sft-iter-1 &
  3.5 &
  4.04 &
   &
   &
  - &
  - &
  - &
  3.76 \\  
{c|}{} &
  sft-iter-2 &
  3.51 &
   &
  4.12 &
   &
  - &
  - &
  - &
   \\  
{c|}{} &
  dpo-iter-1 &
   &
   &
   &
  3.35 &
  - &
  - &
  - &
  3.77 \\  
{c|}{} &
  dpo-iter-2 &
   &
  4.04 &
  4.12 &
  3.33 &
  - &
  - &
  - &
  3.75 \\ 
{l|}{{*}{Execute}} &
  sft-offline &
  3.95 &
  4.68 &
  4.53 &
  3.65 &
  4.24 &
  4.64 &
  4.19 &
  4.27 \\  
{l|}{} &
  sft-iter-1 &
  4.01 &
   &
  4.55 &
  3.76 &
   &
  4.73 &
  4.25 &
  4.32 \\  
{l|}{} &
  sft-iter-2 &
   &
  4.73 &
  4.59 &
   &
  4.11 &
   &
  4.29 &
  4.33 \\  
{l|}{} &
  dpo-iter-1 &
  4.02 &
  4.73 &
   &
  3.74 &
   &
  4.7 &
   &
   \\  
{l|}{} & dpo-iter-2 & 4.03          &  & 4.58          & 3.77          & 4.22       &  &  &  \\ 
{l|}{{*}{Express}} &
  sft-offline &
  4.08 &
  4.79 &
  4.53 &
  3.88 &
   &
  4.76 &
  4.31 &
  4.34 \\  
{l|}{} &
  sft-iter-1 &
  4.09 &
  4.7 &
  4.62 &
  3.96 &
  4.04 &
  4.71 &
  4.39 &
  4.36 \\  
{l|}{} &
  sft-iter-2 &
  4.19 &
  4.75 &
   &
  4.06 &
  3.95 &
  4.77 &
  4.41 &
  4.4 \\  
{l|}{} & dpo-iter-1 & 4.39          &  & 4.71          &  &  & 4.83          &   &  \\  
{l|}{} & dpo-iter-2 &  & 4.79          &  &  & 3.98       &  &   &  \\

==================================================

Chunk 29:
& 4.83          &   &  \\  
{l|}{} & dpo-iter-2 &  & 4.79          &  &  & 3.98       &  &   &  \\}

In this study, we experimented with several different iterative modeling approaches. The SFT-OFFLINE model refers to the SFT model trained exclusively on offline data. The DPO-ITER-1 model is obtained by further training the SFT-OFFLINE model using DPO. Similarly, the DPO-ITER-2 model is derived by continuing the iterative training on DPO-ITER-1 using DPO. The SFT-ITER-1 and SFT-ITER-2 models follow the same iterative training process.

==================================================

Chunk 30:
O-ITER-1 using DPO. The SFT-ITER-1 and SFT-ITER-2 models follow the same iterative training process.As shown in Table , in the three tasks (Plan, Execute, Express), the dpo-iter-2 model stands out with its exceptional performance, particularly in the Express task, where it leads significantly with an average score of 4.56. Meanwhile, the sft-iter-2 model also demonstrates its strength in the Plan task, achieving an average score of 3.78. In the Execute task, the dpo-iter-2 model again takes the top spot with an average score of 4.34.
Overall, the dpo-iter-2 model shows advantages in various metrics, indicating its adaptability and effectiveness across different tasks. Additionally, an increase in iteration count seems to positively impact model performance, though the extent of improvement depends on the specific model used (sft or dpo) and the particular requirements of the downstream tasks.

Table  and table  shows the prompt used for LLM Evaluation.

[t]

{10pt}
{1.2}
{p{}}

==================================================

Chunk 31:
nstream tasks.

Table  and table  shows the prompt used for LLM Evaluation.

[t]

{10pt}
{1.2}
{p{}}You will play a crucial role as a quality evaluator for large language model responses. Your task is to assess and analyze the answers provided by the model for a text-based question answering task, and score the model's responses based on the standard answers and scoring criteria:

\{
  "User Question": "The specific question posed by the user;",\\
  "Context": "Sufficient contextual information provided to answer the user's question;",\\
  "Standard Answer": "The standard answer, i.e., the ideal or reference answer;",\\
  "Model Answer": "Since question answering is an open-ended task, sometimes the model's answer may be better than the standard answer;"
\}
\\
\\
The scoring criteria are as follows:
\\

==================================================

Chunk 32:
l's answer may be better than the standard answer;"
\}
\\
\\
The scoring criteria are as follows:
\\Does the answer form a logical and coherent whole, directly addressing the core requirement of the question?\\
1 = Very Incomplete
2 = Incomplete
3 = Partially Complete
4 = Fairly Complete
5 = Very Complete
\\

Assess the degree to which the answer is related to the question posed.\\
1 = Completely Irrelevant
2 = Largely Irrelevant
3 = Somewhat Relevant
4 = Fairly Relevant
5 = Very Relevant
\\

Evaluate whether the answer is concise, avoiding redundancy or irrelevant information.\\
1 = Very Lengthy
2 = Quite Lengthy
3 = Moderate
4 = Fairly Concise
5 = Very Concise
\\

Assess whether the information in the answer is accurate and fact-based.\\
1 = Completely Inaccurate
2 = Mostly Inaccurate
3 = Partially Accurate
4 = Fairly Accurate
5 = Very Accurate
\\

==================================================

Chunk 33:
ely Inaccurate
2 = Mostly Inaccurate
3 = Partially Accurate
4 = Fairly Accurate
5 = Very Accurate
\\Evaluate whether the answer is logically coherent, reasonable, and aids in understanding.\\
1 = Completely Incoherent
2 = Not Very Coherent
3 = Partially Coherent
4 = Fairly Coherent
5 = Very Coherent
\\

Assess whether the answer is well-structured, with clear paragraph divisions and logical order.\\
1 = Completely Unstructured
2 = Poorly Structured
3 = Moderately Structured
4 = Well Structured
5 = Very Well Structured
\\

Evaluate whether the answer covers all relevant aspects of the question without significant omissions.\\
1 = Very Incomplete
2 = Incomplete
3 = Partially Complete
4 = Fairly Complete
5 = Very Complete
\\
\\
Please return your evaluation results strictly in the following JSON format, adhering to the criteria outlined above:

==================================================

Chunk 34:
r evaluation results strictly in the following JSON format, adhering to the criteria outlined above:\{
  "Analysis Process": "Explanation of the reasoning and process for scoring each dimension;",\\
  "Integrity": "Score;",\\
  "Relevance": "Score;",\\
  "Compactness": "Score;",\\
  "Factuality": "Score;",\\
  "Logic": "Score;",\\
  "Structure": "Score;",\\
  "Comprehensiveness": "Score;"
\}
\\

[]

{p{0.95}}

You will play an important role as a quality evaluator of answers provided by large language models. Your task is to assess and analyze the answers generated by the model for a given text-based question.

==================================================

Chunk 35:
ur task is to assess and analyze the answers generated by the model for a given text-based question.\{\\
  "User Question": "The specific question asked by the user",\\
  "Context": "Sufficient contextual information provided to answer the user's question",\\
  "Expected Answer": "The standard answer, representing an ideal or optimal response",\\
  "Model Answers": "This is a list comprising two answers. Each item in the list is numbered, and each answer may be correct or incorrect. Since the question-answering task is open-ended, sometimes a correct answer may be better than the standard answer"\\
\}

Your tasks are as follows:

Carefully read the two answers provided by the model.
 Compare the two answers and select the better one.
 If both answers are equally good or equally bad, respond with "equally good" or "equally bad," respectively.

During the evaluation, you need to consider the following key aspects:

==================================================

Chunk 36:
"equally bad," respectively.

During the evaluation, you need to consider the following key aspects:: Does the answer accurately address the user's question and align with the provided context?
 : Does the answer exhibit professional knowledge consistent with the standard answer?
 : Compared to the standard answer, does it adhere to current facts or time-sensitive information?
 : Is the answer succinct, avoiding redundancy or irrelevant information?
 : Is the information in the answer accurate and fact-based?
 : Is the answer logically coherent, reasonable, and does it aid understanding?
 : Does the answer cover all relevant aspects of the question without significant omissions?

Based on the analysis above, please strictly adhere to and return your evaluation results in the following JSON format:

==================================================

Chunk 37:
is above, please strictly adhere to and return your evaluation results in the following JSON format:\{\\
  "Reason for Choice": "Please provide detailed reasons for your choice here. Explain why you think one answer is better than the other, or why both are equally good or equally bad.",\\
  "Evaluation Result": "Choose one of the following options: 1 (if answer 1 is better), 2 (if answer 2 is better), equally good (if both answers are equally good), equally bad (if both answers are equally bad)."\\
\}
\\

[t]

{10pt}
{1.2}
{p{}}

From now on, your role is

Name: Research Assistant

Responsibilities:

Skilled at analyzing issues from various perspectives to help users quickly obtain information.
Determine what information to search for based on the context and the user’s question to provide the best possible answer.

First, identify the relevant timeframe based on the context and question, which could be a specific date or general terms like "latest," "recent," or "upcoming."

==================================================

Chunk 38:
nd question, which could be a specific date or general terms like "latest," "recent," or "upcoming."Then, decide what information needs to be searched to answer the question, ensuring a multi-dimensional and multi-angled approach.

Finally, provide clear and unambiguous search conditions, each as a complete sentence.
Rules to Follow:

Sub-questions must have clear and detailed descriptions of the subject, event, and timeframe. Avoid vague terms like "similar," "related," and replace them with specific details from the context.

Unless the question or context specifies a timeframe, default to "latest" or "recent," not "today."

If the question includes a timeframe, the search conditions must reflect this, converting terms like "today" to a specific date.

Each search condition must directly aim to find answers from that angle, without including terms like "search" or "query."

Each search condition must be a complete sentence with no ambiguity.

==================================================

Chunk 39:
erms like "search" or "query."

Each search condition must be a complete sentence with no ambiguity.Do not expand on the question; only answer the question asked.

When breaking down the question, adhere to the context’s requirements.
\\
\\

:
From now on, your role is

Name: Research Assistant

Responsibilities:

When a user searches for information on a particular issue and obtains several relevant pieces of information, you need to integrate, correct, and answer the user’s question.
Rules to Follow:

Do not use incorrect information.

Do not use information with inconsistent subjects. For example, if the question is about "John Doe from XYZ Company," but the information only mentions "John Doe" without confirming he is from "XYZ Company," do not use it.

Do not make vague speculations.

Follow the requirements specified in the context.

Avoid using ambiguous terms like XXX or ABC.

Ensure the content is detailed and emphasizes key points.
\\
\\

:
From now on, your role is

==================================================

Chunk 40:
r ABC.

Ensure the content is detailed and emphasizes key points.
\\
\\

:
From now on, your role isName: Research Assistant

Responsibilities:

Skilled at answering user questions from different angles in an organized manner.
You need to use only the provided knowledge to answer user questions professionally and in detail.
Rules to Follow:

Do not use incorrect information.

Do not use information with inconsistent subjects. For example, if the question is about "John Doe from XYZ Company," but the information only mentions "John Doe" without confirming he is from "XYZ Company," do not use it.

Do not make vague speculations.

Follow the requirements specified in the context.

Avoid using ambiguous terms like XXX or ABC.

Ensure the content is detailed and emphasizes key points.

Use only the provided knowledge and answer the questions step by step.
\\
\\

:
From now on, your role is

Name: Research Assistant

Background:

==================================================

Chunk 41:
he questions step by step.
\\
\\

:
From now on, your role is

Name: Research Assistant

Background:I am an editor for a financial news agency, writing a report for my client.
The client has provided specific requirements for the content and format of the report and desires a higher quality response.

My report includes two parts:

[Analysis Dimensions]: Rewriting or breaking down the user’s question into 3 to 5 queries as search conditions. Each question must be related to the original question’s theme.
[Answer]: Answering the user’s question with a complete and coherent response. The answer should be logically structured, clear, and concise.

Responsibilities:

==================================================

Chunk 42:
coherent response. The answer should be logically structured, clear, and concise.

Responsibilities:As a senior expert with 30 years of experience in the investment field, you need to check if my report meets the client’s requirements and provide feedback for improvement.
Determine whether my analysis dimensions and answers meet the user’s requirements. If the report meets all requirements, set Qualified to True; otherwise, set it to False.
If the report needs improvement, identify which part needs enhancement, either "Analysis Dimensions" or "Answer."
Provide detailed suggestions for improvement based on the user’s requirements. Describe your suggestions in detail without providing the complete revised answer.
Provide only relevant and high-impact feedback, avoiding unnecessary information.
Do not add redundant information or supplement reference sources. Only include highly relevant key information.

Your response structure must be:

Draft: Your thought process

==================================================

Chunk 43:
lude highly relevant key information.

Your response structure must be:

Draft: Your thought processQualified: Whether the user’s requirements are fully met, True or False

Role: The part that needs modification, either "Plan" or "Express"

Suggestion: Your suggested modifications. If the report meets all requirements, this can be empty. Only suggest modifications, without providing the complete revised answer

==================================================

