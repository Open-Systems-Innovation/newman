July 12, 2024
=================================================================================================================================================================
==================================================
In domain-specific applications, GPT-4, augmented with precise prompts or Retrieval-Augmented Generation (RAG), shows notable potential but faces the critical tri-lemma of performance, cost, and data privacy. High performance requires sophisticated processing techniques, yet managing multiple agents within a complex workflow often proves costly and challenging. To address this, we introduce the PEER (Plan, Execute, Express, Review) multi-agent framework. This systematizes domain-specific tasks by integrating precise question decomposition, advanced information retrieval, comprehensive summarization, and rigorous self-assessment. Given the concerns of cost and data privacy, enterprises are shifting from proprietary models like GPT-4 to custom models, striking a balance between cost, security, and performance. We developed industrial practices leveraging online data and user feedback for efficient model tuning. This study provides best practice guidelines for applying multi-agent systems in domain-specific problem-solving and implementing effective agent tuning strategies. Our empirical studies, particularly in the financial question-answering domain, demonstrate that our approach achieves 95.0% of GPT-4‚Äôs performance, while effectively managing costs and ensuring data privacy.
==================================================
Advanced LLMs like GPT-4, enhanced with engineered prompts or Retrieval-Augmented Generation (RAG), show great potential in handling complex tasks across various domains <cit.>. However, deploying these models involves a critical tri-lemma of performance, cost, and data privacy.
==================================================
While domain-specific applications benefit from meticulously fine-tuned models <cit.>, this approach incurs high costs due to the extensive resources needed for training and data acquisition. Alternatively, multi-agent systems have proven effective <cit.>, especially in complex tasks with distinct and conflicting role requirements that challenge even advanced models. However, current implementations often involve dynamic and complex workflows, increasing costs and complicating reproducibility. Consequently, enterprises are shifting from proprietary models like GPT-4 to custom models that better balance cost, security, and performance.
==================================================
To address these challenges, we introduce the PEER (Plan, Execute, Express, Review) multi-agent framework. This framework incorporates precise question decomposition, advanced information retrieval, comprehensive summarization, and rigorous self-assessment, aiming to streamline workflows and enhance problem-solving efficacy. Additionally, our research addresses enterprise demands for private deployment and stringent data privacy by developing industrial best practices that leverage online data and user feedback for effective model tuning. These practices are crucial for optimizing custom model performance while ensuring cost-efficiency and robust data privacy.
==================================================
The main contributions of this study include:
1. Providing and open-sourcing the PEER[https://github.com/alipay/agentUniverse] framework, characterized by its conciseness, effectiveness, and cost-efficiency, for effectively tackling domain-specific tasks. In experiments, it demonstrates superior performance compared to BabyAGI.
==================================================
2. Proposing a customized agent tuning strategy for 10-billion-parameter models, achieving performance comparable to GPT-4.
==================================================
3. Constructing and open-sourcing a dataset for use within the PEER framework, applicable to agent training, pre-training, and supervised fine-tuning in various financial analysis scenarios.
==================================================
With the advent of large models, we simulate the collaborative processes of human experts (e.g. financial) using multiple agents, achieving comparable interpretative results. This approach is encapsulated in the Plan, Execute, Express and Review (PEER) framework, where domain specific (e.g. financial) tasks are divided into these four steps. Each agent specializes in a single task, working together to accomplish the overall objective. The prompt for this section is attached in <ref>.
==================================================
Plan
The "Plan" agent uses a model to generate multiple related sub-questions from users‚Äô domain specific (e.g. financial) queries. These sub-questions serve as an interpretation framework, breaking down the original query into specific and actionable criteria, and expanding it for a comprehensive analysis.
==================================================
Execute
The "Execute" agent gathers information for each sub-question identified by "Plan". Using these sub-questions as search criteria, it finds relevant information from news, domain specific (e.g. financial) data, reports, and articles, enhancing accuracy, efficiency, and comprehensiveness. This information forms the foundation for interpreting domain events and answering questions.
==================================================
Express
The "Express" agent synthesizes collected information to perform comprehensive large-model reasoning, forming final conclusions. It emphasizes integrative reasoning and delivers professional descriptions tailored to the user's requirements.
==================================================
Review
The "Review" agent evaluates whether the "Express" agent's answer meets pre-established criteria. If satisfied, the final answer is delivered; if not, it provides modification suggestions and initiates another PEER iteration, enhancing answer quality through feedback.
==================================================
The PEER multi-agent cooperation framework‚Äôs strong reasoning and analysis abilities stem from its efficient task allocation, cooperation, and the feedback loop and self-optimization enabled by the "Review" agent. This ensures that the answers continuously improve towards the optimal solution. If an answer does not meet user requirements, the "Review" agent suggests modifications for the "Plan," "Execute," or "Express" agents. The relevant agent then adjusts its process to better meet expectations. For some simple tasks, one or more agents in PEER process can be skipped to simplify the procedure. For complex tasks, a nested pattern can be used, designing each agent to perform an isolate PEER process to enhance entire performance.For a more comprehensive understanding of the PEER framework, refer to Figure <ref>, which illustrates how these four agents synergize.
==================================================
font = footnotesize
Cyclic Workflow Diagram of the PEER Framework. The user's query, "Why did Buffett sell BYD stock?", prompts the "Plan" agent to generate four relevant sub-questions. The "Execute" agent then collects information, including BYD's financial data and expert opinions. The "Express" agent synthesizes a comprehensive answer, which the "Review" agent evaluates and, if necessary, suggests modifications.
==================================================
where N is the number of training examples, C is the number of classes, y_ij is the ground truth one-hot encoded vector, and ≈∑_ij is the predicted probability for class j. We used a robust model to generate an offline training dataset ùîª_off, which was then refined and validated by human annotators for quality assurance.
==================================================
Rejection sampling, as used in LLaMA2 <cit.>, involves generating samples from a pre-trained model and filtering based on quality criteria to retain only high-quality examples. Unlike direct offline supervised fine-tuning (SFT), rejection sampling automates initial filtering to reduce low-quality samples before human annotation. In our iterative training process, rejection sampling boosts performance post offline dataset training.
==================================================
Direct Preference Optimization (DPO), has emerged as efficient alternatives to RLHF, eliminating the need for a separate reward model <cit.>. The loss function for DPO is defined as follows:
==================================================
‚Ñí_DPO(œÄ_Œ∏ ; œÄ_ref) 
    = -ùîº_(x, y_w, y_l) ‚àºùíü[    logœÉ( Œ≤logœÄ_Œ∏ (y_w | x)/œÄ_ref (y_w | x)
       - Œ≤logœÄ_Œ∏ (y_l | x)/œÄ_ref (y_l | x)) ]
==================================================
where œÄ_Œ∏ is the language model being optimized and œÄ_ref refers to the model after SFT (œÄ^SFT). The scaling factor Œ≤ measures errors in ranking results and accounts for the KL constraint. In vanilla/offline Direct Preference Optimization (DPO), the model is optimized using a given set of preference data (x, y_w, y_l) ‚àºùíü, where the dataset-generating model and the optimized model are not the same.
==================================================
When optimizing DPO models, offline preference datasets and off-policy updates can cause generalization issues with out-of-distribution (OOD) queries. These issues can be mitigated by incorporating online preference datasets and using on-policy learning approaches. <cit.>.
==================================================
We follow the experimental setup of <cit.>, utilizing a batch size of m in online setting. Our methodology integrates the LLM-as-a-Judge approach for real-time feedback, as introduced by <cit.>, to refine the model progressively.
==================================================
Algorithm <ref> outlines our iterative training process, starting with the initial dataset D_off. The agent processes each batch iteratively, involving model evaluation, data generation, and refinement. It generates multiple candidate responses per input, using a reward model (GPT-4o) to select the optimal response and compare it with the ground truth. If the model-generated response exceeds the quality threshold, it replaces the original training sample. For DPO, the lowest-ranked response is identified as a negative example. The updated dataset is then used to refine the model via SFT or DPO techniques. After multiple iterations, the algorithm outputs the best-performing model variant based on predefined metrics. This iterative process continuously enhances response quality, creating a self-refining training paradigm that progressively improves model performance. Figure <ref> illustrates this process.
==================================================
Initialize M_0^SFT and M_0^DPO using D_off 
    i = 1 to T
        Evaluate M_i-1^SFT and M_i-1^DPO on D_eval
        type in {SFT, DPO}
            D_i^type‚àÖ
            (q, r_gt) in D_off
                R Gen.(N_cand, M_i-1^type(q))
==================================================
r_bestmax_r ‚àà R M_reward(q, r)
M_reward(q, r_best) > M_reward(q, r_gt)
    r_sel r_best
r_sel r_gt
type = DPO
==================================================
r_worstmin_r ‚àà R M_reward(q, r)
                    Add (q, r_sel, r_worst) to D_i^type
                Add (q, r_sel) to D_i^type 
                Train M_i^type using D_i^type 
        Best M_N^SFT, M_N^DPO, and evaluation results
==================================================
D_off: offline training dataset, 
D_eval: evaluation dataset, 
T: number of iterations, 
N_cand: number of candidate responses generated each time, 
M_reward: reward model
==================================================
Iterative Training Process: Initially, Model 0 is trained on offline data. This model then generates two sets of predictions: one to create training data for the next iteration (upper section) and another to provide evaluation results for the current iteration (lower section). This cycle is repeated iteratively across subsequent training phases.
==================================================
We conduct experiment on a real-word industry financial QA dataset to validate the PEER framework discussed in section <ref> and evaluate the agent tuning methods discussed in section <ref>.
==================================================
Dataset
Since the main usage scenario of PEER framework is the interpretation and analysis of domain events and problems, we mainly tested and compared the performance of PEER on the dataset of financial QA. We sampled hundreds of professional questions[https://github.com/alipay/agentUniverse/tree/master/dataset]from our business scenarios and divided them into nine categories. Details of the dataset distribution are shown in Table <ref>.
==================================================
Baselines
We conducted experiments using two base models, GPT-3.5 turbo (16k) and GPT-4o, with Python for execution. For FinQA datasets, we compared with the BabyAGI multi-agent framework due to its similar task creation, organization, and execution capabilities to PEER.
==================================================
To assess the impact of the "Review" agent in the PEER framework, we designed self-ablation experiments with and without the "Review" agent. We set the maximum rounds for both BabyAGI and PEER (with "Review") to 5 and used Google for information retrieval. Under GPT-3.5 turbo (16k), we recalled the top 2 search results with a token limit of 13,000. For GPT-4o, we increased the parameters to the top 6 and 125,000 tokens, leveraging the model's enhanced performance.
==================================================
Metrics
Despite GPT-4‚Äôs widespread use for evaluations, its confidence can be influenced by position and verbosity biases <cit.>. To mitigate these issues, we have developed two evaluation methodologies based on GPT-4:
==================================================
* GPT-4 scores all answers across various dimensions, and we calculate the average score for each dimension. Detailed scoring dimensions, rules, and their meanings are provided in Table <ref> in the Appendix.
==================================================
* GPT-4 selects the best answer between those provided by PEER and the control group. For this evaluation, we use the win rate as the metric, with selection criteria outlined in Table <ref>.
==================================================
4-11 
                 Integrity     Relevance     Compactness     Factuality     Logic     Structure     Comprehensiveness     Average
==================================================
4*Comparison Experiment   2*GPT-3.5-turbo-16K    BabyAGI     3.49     3.79     3.55     3.84     3.94     3.76     3.47     3.69
==================================================
2-11
        2*GPT-4o    BabyAGI     3.16     3.32     3.32     3.98     3.78     3.86     3.14     3.51
==================================================
4*Ablation Experiment   2*GPT-3.5-turbo-16K    PEE     3.91     4.30     3.91     4.26     4.30     4.14     3.78     4.09
==================================================
Analysis In the comparative experiment with BabyAGI, as depicted in Table <ref> and Figure <ref>, PEER consistently surpasses BabyAGI in both average score and win rate, irrespective of the base model employed. PEER demonstrates superior performance in dimensions such as integrity, relevance, logic, structure, and comprehensiveness, often by a margin exceeding one point. Specifically, under the GPT-3.5 turbo (16k) model, PEER achieves a win rate of 83% compared to BabyAGI, and still maintains an 81% win rate under the GPT-4o model. This is attributed to PEER‚Äôs strategy of simultaneously addressing multiple questions and synthesizing responses, in contrast to BabyAGI‚Äôs approach of addressing one question per round.
==================================================
In the ablation experiment, as illustrated in Table <ref> and Figure 3, PEER scores higher in most dimensions and attains a 64% win rate under the GPT-3.5 turbo (16k) model compared to PEE. However, under the GPT-4o model, the advantages conferred by the "Review" agent diminish, as GPT-4o inherently excels in processing, understanding, and expression. The initial outputs from the "Plan," "Execute," and "Express" agents sufficiently meet the requirements, rendering further modifications less impactful. Consequently, PEER‚Äôs win rate decreases to 46%, and the score differences between the two frameworks also narrow with the GPT-4o model compared to the GPT-3.5 turbo (16k) model. This indicates that the "Review" agent can significantly enhance overall quality when the base model‚Äôs performance is less robust.
==================================================
Dataset  We conducted two categories of experiments: one focusing on individual agents and the other on the entire workflow. Dataset sizes are provided in Table <ref>, with the data being open-sourced. The test set for individual agents is derived from the intermediate results of the evaluation set detailed in Table <ref>, whereas the test set for the entire workflow corresponds directly to Table <ref>.
==================================================
Experiment Setup As in Section <ref>, for the evaluation of individual agents and the entire workflow, we also employed the LLM-as-a-Judge approach. Specifically, for individual agents, we used scoring and pairwise comparison to evaluate the performance of each iteration. For the entire workflow, we used GPT-4o to score and compare the results of GPT-4 + PEER, the SFT results using offline data, and the best model obtained through iterative training.
==================================================
Analysis Figure <ref> illustrates the win, tie, and loss rates across different iterations for three agents involved in planning, execution, and expression. Both DPO and SFT show progress with each iteration. For example, for the planning agent, the first iteration of SFT achieves a win rate of 43.15% compared to SFT-offline, improving slightly to 43.21% in the second iteration. DPO demonstrates a faster convergence than SFT. In the second iteration, the win rates for SFT across the three agents are 43.21%, 41.34%, and 53.33%, respectively. In contrast, DPO's win rates are 23.17%, 20.74%, and 27.17%, which are lower than the corresponding tie rates of 56.61%, 60.60%, and 57.61%. Due to space limitations, detailed scoring results are provided in Appendix <ref> and Table <ref>.
==================================================
2-9
     Integrity     Relevance     Logic     Comprehensiveness     Compactness     Factuality     Structure     Average
==================================================
QWEN1.5-14B (sft-offline)+ PEER          4.09          4.58          3.34      4.23                  4.32            4.22           4.03          4.12
==================================================
QWEN1.5-14B (iter-best-model) + PEER     4.4           4.63          3.42      4.35                  4.61            4.77           4.28          4.35
==================================================
font = footnotesize
Evaluation of the entire workflow: the model after iterative training(QWEN1.5-14B (iter-best-model) + PEER), shows improvements across all metrics compared to the single-round SFT model(QWEN1.5-14B (sft-offline)) and it ultimately reaches 95.0% of the performance of GPT-4 + PEER.
==================================================
Win rate of tuned-agent. Both DPO and SFT show progress in each iteration and DPO converges faster than SFT.
==================================================
plan     execute     express 
 training datasize      5000     6847        6193    
 test datasize          100      456         100
==================================================
Table <ref> presents the results of the end-to-end (the entire workflow) evaluation. We conducted experiments on three models: QWEN1.5-14B (sft-offline), QWEN1.5-14B (iter-best-model) and GPT-4, all combined with the PEER framework. QWEN1.5-14B (sft-offline) refers to the QWEN1.5 model fine-tuned with an offline SFT dataset, while QWEN1.5-14B (iter-best-model) indicates the best model obtained through iterative training. We can observe that the QWEN1.5-14B model, after iterative training, shows improvements across all metrics compared to the single-round SFT model. When combined with PEER, it ultimately reaches 95.0% of the performance of GPT-4 + PEER.
==================================================
Multi-agent system Despite pioneering projects in this field, such as AutoGPT, BabyAGI, CAMEL, MetaGPT, and AutoGen <cit.>, demonstrating their potential, achieving fully autonomous AI agents remains a significant challenge. These dynamic process agents, also known as autonomous intelligent agents, can autonomously perceive the environment, make decisions based on observations, and take actions. Subsequently, they reflect on the outcomes of their actions and plan their next steps accordingly. While theoretically generalizable to any scenario, they face issues such as poor controllability, instability, reproducibility problems, and low task completion rates in specialized domains <cit.>. PEER strikes a balance between model flexibility and controllability through effective pattern design, considering practical industrial needs, including efficiency, cost-effectiveness, and operational simplicity, making it more suitable for industrial applications.
==================================================
Agent self-evolution Many research efforts aim to transform past experience into usable knowledge and apply it in new reasoning processes to drive continuous model evolution <cit.>. However, these studies often place high demands on the model's ability to follow instructions, which is particularly challenging for models with fewer parameters. To overcome this challenge, our research adopts an iterative training approach. Specifically, we use both successful and failed cases from previous steps as new training data to promote the model's evolution.
==================================================
Conclusion In this work, we introduced the PEER framework to address the tri-lemma of performance, cost, and data privacy in domain-specific applications. The framework balances flexibility and controllability through effective pattern design, meeting industrial demands for efficiency and cost-effectiveness. We also developed industrial practices that use online data and user feedback for effective model tuning, promoting continuous model evolution. Our empirical studies, particularly in the financial question-answering domain, demonstrate that this approach achieves 95.0% of GPT-4‚Äôs performance while managing costs and safeguarding data privacy.
==================================================
Future work Despite our progress in using multi-agent systems to address domain-specific tasks (e.g.finacial), several areas remain ripe for further exploration and improvement:
==================================================
* Long-term Learning and Memory Mechanisms: Explore ways to equip the model to accumulate and utilize knowledge over extended periods.
==================================================
* User Interaction and Feedback Mechanisms: Study how user interactions and feedback can further guide and optimize the model's behavior, achieving a more user-friendly agent design.
==================================================
* Enhancing Generalization Capability: Investigate methods to further improve the model's generalization ability, enabling agents to tackle other financial problems such as factor-based stock selection or other quantitative trading strategies.
==================================================
Integrity    
  Relevance    
  Logic    
  Comprehensiveness    
  Compactness    
  Factuality    
  Structure    
  Average 
 1c|5*Plan    
  sft-offline    
  3.36    
  3.88    
  3.96    
  3.24    
  -    
  -    
  -    
  3.61 
 2-10 
1c|    
  sft-iter-1    
  3.5    
  4.04    
  4.12    
  3.37    
  -    
  -    
  -    
  3.76 
 2-10 
1c|    
  sft-iter-2    
  3.51    
  4.11    
  4.12    
  3.36    
  -    
  -    
  -    
  3.78 
 2-10 
1c|    
  dpo-iter-1    
  3.52    
  4.09    
  4.13    
  3.35    
  -    
  -    
  -    
  3.77 
 2-10 
1c|    
  dpo-iter-2    
  3.52    
  4.04    
  4.12    
  3.33    
  -    
  -    
  -    
  3.75 
 1l|5*Execute    
  sft-offline    
  3.95    
  4.68    
  4.53    
  3.65    
  4.24    
  4.64    
  4.19    
  4.27 
 2-10 
1l|    
  sft-iter-1    
  4.01    
  4.74    
  4.55    
  3.76    
  4.19    
  4.73    
  4.25    
  4.32 
 2-10 
1l|    
  sft-iter-2    
  4.06    
  4.73    
  4.59    
  3.79    
  4.11    
  4.72    
  4.29    
  4.33 
 2-10 
1l|    
  dpo-iter-1    
  4.02    
  4.73    
  4.61    
  3.74    
  4.23    
  4.7    
  4.3    
  4.33 
 2-10 
1l|     dpo-iter-2     4.03              4.75     4.58              3.77              4.22           4.74     4.31     4.34 
 1l|5*Express    
  sft-offline    
  4.08    
  4.79    
  4.53    
  3.88    
  4.07    
  4.76    
  4.31    
  4.34 
 2-10 
1l|    
  sft-iter-1    
  4.09    
  4.7    
  4.62    
  3.96    
  4.04    
  4.71    
  4.39    
  4.36 
 2-10 
1l|    
  sft-iter-2    
  4.19    
  4.75    
  4.63    
  4.06    
  3.95    
  4.77    
  4.41    
  4.4 
 2-10 
1l|     dpo-iter-1     4.39              4.93     4.71              4.26     4     4.83              4.5      4.52 
 2-10 
1l|     dpo-iter-2     4.46     4.79              4.77     4.36     3.98           4.87     4.7      4.56
==================================================
In this study, we experimented with several different iterative modeling approaches. The SFT-OFFLINE model refers to the SFT model trained exclusively on offline data. The DPO-ITER-1 model is obtained by further training the SFT-OFFLINE model using DPO. Similarly, the DPO-ITER-2 model is derived by continuing the iterative training on DPO-ITER-1 using DPO. The SFT-ITER-1 and SFT-ITER-2 models follow the same iterative training process.
==================================================
As shown in Table <ref>, in the three tasks (Plan, Execute, Express), the dpo-iter-2 model stands out with its exceptional performance, particularly in the Express task, where it leads significantly with an average score of 4.56. Meanwhile, the sft-iter-2 model also demonstrates its strength in the Plan task, achieving an average score of 3.78. In the Execute task, the dpo-iter-2 model again takes the top spot with an average score of 4.34.
Overall, the dpo-iter-2 model shows advantages in various metrics, indicating its adaptability and effectiveness across different tasks. Additionally, an increase in iteration count seems to positively impact model performance, though the extent of improvement depends on the specific model used (sft or dpo) and the particular requirements of the downstream tasks.
==================================================
You will play a crucial role as a quality evaluator for large language model responses. Your task is to assess and analyze the answers provided by the model for a text-based question answering task, and score the model's responses based on the standard answers and scoring criteria:
==================================================
"Model Answer": "Since question answering is an open-ended task, sometimes the model's answer may be better than the standard answer;"
}
==================================================
Integrity
Does the answer form a logical and coherent whole, directly addressing the core requirement of the question?
==================================================
1 = Completely Irrelevant
2 = Largely Irrelevant
3 = Somewhat Relevant
4 = Fairly Relevant
5 = Very Relevant
==================================================
1 = Completely Inaccurate
2 = Mostly Inaccurate
3 = Partially Accurate
4 = Fairly Accurate
5 = Very Accurate
==================================================
1 = Completely Incoherent
2 = Not Very Coherent
3 = Partially Coherent
4 = Fairly Coherent
5 = Very Coherent
==================================================
Structure
Assess whether the answer is well-structured, with clear paragraph divisions and logical order.
==================================================
1 = Completely Unstructured
2 = Poorly Structured
3 = Moderately Structured
4 = Well Structured
5 = Very Well Structured
==================================================
Comprehensiveness
Evaluate whether the answer covers all relevant aspects of the question without significant omissions.
==================================================
Please return your evaluation results strictly in the following JSON format, adhering to the criteria outlined above:
==================================================
You will play an important role as a quality evaluator of answers provided by large language models. Your task is to assess and analyze the answers generated by the model for a given text-based question.
==================================================
"Model Answers": "This is a list comprising two answers. Each item in the list is numbered, and each answer may be correct or incorrect. Since the question-answering task is open-ended, sometimes a correct answer may be better than the standard answer"
==================================================
* If both answers are equally good or equally bad, respond with "equally good" or "equally bad," respectively.
==================================================
* Relevance: Does the answer accurately address the user's question and align with the provided context?
==================================================
* Professionalism: Does the answer exhibit professional knowledge consistent with the standard answer?
==================================================
* Timeliness and Factuality: Compared to the standard answer, does it adhere to current facts or time-sensitive information?
==================================================
* Comprehensiveness: Does the answer cover all relevant aspects of the question without significant omissions?
==================================================
Based on the analysis above, please strictly adhere to and return your evaluation results in the following JSON format:
==================================================
"Reason for Choice": "Please provide detailed reasons for your choice here. Explain why you think one answer is better than the other, or why both are equally good or equally bad.",
==================================================
"Evaluation Result": "Choose one of the following options: 1 (if answer 1 is better), 2 (if answer 2 is better), equally good (if both answers are equally good), equally bad (if both answers are equally bad)."
==================================================
Skilled at analyzing issues from various perspectives to help users quickly obtain information.
Determine what information to search for based on the context and the user‚Äôs question to provide the best possible answer.
==================================================
First, identify the relevant timeframe based on the context and question, which could be a specific date or general terms like "latest," "recent," or "upcoming."
==================================================
Then, decide what information needs to be searched to answer the question, ensuring a multi-dimensional and multi-angled approach.
==================================================
Finally, provide clear and unambiguous search conditions, each as a complete sentence.
Rules to Follow:
==================================================
Sub-questions must have clear and detailed descriptions of the subject, event, and timeframe. Avoid vague terms like "similar," "related," and replace them with specific details from the context.
==================================================
If the question includes a timeframe, the search conditions must reflect this, converting terms like "today" to a specific date.
==================================================
Each search condition must directly aim to find answers from that angle, without including terms like "search" or "query."
==================================================
When a user searches for information on a particular issue and obtains several relevant pieces of information, you need to integrate, correct, and answer the user‚Äôs question.
Rules to Follow:
==================================================
Do not use information with inconsistent subjects. For example, if the question is about "John Doe from XYZ Company," but the information only mentions "John Doe" without confirming he is from "XYZ Company," do not use it.
==================================================
Skilled at answering user questions from different angles in an organized manner.
You need to use only the provided knowledge to answer user questions professionally and in detail.
Rules to Follow:
==================================================
Do not use information with inconsistent subjects. For example, if the question is about "John Doe from XYZ Company," but the information only mentions "John Doe" without confirming he is from "XYZ Company," do not use it.
==================================================
I am an editor for a financial news agency, writing a report for my client.
The client has provided specific requirements for the content and format of the report and desires a higher quality response.
==================================================
[Analysis Dimensions]: Rewriting or breaking down the user‚Äôs question into 3 to 5 queries as search conditions. Each question must be related to the original question‚Äôs theme.
[Answer]: Answering the user‚Äôs question with a complete and coherent response. The answer should be logically structured, clear, and concise.
==================================================
As a senior expert with 30 years of experience in the investment field, you need to check if my report meets the client‚Äôs requirements and provide feedback for improvement.
Determine whether my analysis dimensions and answers meet the user‚Äôs requirements. If the report meets all requirements, set Qualified to True; otherwise, set it to False.
If the report needs improvement, identify which part needs enhancement, either "Analysis Dimensions" or "Answer."
Provide detailed suggestions for improvement based on the user‚Äôs requirements. Describe your suggestions in detail without providing the complete revised answer.
Provide only relevant and high-impact feedback, avoiding unnecessary information.
Do not add redundant information or supplement reference sources. Only include highly relevant key information.
==================================================
Suggestion: Your suggested modifications. If the report meets all requirements, this can be empty. Only suggest modifications, without providing the complete revised answer
==================================================
